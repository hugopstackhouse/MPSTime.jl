{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 10:52:00 [INFO]: Have set the random seed as 1234 for numpy and pytorch.\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import CSDI, BRITS\n",
    "from pypots.utils.random import set_random_seed\n",
    "from pypots.utils.metrics import calc_mae\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../../../Imputation/Imputation_Algs/cdrec\")\n",
    "from cdrec.python.recovery import centroid_recovery as CDrec\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce GTX 1070\n",
      "CUDA ENABLED: True\n"
     ]
    }
   ],
   "source": [
    "# check that GPU acceleration is enabled\n",
    "import torch\n",
    "torch.cuda.device_count()\n",
    "print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "print(f\"CUDA ENABLED: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the original ECG200 train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 96)\n",
      "(100,)\n",
      "(100, 96)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "train_f = np.loadtxt(\"ECG200_TRAIN.txt\")\n",
    "test_f = np.loadtxt(\"ECG200_TEST.txt\")\n",
    "X_train = train_f[:, 1:]\n",
    "y_train = train_f[:, 0]\n",
    "X_test = test_f[:, 1:]\n",
    "y_test = test_f[:, 0]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 96, 1)\n",
      "(100, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape data for imputation model\n",
    "X_train_original = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_original =  X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "y_train_original = y_train\n",
    "y_test_original = y_test\n",
    "print(X_train_original.shape)\n",
    "print(X_test_original.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the train and test splits for resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 96, 1)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "Xs = np.vstack([X_train_original, X_test_original])\n",
    "print(Xs.shape)\n",
    "ys = np.concatenate([y_train_original, y_test_original])\n",
    "print(ys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the resample fold indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n"
     ]
    }
   ],
   "source": [
    "with open(\"resample_folds_python_idx.json\", \"r\") as f:\n",
    "    resample_fold_idxs_f = json.load(f)\n",
    "resample_fold_idxs = {int(k): v for k, v in resample_fold_idxs_f.items()}\n",
    "print(resample_fold_idxs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the first fold corresponds to the original train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 96, 1)\n",
      "(100, 96, 1)\n",
      "(100,)\n",
      "(100,)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "X_train_f1 = Xs[resample_fold_idxs[0][\"train\"]]\n",
    "X_test_f1 = Xs[resample_fold_idxs[0][\"test\"]]\n",
    "y_train_f1 = ys[resample_fold_idxs[0][\"train\"]]\n",
    "y_test_f1 = ys[resample_fold_idxs[0][\"test\"]]\n",
    "\n",
    "print(X_train_f1.shape)\n",
    "print(X_test_f1.shape)\n",
    "print(y_train_f1.shape)\n",
    "print(y_test_f1.shape)\n",
    "\n",
    "print(np.all(np.equal(X_train_f1, X_train_original)))\n",
    "print(np.all(np.equal(y_train_f1, y_train_original)))\n",
    "print(np.all(np.equal(X_test_f1, X_test_original)))\n",
    "print(np.all(np.equal(y_test_f1, y_test_original)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the imputation window idxs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([5, 15, 25, 35, 45, 55, 65, 75, 85, 95])\n"
     ]
    }
   ],
   "source": [
    "with open(\"windows_python_idx.json\", \"r\") as f:\n",
    "    window_idxs_f = json.load(f)\n",
    "window_idxs = {int(float(k)*100): v for k, v in window_idxs_f.items()}\n",
    "print(window_idxs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to evaluate CSDI across folds and window idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_folds_csdi(Xs, ys, fold_idxs, window_idxs, model):\n",
    "    fold_scores = dict()\n",
    "    for fold in range(0, len(fold_idxs)):\n",
    "        print(f\"Evaluating fold {fold}/{len(fold_idxs)-1}...\")\n",
    "        # make the splits\n",
    "        X_train_fold = Xs[fold_idxs[fold][\"train\"]]\n",
    "        y_train_fold = ys[fold_idxs[fold][\"train\"]]\n",
    "        X_test_fold = Xs[fold_idxs[fold][\"test\"]]\n",
    "        y_test_fold = ys[fold_idxs[fold][\"test\"]]\n",
    "        # check class distributions\n",
    "        counts_tr = np.unique(y_train_fold, return_counts=True)[1]\n",
    "        print(f\"Training class distribution: {counts_tr/np.sum(counts_tr)}\")\n",
    "        counts_te = np.unique(y_test_fold, return_counts=True)[1]\n",
    "        print(f\"Testing class distribution: {counts_te/np.sum(counts_te)}\")\n",
    "        print(f\"Training CSDI on fold {fold}...\")\n",
    "        model.fit(train_set={'X':X_train_fold})\n",
    "        print(\"Finished training!\")\n",
    "        # loop over % missing\n",
    "        percent_missing_score = dict()\n",
    "        for pm in window_idxs:\n",
    "            print(f\"Imputing {pm}% missing data over {len(window_idxs[pm])} windows...\")\n",
    "            per_window_scores = dict()\n",
    "            for (idx, widx) in enumerate(window_idxs[pm]):\n",
    "                X_test_corrupted = X_test_fold.copy()\n",
    "                X_test_corrupted[:, widx] = np.nan\n",
    "                mask = np.isnan(X_test_corrupted) # mask ensures only misisng values are imputed\n",
    "                csdi_imputed = model.impute(test_set={'X': X_test_corrupted}).squeeze(axis=1)\n",
    "                errs = [calc_mae(csdi_imputed[i], X_test_fold[i], mask[i]) for i in range(0, X_test_fold.shape[0])] # get individual errors for uncertainty quantification\n",
    "                per_window_scores[idx] = errs\n",
    "            percent_missing_score[pm] = per_window_scores\n",
    "        fold_scores[fold] = percent_missing_score\n",
    "    return fold_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define CSDI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 20:59:44 [INFO]: No given device, using default device: cuda\n",
      "2024-10-20 20:59:44 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "c:\\Users\\smoor\\miniconda3\\envs\\pypots\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "2024-10-20 20:59:44 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,693,601\n"
     ]
    }
   ],
   "source": [
    "csdi = CSDI(\n",
    "    n_steps=len(X_test_original[0]),\n",
    "    n_features=1, # univariate time series, so num features is equal to one\n",
    "    n_layers=6,\n",
    "    n_heads=2,\n",
    "    n_channels=128,\n",
    "    d_time_embedding=64,\n",
    "    d_feature_embedding=32,\n",
    "    d_diffusion_embedding=128,\n",
    "    target_strategy=\"random\",\n",
    "    n_diffusion_steps=50,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    patience=None,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=None,\n",
    "    model_saving_strategy=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate CSDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:00:12 [INFO]: Epoch 001 - training loss: 0.1609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 0/29...\n",
      "Training class distribution: [0.31 0.69]\n",
      "Testing class distribution: [0.36 0.64]\n",
      "Training CSDI on fold 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:00:12 [INFO]: Epoch 002 - training loss: 0.2104\n",
      "2024-10-20 21:00:12 [INFO]: Epoch 003 - training loss: 0.2259\n",
      "2024-10-20 21:00:12 [INFO]: Epoch 004 - training loss: 0.3225\n",
      "2024-10-20 21:00:12 [INFO]: Epoch 005 - training loss: 0.2007\n",
      "2024-10-20 21:00:12 [INFO]: Epoch 006 - training loss: 0.2876\n",
      "2024-10-20 21:00:13 [INFO]: Epoch 007 - training loss: 0.1745\n",
      "2024-10-20 21:00:13 [INFO]: Epoch 008 - training loss: 0.2211\n",
      "2024-10-20 21:00:13 [INFO]: Epoch 009 - training loss: 0.1848\n",
      "2024-10-20 21:00:13 [INFO]: Epoch 010 - training loss: 0.1543\n",
      "2024-10-20 21:00:13 [INFO]: Epoch 011 - training loss: 0.2865\n",
      "2024-10-20 21:00:13 [INFO]: Epoch 012 - training loss: 0.1915\n",
      "2024-10-20 21:00:14 [INFO]: Epoch 013 - training loss: 0.1835\n",
      "2024-10-20 21:00:14 [INFO]: Epoch 014 - training loss: 0.1992\n",
      "2024-10-20 21:00:14 [INFO]: Epoch 015 - training loss: 0.1615\n",
      "2024-10-20 21:00:14 [INFO]: Epoch 016 - training loss: 0.1701\n",
      "2024-10-20 21:00:14 [INFO]: Epoch 017 - training loss: 0.1846\n",
      "2024-10-20 21:00:14 [INFO]: Epoch 018 - training loss: 0.1639\n",
      "2024-10-20 21:00:14 [INFO]: Epoch 019 - training loss: 0.1869\n",
      "2024-10-20 21:00:15 [INFO]: Epoch 020 - training loss: 0.1724\n",
      "2024-10-20 21:00:15 [INFO]: Epoch 021 - training loss: 0.2250\n",
      "2024-10-20 21:00:15 [INFO]: Epoch 022 - training loss: 0.1955\n",
      "2024-10-20 21:00:15 [INFO]: Epoch 023 - training loss: 0.1755\n",
      "2024-10-20 21:00:15 [INFO]: Epoch 024 - training loss: 0.1996\n",
      "2024-10-20 21:00:15 [INFO]: Epoch 025 - training loss: 0.1320\n",
      "2024-10-20 21:00:15 [INFO]: Epoch 026 - training loss: 0.2226\n",
      "2024-10-20 21:00:16 [INFO]: Epoch 027 - training loss: 0.1958\n",
      "2024-10-20 21:00:16 [INFO]: Epoch 028 - training loss: 0.1905\n",
      "2024-10-20 21:00:16 [INFO]: Epoch 029 - training loss: 0.1740\n",
      "2024-10-20 21:00:16 [INFO]: Epoch 030 - training loss: 0.1788\n",
      "2024-10-20 21:00:16 [INFO]: Epoch 031 - training loss: 0.2841\n",
      "2024-10-20 21:00:16 [INFO]: Epoch 032 - training loss: 0.2765\n",
      "2024-10-20 21:00:17 [INFO]: Epoch 033 - training loss: 0.2613\n",
      "2024-10-20 21:00:17 [INFO]: Epoch 034 - training loss: 0.2826\n",
      "2024-10-20 21:00:17 [INFO]: Epoch 035 - training loss: 0.2416\n",
      "2024-10-20 21:00:17 [INFO]: Epoch 036 - training loss: 0.1651\n",
      "2024-10-20 21:00:17 [INFO]: Epoch 037 - training loss: 0.1980\n",
      "2024-10-20 21:00:17 [INFO]: Epoch 038 - training loss: 0.1786\n",
      "2024-10-20 21:00:17 [INFO]: Epoch 039 - training loss: 0.2325\n",
      "2024-10-20 21:00:18 [INFO]: Epoch 040 - training loss: 0.3237\n",
      "2024-10-20 21:00:18 [INFO]: Epoch 041 - training loss: 0.1937\n",
      "2024-10-20 21:00:18 [INFO]: Epoch 042 - training loss: 0.2247\n",
      "2024-10-20 21:00:18 [INFO]: Epoch 043 - training loss: 0.1680\n",
      "2024-10-20 21:00:18 [INFO]: Epoch 044 - training loss: 0.1785\n",
      "2024-10-20 21:00:18 [INFO]: Epoch 045 - training loss: 0.2174\n",
      "2024-10-20 21:00:19 [INFO]: Epoch 046 - training loss: 0.1996\n",
      "2024-10-20 21:00:19 [INFO]: Epoch 047 - training loss: 0.1801\n",
      "2024-10-20 21:00:19 [INFO]: Epoch 048 - training loss: 0.2036\n",
      "2024-10-20 21:00:19 [INFO]: Epoch 049 - training loss: 0.2759\n",
      "2024-10-20 21:00:19 [INFO]: Epoch 050 - training loss: 0.2078\n",
      "2024-10-20 21:00:19 [INFO]: Epoch 051 - training loss: 0.2080\n",
      "2024-10-20 21:00:19 [INFO]: Epoch 052 - training loss: 0.2435\n",
      "2024-10-20 21:00:20 [INFO]: Epoch 053 - training loss: 0.1719\n",
      "2024-10-20 21:00:20 [INFO]: Epoch 054 - training loss: 0.1530\n",
      "2024-10-20 21:00:20 [INFO]: Epoch 055 - training loss: 0.2614\n",
      "2024-10-20 21:00:20 [INFO]: Epoch 056 - training loss: 0.2359\n",
      "2024-10-20 21:00:20 [INFO]: Epoch 057 - training loss: 0.1955\n",
      "2024-10-20 21:00:20 [INFO]: Epoch 058 - training loss: 0.1936\n",
      "2024-10-20 21:00:21 [INFO]: Epoch 059 - training loss: 0.2030\n",
      "2024-10-20 21:00:21 [INFO]: Epoch 060 - training loss: 0.2104\n",
      "2024-10-20 21:00:21 [INFO]: Epoch 061 - training loss: 0.2036\n",
      "2024-10-20 21:00:21 [INFO]: Epoch 062 - training loss: 0.1975\n",
      "2024-10-20 21:00:21 [INFO]: Epoch 063 - training loss: 0.1539\n",
      "2024-10-20 21:00:21 [INFO]: Epoch 064 - training loss: 0.1502\n",
      "2024-10-20 21:00:21 [INFO]: Epoch 065 - training loss: 0.2074\n",
      "2024-10-20 21:00:22 [INFO]: Epoch 066 - training loss: 0.3330\n",
      "2024-10-20 21:00:22 [INFO]: Epoch 067 - training loss: 0.1537\n",
      "2024-10-20 21:00:22 [INFO]: Epoch 068 - training loss: 0.2270\n",
      "2024-10-20 21:00:22 [INFO]: Epoch 069 - training loss: 0.2681\n",
      "2024-10-20 21:00:22 [INFO]: Epoch 070 - training loss: 0.2836\n",
      "2024-10-20 21:00:22 [INFO]: Epoch 071 - training loss: 0.2159\n",
      "2024-10-20 21:00:23 [INFO]: Epoch 072 - training loss: 0.2142\n",
      "2024-10-20 21:00:23 [INFO]: Epoch 073 - training loss: 0.2648\n",
      "2024-10-20 21:00:23 [INFO]: Epoch 074 - training loss: 0.2368\n",
      "2024-10-20 21:00:23 [INFO]: Epoch 075 - training loss: 0.3219\n",
      "2024-10-20 21:00:23 [INFO]: Epoch 076 - training loss: 0.1653\n",
      "2024-10-20 21:00:23 [INFO]: Epoch 077 - training loss: 0.2472\n",
      "2024-10-20 21:00:23 [INFO]: Epoch 078 - training loss: 0.1518\n",
      "2024-10-20 21:00:24 [INFO]: Epoch 079 - training loss: 0.2462\n",
      "2024-10-20 21:00:24 [INFO]: Epoch 080 - training loss: 0.1936\n",
      "2024-10-20 21:00:24 [INFO]: Epoch 081 - training loss: 0.1433\n",
      "2024-10-20 21:00:24 [INFO]: Epoch 082 - training loss: 0.2073\n",
      "2024-10-20 21:00:24 [INFO]: Epoch 083 - training loss: 0.1673\n",
      "2024-10-20 21:00:24 [INFO]: Epoch 084 - training loss: 0.1597\n",
      "2024-10-20 21:00:25 [INFO]: Epoch 085 - training loss: 0.3013\n",
      "2024-10-20 21:00:25 [INFO]: Epoch 086 - training loss: 0.2711\n",
      "2024-10-20 21:00:25 [INFO]: Epoch 087 - training loss: 0.2756\n",
      "2024-10-20 21:00:25 [INFO]: Epoch 088 - training loss: 0.1693\n",
      "2024-10-20 21:00:25 [INFO]: Epoch 089 - training loss: 0.1483\n",
      "2024-10-20 21:00:25 [INFO]: Epoch 090 - training loss: 0.2139\n",
      "2024-10-20 21:00:25 [INFO]: Epoch 091 - training loss: 0.1836\n",
      "2024-10-20 21:00:26 [INFO]: Epoch 092 - training loss: 0.2121\n",
      "2024-10-20 21:00:26 [INFO]: Epoch 093 - training loss: 0.2185\n",
      "2024-10-20 21:00:26 [INFO]: Epoch 094 - training loss: 0.1334\n",
      "2024-10-20 21:00:26 [INFO]: Epoch 095 - training loss: 0.1526\n",
      "2024-10-20 21:00:26 [INFO]: Epoch 096 - training loss: 0.1967\n",
      "2024-10-20 21:00:26 [INFO]: Epoch 097 - training loss: 0.1379\n",
      "2024-10-20 21:00:26 [INFO]: Epoch 098 - training loss: 0.1624\n",
      "2024-10-20 21:00:27 [INFO]: Epoch 099 - training loss: 0.2203\n",
      "2024-10-20 21:00:27 [INFO]: Epoch 100 - training loss: 0.2650\n",
      "2024-10-20 21:00:27 [INFO]: Finished training. The best model is from epoch#25.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:04:15 [INFO]: Epoch 001 - training loss: 0.1595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:04:15 [INFO]: Epoch 002 - training loss: 0.2603\n",
      "2024-10-20 21:04:15 [INFO]: Epoch 003 - training loss: 0.2220\n",
      "2024-10-20 21:04:15 [INFO]: Epoch 004 - training loss: 0.2677\n",
      "2024-10-20 21:04:15 [INFO]: Epoch 005 - training loss: 0.1760\n",
      "2024-10-20 21:04:15 [INFO]: Epoch 006 - training loss: 0.2579\n",
      "2024-10-20 21:04:15 [INFO]: Epoch 007 - training loss: 0.1384\n",
      "2024-10-20 21:04:16 [INFO]: Epoch 008 - training loss: 0.2309\n",
      "2024-10-20 21:04:16 [INFO]: Epoch 009 - training loss: 0.1610\n",
      "2024-10-20 21:04:16 [INFO]: Epoch 010 - training loss: 0.1623\n",
      "2024-10-20 21:04:16 [INFO]: Epoch 011 - training loss: 0.2104\n",
      "2024-10-20 21:04:16 [INFO]: Epoch 012 - training loss: 0.2072\n",
      "2024-10-20 21:04:16 [INFO]: Epoch 013 - training loss: 0.2379\n",
      "2024-10-20 21:04:17 [INFO]: Epoch 014 - training loss: 0.2609\n",
      "2024-10-20 21:04:17 [INFO]: Epoch 015 - training loss: 0.1571\n",
      "2024-10-20 21:04:17 [INFO]: Epoch 016 - training loss: 0.2452\n",
      "2024-10-20 21:04:17 [INFO]: Epoch 017 - training loss: 0.2982\n",
      "2024-10-20 21:04:17 [INFO]: Epoch 018 - training loss: 0.1930\n",
      "2024-10-20 21:04:17 [INFO]: Epoch 019 - training loss: 0.1816\n",
      "2024-10-20 21:04:17 [INFO]: Epoch 020 - training loss: 0.2106\n",
      "2024-10-20 21:04:18 [INFO]: Epoch 021 - training loss: 0.1657\n",
      "2024-10-20 21:04:18 [INFO]: Epoch 022 - training loss: 0.1939\n",
      "2024-10-20 21:04:18 [INFO]: Epoch 023 - training loss: 0.2435\n",
      "2024-10-20 21:04:18 [INFO]: Epoch 024 - training loss: 0.2217\n",
      "2024-10-20 21:04:18 [INFO]: Epoch 025 - training loss: 0.2141\n",
      "2024-10-20 21:04:18 [INFO]: Epoch 026 - training loss: 0.1657\n",
      "2024-10-20 21:04:18 [INFO]: Epoch 027 - training loss: 0.1905\n",
      "2024-10-20 21:04:19 [INFO]: Epoch 028 - training loss: 0.3094\n",
      "2024-10-20 21:04:19 [INFO]: Epoch 029 - training loss: 0.3244\n",
      "2024-10-20 21:04:19 [INFO]: Epoch 030 - training loss: 0.1751\n",
      "2024-10-20 21:04:19 [INFO]: Epoch 031 - training loss: 0.2140\n",
      "2024-10-20 21:04:19 [INFO]: Epoch 032 - training loss: 0.2018\n",
      "2024-10-20 21:04:19 [INFO]: Epoch 033 - training loss: 0.1425\n",
      "2024-10-20 21:04:20 [INFO]: Epoch 034 - training loss: 0.2057\n",
      "2024-10-20 21:04:20 [INFO]: Epoch 035 - training loss: 0.1926\n",
      "2024-10-20 21:04:20 [INFO]: Epoch 036 - training loss: 0.1399\n",
      "2024-10-20 21:04:20 [INFO]: Epoch 037 - training loss: 0.1609\n",
      "2024-10-20 21:04:20 [INFO]: Epoch 038 - training loss: 0.2172\n",
      "2024-10-20 21:04:20 [INFO]: Epoch 039 - training loss: 0.1158\n",
      "2024-10-20 21:04:20 [INFO]: Epoch 040 - training loss: 0.2419\n",
      "2024-10-20 21:04:21 [INFO]: Epoch 041 - training loss: 0.2465\n",
      "2024-10-20 21:04:21 [INFO]: Epoch 042 - training loss: 0.1934\n",
      "2024-10-20 21:04:21 [INFO]: Epoch 043 - training loss: 0.1418\n",
      "2024-10-20 21:04:21 [INFO]: Epoch 044 - training loss: 0.2702\n",
      "2024-10-20 21:04:21 [INFO]: Epoch 045 - training loss: 0.2263\n",
      "2024-10-20 21:04:21 [INFO]: Epoch 046 - training loss: 0.1472\n",
      "2024-10-20 21:04:21 [INFO]: Epoch 047 - training loss: 0.2552\n",
      "2024-10-20 21:04:22 [INFO]: Epoch 048 - training loss: 0.1749\n",
      "2024-10-20 21:04:22 [INFO]: Epoch 049 - training loss: 0.1531\n",
      "2024-10-20 21:04:22 [INFO]: Epoch 050 - training loss: 0.1267\n",
      "2024-10-20 21:04:22 [INFO]: Epoch 051 - training loss: 0.2246\n",
      "2024-10-20 21:04:22 [INFO]: Epoch 052 - training loss: 0.1471\n",
      "2024-10-20 21:04:22 [INFO]: Epoch 053 - training loss: 0.1490\n",
      "2024-10-20 21:04:22 [INFO]: Epoch 054 - training loss: 0.1346\n",
      "2024-10-20 21:04:23 [INFO]: Epoch 055 - training loss: 0.1956\n",
      "2024-10-20 21:04:23 [INFO]: Epoch 056 - training loss: 0.1118\n",
      "2024-10-20 21:04:23 [INFO]: Epoch 057 - training loss: 0.2586\n",
      "2024-10-20 21:04:23 [INFO]: Epoch 058 - training loss: 0.2200\n",
      "2024-10-20 21:04:23 [INFO]: Epoch 059 - training loss: 0.2673\n",
      "2024-10-20 21:04:23 [INFO]: Epoch 060 - training loss: 0.2726\n",
      "2024-10-20 21:04:24 [INFO]: Epoch 061 - training loss: 0.1701\n",
      "2024-10-20 21:04:24 [INFO]: Epoch 062 - training loss: 0.2514\n",
      "2024-10-20 21:04:24 [INFO]: Epoch 063 - training loss: 0.1218\n",
      "2024-10-20 21:04:24 [INFO]: Epoch 064 - training loss: 0.2547\n",
      "2024-10-20 21:04:24 [INFO]: Epoch 065 - training loss: 0.2058\n",
      "2024-10-20 21:04:24 [INFO]: Epoch 066 - training loss: 0.1205\n",
      "2024-10-20 21:04:24 [INFO]: Epoch 067 - training loss: 0.1230\n",
      "2024-10-20 21:04:25 [INFO]: Epoch 068 - training loss: 0.1923\n",
      "2024-10-20 21:04:25 [INFO]: Epoch 069 - training loss: 0.1440\n",
      "2024-10-20 21:04:25 [INFO]: Epoch 070 - training loss: 0.1602\n",
      "2024-10-20 21:04:25 [INFO]: Epoch 071 - training loss: 0.2341\n",
      "2024-10-20 21:04:25 [INFO]: Epoch 072 - training loss: 0.1326\n",
      "2024-10-20 21:04:25 [INFO]: Epoch 073 - training loss: 0.1891\n",
      "2024-10-20 21:04:25 [INFO]: Epoch 074 - training loss: 0.1774\n",
      "2024-10-20 21:04:26 [INFO]: Epoch 075 - training loss: 0.1221\n",
      "2024-10-20 21:04:26 [INFO]: Epoch 076 - training loss: 0.2037\n",
      "2024-10-20 21:04:26 [INFO]: Epoch 077 - training loss: 0.1907\n",
      "2024-10-20 21:04:26 [INFO]: Epoch 078 - training loss: 0.1733\n",
      "2024-10-20 21:04:26 [INFO]: Epoch 079 - training loss: 0.1561\n",
      "2024-10-20 21:04:26 [INFO]: Epoch 080 - training loss: 0.2823\n",
      "2024-10-20 21:04:27 [INFO]: Epoch 081 - training loss: 0.1834\n",
      "2024-10-20 21:04:27 [INFO]: Epoch 082 - training loss: 0.3205\n",
      "2024-10-20 21:04:27 [INFO]: Epoch 083 - training loss: 0.2858\n",
      "2024-10-20 21:04:27 [INFO]: Epoch 084 - training loss: 0.3117\n",
      "2024-10-20 21:04:27 [INFO]: Epoch 085 - training loss: 0.1448\n",
      "2024-10-20 21:04:27 [INFO]: Epoch 086 - training loss: 0.3163\n",
      "2024-10-20 21:04:27 [INFO]: Epoch 087 - training loss: 0.1529\n",
      "2024-10-20 21:04:28 [INFO]: Epoch 088 - training loss: 0.1711\n",
      "2024-10-20 21:04:28 [INFO]: Epoch 089 - training loss: 0.2103\n",
      "2024-10-20 21:04:28 [INFO]: Epoch 090 - training loss: 0.1490\n",
      "2024-10-20 21:04:28 [INFO]: Epoch 091 - training loss: 0.2491\n",
      "2024-10-20 21:04:28 [INFO]: Epoch 092 - training loss: 0.1555\n",
      "2024-10-20 21:04:28 [INFO]: Epoch 093 - training loss: 0.1974\n",
      "2024-10-20 21:04:28 [INFO]: Epoch 094 - training loss: 0.2112\n",
      "2024-10-20 21:04:29 [INFO]: Epoch 095 - training loss: 0.1859\n",
      "2024-10-20 21:04:29 [INFO]: Epoch 096 - training loss: 0.1542\n",
      "2024-10-20 21:04:29 [INFO]: Epoch 097 - training loss: 0.1592\n",
      "2024-10-20 21:04:29 [INFO]: Epoch 098 - training loss: 0.2760\n",
      "2024-10-20 21:04:29 [INFO]: Epoch 099 - training loss: 0.2556\n",
      "2024-10-20 21:04:29 [INFO]: Epoch 100 - training loss: 0.2775\n",
      "2024-10-20 21:04:29 [INFO]: Finished training. The best model is from epoch#56.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:08:19 [INFO]: Epoch 001 - training loss: 0.2883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 2/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:08:19 [INFO]: Epoch 002 - training loss: 0.2717\n",
      "2024-10-20 21:08:19 [INFO]: Epoch 003 - training loss: 0.2388\n",
      "2024-10-20 21:08:19 [INFO]: Epoch 004 - training loss: 0.2221\n",
      "2024-10-20 21:08:19 [INFO]: Epoch 005 - training loss: 0.1722\n",
      "2024-10-20 21:08:19 [INFO]: Epoch 006 - training loss: 0.3317\n",
      "2024-10-20 21:08:19 [INFO]: Epoch 007 - training loss: 0.1438\n",
      "2024-10-20 21:08:20 [INFO]: Epoch 008 - training loss: 0.2268\n",
      "2024-10-20 21:08:20 [INFO]: Epoch 009 - training loss: 0.1312\n",
      "2024-10-20 21:08:20 [INFO]: Epoch 010 - training loss: 0.2458\n",
      "2024-10-20 21:08:20 [INFO]: Epoch 011 - training loss: 0.1727\n",
      "2024-10-20 21:08:20 [INFO]: Epoch 012 - training loss: 0.1653\n",
      "2024-10-20 21:08:20 [INFO]: Epoch 013 - training loss: 0.2252\n",
      "2024-10-20 21:08:21 [INFO]: Epoch 014 - training loss: 0.2084\n",
      "2024-10-20 21:08:21 [INFO]: Epoch 015 - training loss: 0.1318\n",
      "2024-10-20 21:08:21 [INFO]: Epoch 016 - training loss: 0.2313\n",
      "2024-10-20 21:08:21 [INFO]: Epoch 017 - training loss: 0.2088\n",
      "2024-10-20 21:08:21 [INFO]: Epoch 018 - training loss: 0.2172\n",
      "2024-10-20 21:08:21 [INFO]: Epoch 019 - training loss: 0.1465\n",
      "2024-10-20 21:08:21 [INFO]: Epoch 020 - training loss: 0.1650\n",
      "2024-10-20 21:08:22 [INFO]: Epoch 021 - training loss: 0.1986\n",
      "2024-10-20 21:08:22 [INFO]: Epoch 022 - training loss: 0.1816\n",
      "2024-10-20 21:08:22 [INFO]: Epoch 023 - training loss: 0.3026\n",
      "2024-10-20 21:08:22 [INFO]: Epoch 024 - training loss: 0.2689\n",
      "2024-10-20 21:08:22 [INFO]: Epoch 025 - training loss: 0.2029\n",
      "2024-10-20 21:08:22 [INFO]: Epoch 026 - training loss: 0.1870\n",
      "2024-10-20 21:08:22 [INFO]: Epoch 027 - training loss: 0.1533\n",
      "2024-10-20 21:08:23 [INFO]: Epoch 028 - training loss: 0.1584\n",
      "2024-10-20 21:08:23 [INFO]: Epoch 029 - training loss: 0.2249\n",
      "2024-10-20 21:08:23 [INFO]: Epoch 030 - training loss: 0.2030\n",
      "2024-10-20 21:08:23 [INFO]: Epoch 031 - training loss: 0.2026\n",
      "2024-10-20 21:08:23 [INFO]: Epoch 032 - training loss: 0.2650\n",
      "2024-10-20 21:08:23 [INFO]: Epoch 033 - training loss: 0.1771\n",
      "2024-10-20 21:08:23 [INFO]: Epoch 034 - training loss: 0.1243\n",
      "2024-10-20 21:08:24 [INFO]: Epoch 035 - training loss: 0.2182\n",
      "2024-10-20 21:08:24 [INFO]: Epoch 036 - training loss: 0.2142\n",
      "2024-10-20 21:08:24 [INFO]: Epoch 037 - training loss: 0.3206\n",
      "2024-10-20 21:08:24 [INFO]: Epoch 038 - training loss: 0.1658\n",
      "2024-10-20 21:08:24 [INFO]: Epoch 039 - training loss: 0.2445\n",
      "2024-10-20 21:08:24 [INFO]: Epoch 040 - training loss: 0.1382\n",
      "2024-10-20 21:08:25 [INFO]: Epoch 041 - training loss: 0.2262\n",
      "2024-10-20 21:08:25 [INFO]: Epoch 042 - training loss: 0.1889\n",
      "2024-10-20 21:08:25 [INFO]: Epoch 043 - training loss: 0.2167\n",
      "2024-10-20 21:08:25 [INFO]: Epoch 044 - training loss: 0.1846\n",
      "2024-10-20 21:08:25 [INFO]: Epoch 045 - training loss: 0.2573\n",
      "2024-10-20 21:08:25 [INFO]: Epoch 046 - training loss: 0.2504\n",
      "2024-10-20 21:08:25 [INFO]: Epoch 047 - training loss: 0.2522\n",
      "2024-10-20 21:08:26 [INFO]: Epoch 048 - training loss: 0.1800\n",
      "2024-10-20 21:08:26 [INFO]: Epoch 049 - training loss: 0.1731\n",
      "2024-10-20 21:08:26 [INFO]: Epoch 050 - training loss: 0.1831\n",
      "2024-10-20 21:08:26 [INFO]: Epoch 051 - training loss: 0.2506\n",
      "2024-10-20 21:08:26 [INFO]: Epoch 052 - training loss: 0.1543\n",
      "2024-10-20 21:08:26 [INFO]: Epoch 053 - training loss: 0.1798\n",
      "2024-10-20 21:08:26 [INFO]: Epoch 054 - training loss: 0.1935\n",
      "2024-10-20 21:08:27 [INFO]: Epoch 055 - training loss: 0.2445\n",
      "2024-10-20 21:08:27 [INFO]: Epoch 056 - training loss: 0.2162\n",
      "2024-10-20 21:08:27 [INFO]: Epoch 057 - training loss: 0.1701\n",
      "2024-10-20 21:08:27 [INFO]: Epoch 058 - training loss: 0.1533\n",
      "2024-10-20 21:08:27 [INFO]: Epoch 059 - training loss: 0.1475\n",
      "2024-10-20 21:08:27 [INFO]: Epoch 060 - training loss: 0.2450\n",
      "2024-10-20 21:08:28 [INFO]: Epoch 061 - training loss: 0.2678\n",
      "2024-10-20 21:08:28 [INFO]: Epoch 062 - training loss: 0.1214\n",
      "2024-10-20 21:08:28 [INFO]: Epoch 063 - training loss: 0.2105\n",
      "2024-10-20 21:08:28 [INFO]: Epoch 064 - training loss: 0.3036\n",
      "2024-10-20 21:08:28 [INFO]: Epoch 065 - training loss: 0.1357\n",
      "2024-10-20 21:08:28 [INFO]: Epoch 066 - training loss: 0.2136\n",
      "2024-10-20 21:08:28 [INFO]: Epoch 067 - training loss: 0.2890\n",
      "2024-10-20 21:08:29 [INFO]: Epoch 068 - training loss: 0.2254\n",
      "2024-10-20 21:08:29 [INFO]: Epoch 069 - training loss: 0.1488\n",
      "2024-10-20 21:08:29 [INFO]: Epoch 070 - training loss: 0.2198\n",
      "2024-10-20 21:08:29 [INFO]: Epoch 071 - training loss: 0.1898\n",
      "2024-10-20 21:08:29 [INFO]: Epoch 072 - training loss: 0.1632\n",
      "2024-10-20 21:08:29 [INFO]: Epoch 073 - training loss: 0.2019\n",
      "2024-10-20 21:08:29 [INFO]: Epoch 074 - training loss: 0.2355\n",
      "2024-10-20 21:08:30 [INFO]: Epoch 075 - training loss: 0.2715\n",
      "2024-10-20 21:08:30 [INFO]: Epoch 076 - training loss: 0.2216\n",
      "2024-10-20 21:08:30 [INFO]: Epoch 077 - training loss: 0.2057\n",
      "2024-10-20 21:08:30 [INFO]: Epoch 078 - training loss: 0.1434\n",
      "2024-10-20 21:08:30 [INFO]: Epoch 079 - training loss: 0.2244\n",
      "2024-10-20 21:08:30 [INFO]: Epoch 080 - training loss: 0.2270\n",
      "2024-10-20 21:08:31 [INFO]: Epoch 081 - training loss: 0.2173\n",
      "2024-10-20 21:08:31 [INFO]: Epoch 082 - training loss: 0.2405\n",
      "2024-10-20 21:08:31 [INFO]: Epoch 083 - training loss: 0.1969\n",
      "2024-10-20 21:08:31 [INFO]: Epoch 084 - training loss: 0.2816\n",
      "2024-10-20 21:08:31 [INFO]: Epoch 085 - training loss: 0.1245\n",
      "2024-10-20 21:08:31 [INFO]: Epoch 086 - training loss: 0.2224\n",
      "2024-10-20 21:08:31 [INFO]: Epoch 087 - training loss: 0.2319\n",
      "2024-10-20 21:08:32 [INFO]: Epoch 088 - training loss: 0.1377\n",
      "2024-10-20 21:08:32 [INFO]: Epoch 089 - training loss: 0.1899\n",
      "2024-10-20 21:08:32 [INFO]: Epoch 090 - training loss: 0.1845\n",
      "2024-10-20 21:08:32 [INFO]: Epoch 091 - training loss: 0.2278\n",
      "2024-10-20 21:08:32 [INFO]: Epoch 092 - training loss: 0.1696\n",
      "2024-10-20 21:08:32 [INFO]: Epoch 093 - training loss: 0.1906\n",
      "2024-10-20 21:08:32 [INFO]: Epoch 094 - training loss: 0.2013\n",
      "2024-10-20 21:08:33 [INFO]: Epoch 095 - training loss: 0.1696\n",
      "2024-10-20 21:08:33 [INFO]: Epoch 096 - training loss: 0.1771\n",
      "2024-10-20 21:08:33 [INFO]: Epoch 097 - training loss: 0.2444\n",
      "2024-10-20 21:08:33 [INFO]: Epoch 098 - training loss: 0.1549\n",
      "2024-10-20 21:08:33 [INFO]: Epoch 099 - training loss: 0.1902\n",
      "2024-10-20 21:08:33 [INFO]: Epoch 100 - training loss: 0.1865\n",
      "2024-10-20 21:08:33 [INFO]: Finished training. The best model is from epoch#62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:12:22 [INFO]: Epoch 001 - training loss: 0.2071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 3/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:12:22 [INFO]: Epoch 002 - training loss: 0.1538\n",
      "2024-10-20 21:12:22 [INFO]: Epoch 003 - training loss: 0.2040\n",
      "2024-10-20 21:12:22 [INFO]: Epoch 004 - training loss: 0.2706\n",
      "2024-10-20 21:12:22 [INFO]: Epoch 005 - training loss: 0.2321\n",
      "2024-10-20 21:12:23 [INFO]: Epoch 006 - training loss: 0.1410\n",
      "2024-10-20 21:12:23 [INFO]: Epoch 007 - training loss: 0.2554\n",
      "2024-10-20 21:12:23 [INFO]: Epoch 008 - training loss: 0.1603\n",
      "2024-10-20 21:12:23 [INFO]: Epoch 009 - training loss: 0.0977\n",
      "2024-10-20 21:12:23 [INFO]: Epoch 010 - training loss: 0.1363\n",
      "2024-10-20 21:12:23 [INFO]: Epoch 011 - training loss: 0.1481\n",
      "2024-10-20 21:12:24 [INFO]: Epoch 012 - training loss: 0.1827\n",
      "2024-10-20 21:12:24 [INFO]: Epoch 013 - training loss: 0.0939\n",
      "2024-10-20 21:12:24 [INFO]: Epoch 014 - training loss: 0.1526\n",
      "2024-10-20 21:12:24 [INFO]: Epoch 015 - training loss: 0.2443\n",
      "2024-10-20 21:12:24 [INFO]: Epoch 016 - training loss: 0.1676\n",
      "2024-10-20 21:12:24 [INFO]: Epoch 017 - training loss: 0.2226\n",
      "2024-10-20 21:12:24 [INFO]: Epoch 018 - training loss: 0.1744\n",
      "2024-10-20 21:12:25 [INFO]: Epoch 019 - training loss: 0.1814\n",
      "2024-10-20 21:12:25 [INFO]: Epoch 020 - training loss: 0.1906\n",
      "2024-10-20 21:12:25 [INFO]: Epoch 021 - training loss: 0.1535\n",
      "2024-10-20 21:12:25 [INFO]: Epoch 022 - training loss: 0.1806\n",
      "2024-10-20 21:12:25 [INFO]: Epoch 023 - training loss: 0.2095\n",
      "2024-10-20 21:12:25 [INFO]: Epoch 024 - training loss: 0.2117\n",
      "2024-10-20 21:12:25 [INFO]: Epoch 025 - training loss: 0.3072\n",
      "2024-10-20 21:12:26 [INFO]: Epoch 026 - training loss: 0.2554\n",
      "2024-10-20 21:12:26 [INFO]: Epoch 027 - training loss: 0.1775\n",
      "2024-10-20 21:12:26 [INFO]: Epoch 028 - training loss: 0.2176\n",
      "2024-10-20 21:12:26 [INFO]: Epoch 029 - training loss: 0.1957\n",
      "2024-10-20 21:12:26 [INFO]: Epoch 030 - training loss: 0.1856\n",
      "2024-10-20 21:12:26 [INFO]: Epoch 031 - training loss: 0.2410\n",
      "2024-10-20 21:12:27 [INFO]: Epoch 032 - training loss: 0.2968\n",
      "2024-10-20 21:12:27 [INFO]: Epoch 033 - training loss: 0.1830\n",
      "2024-10-20 21:12:27 [INFO]: Epoch 034 - training loss: 0.1096\n",
      "2024-10-20 21:12:27 [INFO]: Epoch 035 - training loss: 0.2073\n",
      "2024-10-20 21:12:27 [INFO]: Epoch 036 - training loss: 0.2376\n",
      "2024-10-20 21:12:27 [INFO]: Epoch 037 - training loss: 0.1387\n",
      "2024-10-20 21:12:27 [INFO]: Epoch 038 - training loss: 0.2062\n",
      "2024-10-20 21:12:28 [INFO]: Epoch 039 - training loss: 0.2149\n",
      "2024-10-20 21:12:28 [INFO]: Epoch 040 - training loss: 0.1929\n",
      "2024-10-20 21:12:28 [INFO]: Epoch 041 - training loss: 0.2054\n",
      "2024-10-20 21:12:28 [INFO]: Epoch 042 - training loss: 0.2039\n",
      "2024-10-20 21:12:28 [INFO]: Epoch 043 - training loss: 0.2010\n",
      "2024-10-20 21:12:28 [INFO]: Epoch 044 - training loss: 0.2232\n",
      "2024-10-20 21:12:28 [INFO]: Epoch 045 - training loss: 0.1452\n",
      "2024-10-20 21:12:29 [INFO]: Epoch 046 - training loss: 0.1588\n",
      "2024-10-20 21:12:29 [INFO]: Epoch 047 - training loss: 0.1578\n",
      "2024-10-20 21:12:29 [INFO]: Epoch 048 - training loss: 0.1673\n",
      "2024-10-20 21:12:29 [INFO]: Epoch 049 - training loss: 0.2116\n",
      "2024-10-20 21:12:29 [INFO]: Epoch 050 - training loss: 0.1497\n",
      "2024-10-20 21:12:29 [INFO]: Epoch 051 - training loss: 0.3092\n",
      "2024-10-20 21:12:29 [INFO]: Epoch 052 - training loss: 0.1257\n",
      "2024-10-20 21:12:30 [INFO]: Epoch 053 - training loss: 0.1898\n",
      "2024-10-20 21:12:30 [INFO]: Epoch 054 - training loss: 0.2234\n",
      "2024-10-20 21:12:30 [INFO]: Epoch 055 - training loss: 0.2051\n",
      "2024-10-20 21:12:30 [INFO]: Epoch 056 - training loss: 0.2320\n",
      "2024-10-20 21:12:30 [INFO]: Epoch 057 - training loss: 0.1739\n",
      "2024-10-20 21:12:30 [INFO]: Epoch 058 - training loss: 0.1737\n",
      "2024-10-20 21:12:31 [INFO]: Epoch 059 - training loss: 0.2519\n",
      "2024-10-20 21:12:31 [INFO]: Epoch 060 - training loss: 0.2287\n",
      "2024-10-20 21:12:31 [INFO]: Epoch 061 - training loss: 0.1256\n",
      "2024-10-20 21:12:31 [INFO]: Epoch 062 - training loss: 0.1975\n",
      "2024-10-20 21:12:31 [INFO]: Epoch 063 - training loss: 0.1335\n",
      "2024-10-20 21:12:31 [INFO]: Epoch 064 - training loss: 0.1719\n",
      "2024-10-20 21:12:31 [INFO]: Epoch 065 - training loss: 0.1695\n",
      "2024-10-20 21:12:32 [INFO]: Epoch 066 - training loss: 0.1347\n",
      "2024-10-20 21:12:32 [INFO]: Epoch 067 - training loss: 0.1617\n",
      "2024-10-20 21:12:32 [INFO]: Epoch 068 - training loss: 0.1705\n",
      "2024-10-20 21:12:32 [INFO]: Epoch 069 - training loss: 0.1637\n",
      "2024-10-20 21:12:32 [INFO]: Epoch 070 - training loss: 0.1468\n",
      "2024-10-20 21:12:32 [INFO]: Epoch 071 - training loss: 0.1527\n",
      "2024-10-20 21:12:33 [INFO]: Epoch 072 - training loss: 0.1497\n",
      "2024-10-20 21:12:33 [INFO]: Epoch 073 - training loss: 0.2190\n",
      "2024-10-20 21:12:33 [INFO]: Epoch 074 - training loss: 0.1448\n",
      "2024-10-20 21:12:33 [INFO]: Epoch 075 - training loss: 0.1327\n",
      "2024-10-20 21:12:33 [INFO]: Epoch 076 - training loss: 0.1356\n",
      "2024-10-20 21:12:33 [INFO]: Epoch 077 - training loss: 0.1721\n",
      "2024-10-20 21:12:33 [INFO]: Epoch 078 - training loss: 0.3203\n",
      "2024-10-20 21:12:34 [INFO]: Epoch 079 - training loss: 0.1283\n",
      "2024-10-20 21:12:34 [INFO]: Epoch 080 - training loss: 0.1571\n",
      "2024-10-20 21:12:34 [INFO]: Epoch 081 - training loss: 0.2205\n",
      "2024-10-20 21:12:34 [INFO]: Epoch 082 - training loss: 0.1439\n",
      "2024-10-20 21:12:34 [INFO]: Epoch 083 - training loss: 0.1809\n",
      "2024-10-20 21:12:34 [INFO]: Epoch 084 - training loss: 0.2891\n",
      "2024-10-20 21:12:34 [INFO]: Epoch 085 - training loss: 0.2179\n",
      "2024-10-20 21:12:35 [INFO]: Epoch 086 - training loss: 0.1914\n",
      "2024-10-20 21:12:35 [INFO]: Epoch 087 - training loss: 0.1216\n",
      "2024-10-20 21:12:35 [INFO]: Epoch 088 - training loss: 0.1464\n",
      "2024-10-20 21:12:35 [INFO]: Epoch 089 - training loss: 0.1937\n",
      "2024-10-20 21:12:35 [INFO]: Epoch 090 - training loss: 0.1775\n",
      "2024-10-20 21:12:35 [INFO]: Epoch 091 - training loss: 0.2577\n",
      "2024-10-20 21:12:36 [INFO]: Epoch 092 - training loss: 0.1376\n",
      "2024-10-20 21:12:36 [INFO]: Epoch 093 - training loss: 0.1434\n",
      "2024-10-20 21:12:36 [INFO]: Epoch 094 - training loss: 0.1869\n",
      "2024-10-20 21:12:36 [INFO]: Epoch 095 - training loss: 0.1399\n",
      "2024-10-20 21:12:36 [INFO]: Epoch 096 - training loss: 0.1732\n",
      "2024-10-20 21:12:36 [INFO]: Epoch 097 - training loss: 0.1923\n",
      "2024-10-20 21:12:36 [INFO]: Epoch 098 - training loss: 0.1882\n",
      "2024-10-20 21:12:37 [INFO]: Epoch 099 - training loss: 0.2921\n",
      "2024-10-20 21:12:37 [INFO]: Epoch 100 - training loss: 0.2334\n",
      "2024-10-20 21:12:37 [INFO]: Finished training. The best model is from epoch#13.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:16:25 [INFO]: Epoch 001 - training loss: 0.2712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 4/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:16:25 [INFO]: Epoch 002 - training loss: 0.2041\n",
      "2024-10-20 21:16:25 [INFO]: Epoch 003 - training loss: 0.1128\n",
      "2024-10-20 21:16:25 [INFO]: Epoch 004 - training loss: 0.2400\n",
      "2024-10-20 21:16:26 [INFO]: Epoch 005 - training loss: 0.2355\n",
      "2024-10-20 21:16:26 [INFO]: Epoch 006 - training loss: 0.1542\n",
      "2024-10-20 21:16:26 [INFO]: Epoch 007 - training loss: 0.2581\n",
      "2024-10-20 21:16:26 [INFO]: Epoch 008 - training loss: 0.2118\n",
      "2024-10-20 21:16:26 [INFO]: Epoch 009 - training loss: 0.1883\n",
      "2024-10-20 21:16:26 [INFO]: Epoch 010 - training loss: 0.1609\n",
      "2024-10-20 21:16:27 [INFO]: Epoch 011 - training loss: 0.2833\n",
      "2024-10-20 21:16:27 [INFO]: Epoch 012 - training loss: 0.2256\n",
      "2024-10-20 21:16:27 [INFO]: Epoch 013 - training loss: 0.1001\n",
      "2024-10-20 21:16:27 [INFO]: Epoch 014 - training loss: 0.1539\n",
      "2024-10-20 21:16:27 [INFO]: Epoch 015 - training loss: 0.1437\n",
      "2024-10-20 21:16:27 [INFO]: Epoch 016 - training loss: 0.1824\n",
      "2024-10-20 21:16:27 [INFO]: Epoch 017 - training loss: 0.2157\n",
      "2024-10-20 21:16:28 [INFO]: Epoch 018 - training loss: 0.2855\n",
      "2024-10-20 21:16:28 [INFO]: Epoch 019 - training loss: 0.2584\n",
      "2024-10-20 21:16:28 [INFO]: Epoch 020 - training loss: 0.1714\n",
      "2024-10-20 21:16:28 [INFO]: Epoch 021 - training loss: 0.2670\n",
      "2024-10-20 21:16:28 [INFO]: Epoch 022 - training loss: 0.2074\n",
      "2024-10-20 21:16:28 [INFO]: Epoch 023 - training loss: 0.1809\n",
      "2024-10-20 21:16:29 [INFO]: Epoch 024 - training loss: 0.2226\n",
      "2024-10-20 21:16:29 [INFO]: Epoch 025 - training loss: 0.1698\n",
      "2024-10-20 21:16:29 [INFO]: Epoch 026 - training loss: 0.1974\n",
      "2024-10-20 21:16:29 [INFO]: Epoch 027 - training loss: 0.1161\n",
      "2024-10-20 21:16:29 [INFO]: Epoch 028 - training loss: 0.1886\n",
      "2024-10-20 21:16:29 [INFO]: Epoch 029 - training loss: 0.2448\n",
      "2024-10-20 21:16:29 [INFO]: Epoch 030 - training loss: 0.1793\n",
      "2024-10-20 21:16:30 [INFO]: Epoch 031 - training loss: 0.2059\n",
      "2024-10-20 21:16:30 [INFO]: Epoch 032 - training loss: 0.2563\n",
      "2024-10-20 21:16:30 [INFO]: Epoch 033 - training loss: 0.1721\n",
      "2024-10-20 21:16:30 [INFO]: Epoch 034 - training loss: 0.1354\n",
      "2024-10-20 21:16:30 [INFO]: Epoch 035 - training loss: 0.1274\n",
      "2024-10-20 21:16:30 [INFO]: Epoch 036 - training loss: 0.1763\n",
      "2024-10-20 21:16:30 [INFO]: Epoch 037 - training loss: 0.2544\n",
      "2024-10-20 21:16:31 [INFO]: Epoch 038 - training loss: 0.1550\n",
      "2024-10-20 21:16:31 [INFO]: Epoch 039 - training loss: 0.1928\n",
      "2024-10-20 21:16:31 [INFO]: Epoch 040 - training loss: 0.1850\n",
      "2024-10-20 21:16:31 [INFO]: Epoch 041 - training loss: 0.2681\n",
      "2024-10-20 21:16:31 [INFO]: Epoch 042 - training loss: 0.1903\n",
      "2024-10-20 21:16:31 [INFO]: Epoch 043 - training loss: 0.1492\n",
      "2024-10-20 21:16:31 [INFO]: Epoch 044 - training loss: 0.1664\n",
      "2024-10-20 21:16:32 [INFO]: Epoch 045 - training loss: 0.1756\n",
      "2024-10-20 21:16:32 [INFO]: Epoch 046 - training loss: 0.1742\n",
      "2024-10-20 21:16:32 [INFO]: Epoch 047 - training loss: 0.1621\n",
      "2024-10-20 21:16:32 [INFO]: Epoch 048 - training loss: 0.1980\n",
      "2024-10-20 21:16:32 [INFO]: Epoch 049 - training loss: 0.3157\n",
      "2024-10-20 21:16:32 [INFO]: Epoch 050 - training loss: 0.1783\n",
      "2024-10-20 21:16:33 [INFO]: Epoch 051 - training loss: 0.1424\n",
      "2024-10-20 21:16:33 [INFO]: Epoch 052 - training loss: 0.2049\n",
      "2024-10-20 21:16:33 [INFO]: Epoch 053 - training loss: 0.1699\n",
      "2024-10-20 21:16:33 [INFO]: Epoch 054 - training loss: 0.1276\n",
      "2024-10-20 21:16:33 [INFO]: Epoch 055 - training loss: 0.2132\n",
      "2024-10-20 21:16:33 [INFO]: Epoch 056 - training loss: 0.1269\n",
      "2024-10-20 21:16:33 [INFO]: Epoch 057 - training loss: 0.1231\n",
      "2024-10-20 21:16:34 [INFO]: Epoch 058 - training loss: 0.2090\n",
      "2024-10-20 21:16:34 [INFO]: Epoch 059 - training loss: 0.1570\n",
      "2024-10-20 21:16:34 [INFO]: Epoch 060 - training loss: 0.2170\n",
      "2024-10-20 21:16:34 [INFO]: Epoch 061 - training loss: 0.1946\n",
      "2024-10-20 21:16:34 [INFO]: Epoch 062 - training loss: 0.2235\n",
      "2024-10-20 21:16:34 [INFO]: Epoch 063 - training loss: 0.1502\n",
      "2024-10-20 21:16:34 [INFO]: Epoch 064 - training loss: 0.2508\n",
      "2024-10-20 21:16:35 [INFO]: Epoch 065 - training loss: 0.2166\n",
      "2024-10-20 21:16:35 [INFO]: Epoch 066 - training loss: 0.1742\n",
      "2024-10-20 21:16:35 [INFO]: Epoch 067 - training loss: 0.2111\n",
      "2024-10-20 21:16:35 [INFO]: Epoch 068 - training loss: 0.2090\n",
      "2024-10-20 21:16:35 [INFO]: Epoch 069 - training loss: 0.2165\n",
      "2024-10-20 21:16:35 [INFO]: Epoch 070 - training loss: 0.2505\n",
      "2024-10-20 21:16:35 [INFO]: Epoch 071 - training loss: 0.1644\n",
      "2024-10-20 21:16:36 [INFO]: Epoch 072 - training loss: 0.2649\n",
      "2024-10-20 21:16:36 [INFO]: Epoch 073 - training loss: 0.2646\n",
      "2024-10-20 21:16:36 [INFO]: Epoch 074 - training loss: 0.1539\n",
      "2024-10-20 21:16:36 [INFO]: Epoch 075 - training loss: 0.1525\n",
      "2024-10-20 21:16:36 [INFO]: Epoch 076 - training loss: 0.2287\n",
      "2024-10-20 21:16:36 [INFO]: Epoch 077 - training loss: 0.1307\n",
      "2024-10-20 21:16:37 [INFO]: Epoch 078 - training loss: 0.2080\n",
      "2024-10-20 21:16:37 [INFO]: Epoch 079 - training loss: 0.2074\n",
      "2024-10-20 21:16:37 [INFO]: Epoch 080 - training loss: 0.1976\n",
      "2024-10-20 21:16:37 [INFO]: Epoch 081 - training loss: 0.2517\n",
      "2024-10-20 21:16:37 [INFO]: Epoch 082 - training loss: 0.2142\n",
      "2024-10-20 21:16:37 [INFO]: Epoch 083 - training loss: 0.1202\n",
      "2024-10-20 21:16:37 [INFO]: Epoch 084 - training loss: 0.1764\n",
      "2024-10-20 21:16:38 [INFO]: Epoch 085 - training loss: 0.2665\n",
      "2024-10-20 21:16:38 [INFO]: Epoch 086 - training loss: 0.2575\n",
      "2024-10-20 21:16:38 [INFO]: Epoch 087 - training loss: 0.1506\n",
      "2024-10-20 21:16:38 [INFO]: Epoch 088 - training loss: 0.2317\n",
      "2024-10-20 21:16:38 [INFO]: Epoch 089 - training loss: 0.1975\n",
      "2024-10-20 21:16:38 [INFO]: Epoch 090 - training loss: 0.1772\n",
      "2024-10-20 21:16:38 [INFO]: Epoch 091 - training loss: 0.3207\n",
      "2024-10-20 21:16:39 [INFO]: Epoch 092 - training loss: 0.1914\n",
      "2024-10-20 21:16:39 [INFO]: Epoch 093 - training loss: 0.1840\n",
      "2024-10-20 21:16:39 [INFO]: Epoch 094 - training loss: 0.2162\n",
      "2024-10-20 21:16:39 [INFO]: Epoch 095 - training loss: 0.1941\n",
      "2024-10-20 21:16:39 [INFO]: Epoch 096 - training loss: 0.1391\n",
      "2024-10-20 21:16:39 [INFO]: Epoch 097 - training loss: 0.1764\n",
      "2024-10-20 21:16:40 [INFO]: Epoch 098 - training loss: 0.1689\n",
      "2024-10-20 21:16:40 [INFO]: Epoch 099 - training loss: 0.1231\n",
      "2024-10-20 21:16:40 [INFO]: Epoch 100 - training loss: 0.2545\n",
      "2024-10-20 21:16:40 [INFO]: Finished training. The best model is from epoch#13.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:20:29 [INFO]: Epoch 001 - training loss: 0.2835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 5/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:20:29 [INFO]: Epoch 002 - training loss: 0.1646\n",
      "2024-10-20 21:20:29 [INFO]: Epoch 003 - training loss: 0.2665\n",
      "2024-10-20 21:20:29 [INFO]: Epoch 004 - training loss: 0.1316\n",
      "2024-10-20 21:20:29 [INFO]: Epoch 005 - training loss: 0.1354\n",
      "2024-10-20 21:20:29 [INFO]: Epoch 006 - training loss: 0.1848\n",
      "2024-10-20 21:20:29 [INFO]: Epoch 007 - training loss: 0.2518\n",
      "2024-10-20 21:20:30 [INFO]: Epoch 008 - training loss: 0.1776\n",
      "2024-10-20 21:20:30 [INFO]: Epoch 009 - training loss: 0.1838\n",
      "2024-10-20 21:20:30 [INFO]: Epoch 010 - training loss: 0.2839\n",
      "2024-10-20 21:20:30 [INFO]: Epoch 011 - training loss: 0.1246\n",
      "2024-10-20 21:20:30 [INFO]: Epoch 012 - training loss: 0.2765\n",
      "2024-10-20 21:20:30 [INFO]: Epoch 013 - training loss: 0.2054\n",
      "2024-10-20 21:20:31 [INFO]: Epoch 014 - training loss: 0.1649\n",
      "2024-10-20 21:20:31 [INFO]: Epoch 015 - training loss: 0.1343\n",
      "2024-10-20 21:20:31 [INFO]: Epoch 016 - training loss: 0.1517\n",
      "2024-10-20 21:20:31 [INFO]: Epoch 017 - training loss: 0.2937\n",
      "2024-10-20 21:20:31 [INFO]: Epoch 018 - training loss: 0.1802\n",
      "2024-10-20 21:20:31 [INFO]: Epoch 019 - training loss: 0.3432\n",
      "2024-10-20 21:20:31 [INFO]: Epoch 020 - training loss: 0.2395\n",
      "2024-10-20 21:20:32 [INFO]: Epoch 021 - training loss: 0.2694\n",
      "2024-10-20 21:20:32 [INFO]: Epoch 022 - training loss: 0.1163\n",
      "2024-10-20 21:20:32 [INFO]: Epoch 023 - training loss: 0.1704\n",
      "2024-10-20 21:20:32 [INFO]: Epoch 024 - training loss: 0.2318\n",
      "2024-10-20 21:20:32 [INFO]: Epoch 025 - training loss: 0.1783\n",
      "2024-10-20 21:20:32 [INFO]: Epoch 026 - training loss: 0.1515\n",
      "2024-10-20 21:20:33 [INFO]: Epoch 027 - training loss: 0.1983\n",
      "2024-10-20 21:20:33 [INFO]: Epoch 028 - training loss: 0.0983\n",
      "2024-10-20 21:20:33 [INFO]: Epoch 029 - training loss: 0.2601\n",
      "2024-10-20 21:20:33 [INFO]: Epoch 030 - training loss: 0.1839\n",
      "2024-10-20 21:20:33 [INFO]: Epoch 031 - training loss: 0.2489\n",
      "2024-10-20 21:20:33 [INFO]: Epoch 032 - training loss: 0.1268\n",
      "2024-10-20 21:20:33 [INFO]: Epoch 033 - training loss: 0.1468\n",
      "2024-10-20 21:20:34 [INFO]: Epoch 034 - training loss: 0.1439\n",
      "2024-10-20 21:20:34 [INFO]: Epoch 035 - training loss: 0.2519\n",
      "2024-10-20 21:20:34 [INFO]: Epoch 036 - training loss: 0.2418\n",
      "2024-10-20 21:20:34 [INFO]: Epoch 037 - training loss: 0.1662\n",
      "2024-10-20 21:20:34 [INFO]: Epoch 038 - training loss: 0.1556\n",
      "2024-10-20 21:20:34 [INFO]: Epoch 039 - training loss: 0.2309\n",
      "2024-10-20 21:20:34 [INFO]: Epoch 040 - training loss: 0.1919\n",
      "2024-10-20 21:20:35 [INFO]: Epoch 041 - training loss: 0.1720\n",
      "2024-10-20 21:20:35 [INFO]: Epoch 042 - training loss: 0.2036\n",
      "2024-10-20 21:20:35 [INFO]: Epoch 043 - training loss: 0.1645\n",
      "2024-10-20 21:20:35 [INFO]: Epoch 044 - training loss: 0.1464\n",
      "2024-10-20 21:20:35 [INFO]: Epoch 045 - training loss: 0.2316\n",
      "2024-10-20 21:20:35 [INFO]: Epoch 046 - training loss: 0.2303\n",
      "2024-10-20 21:20:36 [INFO]: Epoch 047 - training loss: 0.2025\n",
      "2024-10-20 21:20:36 [INFO]: Epoch 048 - training loss: 0.1755\n",
      "2024-10-20 21:20:36 [INFO]: Epoch 049 - training loss: 0.1896\n",
      "2024-10-20 21:20:36 [INFO]: Epoch 050 - training loss: 0.1413\n",
      "2024-10-20 21:20:36 [INFO]: Epoch 051 - training loss: 0.1569\n",
      "2024-10-20 21:20:36 [INFO]: Epoch 052 - training loss: 0.2680\n",
      "2024-10-20 21:20:36 [INFO]: Epoch 053 - training loss: 0.1312\n",
      "2024-10-20 21:20:37 [INFO]: Epoch 054 - training loss: 0.2355\n",
      "2024-10-20 21:20:37 [INFO]: Epoch 055 - training loss: 0.2463\n",
      "2024-10-20 21:20:37 [INFO]: Epoch 056 - training loss: 0.1420\n",
      "2024-10-20 21:20:37 [INFO]: Epoch 057 - training loss: 0.2805\n",
      "2024-10-20 21:20:37 [INFO]: Epoch 058 - training loss: 0.1185\n",
      "2024-10-20 21:20:37 [INFO]: Epoch 059 - training loss: 0.0986\n",
      "2024-10-20 21:20:37 [INFO]: Epoch 060 - training loss: 0.1674\n",
      "2024-10-20 21:20:38 [INFO]: Epoch 061 - training loss: 0.3127\n",
      "2024-10-20 21:20:38 [INFO]: Epoch 062 - training loss: 0.1691\n",
      "2024-10-20 21:20:38 [INFO]: Epoch 063 - training loss: 0.1526\n",
      "2024-10-20 21:20:38 [INFO]: Epoch 064 - training loss: 0.2249\n",
      "2024-10-20 21:20:38 [INFO]: Epoch 065 - training loss: 0.2453\n",
      "2024-10-20 21:20:38 [INFO]: Epoch 066 - training loss: 0.1485\n",
      "2024-10-20 21:20:39 [INFO]: Epoch 067 - training loss: 0.1879\n",
      "2024-10-20 21:20:39 [INFO]: Epoch 068 - training loss: 0.1724\n",
      "2024-10-20 21:20:39 [INFO]: Epoch 069 - training loss: 0.2715\n",
      "2024-10-20 21:20:39 [INFO]: Epoch 070 - training loss: 0.2985\n",
      "2024-10-20 21:20:39 [INFO]: Epoch 071 - training loss: 0.1549\n",
      "2024-10-20 21:20:39 [INFO]: Epoch 072 - training loss: 0.1870\n",
      "2024-10-20 21:20:39 [INFO]: Epoch 073 - training loss: 0.1876\n",
      "2024-10-20 21:20:40 [INFO]: Epoch 074 - training loss: 0.1622\n",
      "2024-10-20 21:20:40 [INFO]: Epoch 075 - training loss: 0.2088\n",
      "2024-10-20 21:20:40 [INFO]: Epoch 076 - training loss: 0.2132\n",
      "2024-10-20 21:20:40 [INFO]: Epoch 077 - training loss: 0.2369\n",
      "2024-10-20 21:20:40 [INFO]: Epoch 078 - training loss: 0.1750\n",
      "2024-10-20 21:20:40 [INFO]: Epoch 079 - training loss: 0.2131\n",
      "2024-10-20 21:20:40 [INFO]: Epoch 080 - training loss: 0.1814\n",
      "2024-10-20 21:20:41 [INFO]: Epoch 081 - training loss: 0.1900\n",
      "2024-10-20 21:20:41 [INFO]: Epoch 082 - training loss: 0.1501\n",
      "2024-10-20 21:20:41 [INFO]: Epoch 083 - training loss: 0.1255\n",
      "2024-10-20 21:20:41 [INFO]: Epoch 084 - training loss: 0.1189\n",
      "2024-10-20 21:20:41 [INFO]: Epoch 085 - training loss: 0.1467\n",
      "2024-10-20 21:20:41 [INFO]: Epoch 086 - training loss: 0.1668\n",
      "2024-10-20 21:20:42 [INFO]: Epoch 087 - training loss: 0.2519\n",
      "2024-10-20 21:20:42 [INFO]: Epoch 088 - training loss: 0.1159\n",
      "2024-10-20 21:20:42 [INFO]: Epoch 089 - training loss: 0.2254\n",
      "2024-10-20 21:20:42 [INFO]: Epoch 090 - training loss: 0.2064\n",
      "2024-10-20 21:20:42 [INFO]: Epoch 091 - training loss: 0.1106\n",
      "2024-10-20 21:20:42 [INFO]: Epoch 092 - training loss: 0.1054\n",
      "2024-10-20 21:20:42 [INFO]: Epoch 093 - training loss: 0.1591\n",
      "2024-10-20 21:20:43 [INFO]: Epoch 094 - training loss: 0.2032\n",
      "2024-10-20 21:20:43 [INFO]: Epoch 095 - training loss: 0.1494\n",
      "2024-10-20 21:20:43 [INFO]: Epoch 096 - training loss: 0.1127\n",
      "2024-10-20 21:20:43 [INFO]: Epoch 097 - training loss: 0.2425\n",
      "2024-10-20 21:20:43 [INFO]: Epoch 098 - training loss: 0.1621\n",
      "2024-10-20 21:20:43 [INFO]: Epoch 099 - training loss: 0.1255\n",
      "2024-10-20 21:20:43 [INFO]: Epoch 100 - training loss: 0.1400\n",
      "2024-10-20 21:20:43 [INFO]: Finished training. The best model is from epoch#28.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:24:33 [INFO]: Epoch 001 - training loss: 0.2319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 6/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:24:33 [INFO]: Epoch 002 - training loss: 0.1316\n",
      "2024-10-20 21:24:34 [INFO]: Epoch 003 - training loss: 0.1655\n",
      "2024-10-20 21:24:34 [INFO]: Epoch 004 - training loss: 0.1964\n",
      "2024-10-20 21:24:34 [INFO]: Epoch 005 - training loss: 0.1620\n",
      "2024-10-20 21:24:34 [INFO]: Epoch 006 - training loss: 0.2011\n",
      "2024-10-20 21:24:34 [INFO]: Epoch 007 - training loss: 0.2154\n",
      "2024-10-20 21:24:34 [INFO]: Epoch 008 - training loss: 0.1783\n",
      "2024-10-20 21:24:34 [INFO]: Epoch 009 - training loss: 0.4012\n",
      "2024-10-20 21:24:35 [INFO]: Epoch 010 - training loss: 0.1421\n",
      "2024-10-20 21:24:35 [INFO]: Epoch 011 - training loss: 0.1248\n",
      "2024-10-20 21:24:35 [INFO]: Epoch 012 - training loss: 0.1695\n",
      "2024-10-20 21:24:35 [INFO]: Epoch 013 - training loss: 0.1352\n",
      "2024-10-20 21:24:35 [INFO]: Epoch 014 - training loss: 0.1922\n",
      "2024-10-20 21:24:35 [INFO]: Epoch 015 - training loss: 0.1589\n",
      "2024-10-20 21:24:35 [INFO]: Epoch 016 - training loss: 0.1300\n",
      "2024-10-20 21:24:36 [INFO]: Epoch 017 - training loss: 0.2157\n",
      "2024-10-20 21:24:36 [INFO]: Epoch 018 - training loss: 0.2004\n",
      "2024-10-20 21:24:36 [INFO]: Epoch 019 - training loss: 0.1826\n",
      "2024-10-20 21:24:36 [INFO]: Epoch 020 - training loss: 0.1868\n",
      "2024-10-20 21:24:36 [INFO]: Epoch 021 - training loss: 0.2405\n",
      "2024-10-20 21:24:36 [INFO]: Epoch 022 - training loss: 0.2196\n",
      "2024-10-20 21:24:37 [INFO]: Epoch 023 - training loss: 0.2108\n",
      "2024-10-20 21:24:37 [INFO]: Epoch 024 - training loss: 0.1781\n",
      "2024-10-20 21:24:37 [INFO]: Epoch 025 - training loss: 0.1811\n",
      "2024-10-20 21:24:37 [INFO]: Epoch 026 - training loss: 0.1946\n",
      "2024-10-20 21:24:37 [INFO]: Epoch 027 - training loss: 0.1984\n",
      "2024-10-20 21:24:37 [INFO]: Epoch 028 - training loss: 0.1091\n",
      "2024-10-20 21:24:37 [INFO]: Epoch 029 - training loss: 0.0960\n",
      "2024-10-20 21:24:38 [INFO]: Epoch 030 - training loss: 0.1938\n",
      "2024-10-20 21:24:38 [INFO]: Epoch 031 - training loss: 0.1341\n",
      "2024-10-20 21:24:38 [INFO]: Epoch 032 - training loss: 0.1533\n",
      "2024-10-20 21:24:38 [INFO]: Epoch 033 - training loss: 0.2519\n",
      "2024-10-20 21:24:38 [INFO]: Epoch 034 - training loss: 0.1761\n",
      "2024-10-20 21:24:38 [INFO]: Epoch 035 - training loss: 0.1273\n",
      "2024-10-20 21:24:39 [INFO]: Epoch 036 - training loss: 0.1341\n",
      "2024-10-20 21:24:39 [INFO]: Epoch 037 - training loss: 0.1553\n",
      "2024-10-20 21:24:39 [INFO]: Epoch 038 - training loss: 0.1682\n",
      "2024-10-20 21:24:39 [INFO]: Epoch 039 - training loss: 0.1265\n",
      "2024-10-20 21:24:39 [INFO]: Epoch 040 - training loss: 0.1342\n",
      "2024-10-20 21:24:39 [INFO]: Epoch 041 - training loss: 0.1820\n",
      "2024-10-20 21:24:39 [INFO]: Epoch 042 - training loss: 0.1425\n",
      "2024-10-20 21:24:40 [INFO]: Epoch 043 - training loss: 0.1369\n",
      "2024-10-20 21:24:40 [INFO]: Epoch 044 - training loss: 0.1318\n",
      "2024-10-20 21:24:40 [INFO]: Epoch 045 - training loss: 0.2445\n",
      "2024-10-20 21:24:40 [INFO]: Epoch 046 - training loss: 0.2896\n",
      "2024-10-20 21:24:40 [INFO]: Epoch 047 - training loss: 0.1433\n",
      "2024-10-20 21:24:40 [INFO]: Epoch 048 - training loss: 0.1543\n",
      "2024-10-20 21:24:40 [INFO]: Epoch 049 - training loss: 0.1787\n",
      "2024-10-20 21:24:41 [INFO]: Epoch 050 - training loss: 0.1447\n",
      "2024-10-20 21:24:41 [INFO]: Epoch 051 - training loss: 0.2425\n",
      "2024-10-20 21:24:41 [INFO]: Epoch 052 - training loss: 0.1605\n",
      "2024-10-20 21:24:41 [INFO]: Epoch 053 - training loss: 0.1752\n",
      "2024-10-20 21:24:41 [INFO]: Epoch 054 - training loss: 0.2799\n",
      "2024-10-20 21:24:41 [INFO]: Epoch 055 - training loss: 0.1725\n",
      "2024-10-20 21:24:42 [INFO]: Epoch 056 - training loss: 0.2095\n",
      "2024-10-20 21:24:42 [INFO]: Epoch 057 - training loss: 0.2064\n",
      "2024-10-20 21:24:42 [INFO]: Epoch 058 - training loss: 0.1750\n",
      "2024-10-20 21:24:42 [INFO]: Epoch 059 - training loss: 0.1810\n",
      "2024-10-20 21:24:42 [INFO]: Epoch 060 - training loss: 0.2480\n",
      "2024-10-20 21:24:42 [INFO]: Epoch 061 - training loss: 0.2198\n",
      "2024-10-20 21:24:42 [INFO]: Epoch 062 - training loss: 0.1479\n",
      "2024-10-20 21:24:43 [INFO]: Epoch 063 - training loss: 0.1985\n",
      "2024-10-20 21:24:43 [INFO]: Epoch 064 - training loss: 0.1751\n",
      "2024-10-20 21:24:43 [INFO]: Epoch 065 - training loss: 0.1603\n",
      "2024-10-20 21:24:43 [INFO]: Epoch 066 - training loss: 0.1903\n",
      "2024-10-20 21:24:43 [INFO]: Epoch 067 - training loss: 0.2146\n",
      "2024-10-20 21:24:43 [INFO]: Epoch 068 - training loss: 0.1874\n",
      "2024-10-20 21:24:44 [INFO]: Epoch 069 - training loss: 0.1445\n",
      "2024-10-20 21:24:44 [INFO]: Epoch 070 - training loss: 0.2630\n",
      "2024-10-20 21:24:44 [INFO]: Epoch 071 - training loss: 0.1638\n",
      "2024-10-20 21:24:44 [INFO]: Epoch 072 - training loss: 0.2182\n",
      "2024-10-20 21:24:44 [INFO]: Epoch 073 - training loss: 0.1547\n",
      "2024-10-20 21:24:44 [INFO]: Epoch 074 - training loss: 0.1170\n",
      "2024-10-20 21:24:44 [INFO]: Epoch 075 - training loss: 0.1949\n",
      "2024-10-20 21:24:45 [INFO]: Epoch 076 - training loss: 0.1697\n",
      "2024-10-20 21:24:45 [INFO]: Epoch 077 - training loss: 0.1820\n",
      "2024-10-20 21:24:45 [INFO]: Epoch 078 - training loss: 0.2497\n",
      "2024-10-20 21:24:45 [INFO]: Epoch 079 - training loss: 0.1319\n",
      "2024-10-20 21:24:45 [INFO]: Epoch 080 - training loss: 0.1713\n",
      "2024-10-20 21:24:45 [INFO]: Epoch 081 - training loss: 0.1490\n",
      "2024-10-20 21:24:45 [INFO]: Epoch 082 - training loss: 0.2006\n",
      "2024-10-20 21:24:46 [INFO]: Epoch 083 - training loss: 0.1666\n",
      "2024-10-20 21:24:46 [INFO]: Epoch 084 - training loss: 0.1370\n",
      "2024-10-20 21:24:46 [INFO]: Epoch 085 - training loss: 0.2195\n",
      "2024-10-20 21:24:46 [INFO]: Epoch 086 - training loss: 0.1596\n",
      "2024-10-20 21:24:46 [INFO]: Epoch 087 - training loss: 0.1462\n",
      "2024-10-20 21:24:46 [INFO]: Epoch 088 - training loss: 0.4038\n",
      "2024-10-20 21:24:47 [INFO]: Epoch 089 - training loss: 0.1735\n",
      "2024-10-20 21:24:47 [INFO]: Epoch 090 - training loss: 0.2086\n",
      "2024-10-20 21:24:47 [INFO]: Epoch 091 - training loss: 0.1122\n",
      "2024-10-20 21:24:47 [INFO]: Epoch 092 - training loss: 0.2371\n",
      "2024-10-20 21:24:47 [INFO]: Epoch 093 - training loss: 0.1981\n",
      "2024-10-20 21:24:47 [INFO]: Epoch 094 - training loss: 0.3269\n",
      "2024-10-20 21:24:47 [INFO]: Epoch 095 - training loss: 0.1458\n",
      "2024-10-20 21:24:48 [INFO]: Epoch 096 - training loss: 0.1912\n",
      "2024-10-20 21:24:48 [INFO]: Epoch 097 - training loss: 0.1369\n",
      "2024-10-20 21:24:48 [INFO]: Epoch 098 - training loss: 0.1532\n",
      "2024-10-20 21:24:48 [INFO]: Epoch 099 - training loss: 0.1126\n",
      "2024-10-20 21:24:48 [INFO]: Epoch 100 - training loss: 0.1660\n",
      "2024-10-20 21:24:48 [INFO]: Finished training. The best model is from epoch#29.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:28:37 [INFO]: Epoch 001 - training loss: 0.1935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 7/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:28:37 [INFO]: Epoch 002 - training loss: 0.1463\n",
      "2024-10-20 21:28:37 [INFO]: Epoch 003 - training loss: 0.1773\n",
      "2024-10-20 21:28:37 [INFO]: Epoch 004 - training loss: 0.1617\n",
      "2024-10-20 21:28:38 [INFO]: Epoch 005 - training loss: 0.2548\n",
      "2024-10-20 21:28:38 [INFO]: Epoch 006 - training loss: 0.2121\n",
      "2024-10-20 21:28:38 [INFO]: Epoch 007 - training loss: 0.2575\n",
      "2024-10-20 21:28:38 [INFO]: Epoch 008 - training loss: 0.2279\n",
      "2024-10-20 21:28:38 [INFO]: Epoch 009 - training loss: 0.1226\n",
      "2024-10-20 21:28:38 [INFO]: Epoch 010 - training loss: 0.1922\n",
      "2024-10-20 21:28:38 [INFO]: Epoch 011 - training loss: 0.1428\n",
      "2024-10-20 21:28:39 [INFO]: Epoch 012 - training loss: 0.1667\n",
      "2024-10-20 21:28:39 [INFO]: Epoch 013 - training loss: 0.1581\n",
      "2024-10-20 21:28:39 [INFO]: Epoch 014 - training loss: 0.1356\n",
      "2024-10-20 21:28:39 [INFO]: Epoch 015 - training loss: 0.2651\n",
      "2024-10-20 21:28:39 [INFO]: Epoch 016 - training loss: 0.1767\n",
      "2024-10-20 21:28:39 [INFO]: Epoch 017 - training loss: 0.2526\n",
      "2024-10-20 21:28:40 [INFO]: Epoch 018 - training loss: 0.1881\n",
      "2024-10-20 21:28:40 [INFO]: Epoch 019 - training loss: 0.1564\n",
      "2024-10-20 21:28:40 [INFO]: Epoch 020 - training loss: 0.2079\n",
      "2024-10-20 21:28:40 [INFO]: Epoch 021 - training loss: 0.2550\n",
      "2024-10-20 21:28:40 [INFO]: Epoch 022 - training loss: 0.1586\n",
      "2024-10-20 21:28:40 [INFO]: Epoch 023 - training loss: 0.1104\n",
      "2024-10-20 21:28:40 [INFO]: Epoch 024 - training loss: 0.1158\n",
      "2024-10-20 21:28:41 [INFO]: Epoch 025 - training loss: 0.1701\n",
      "2024-10-20 21:28:41 [INFO]: Epoch 026 - training loss: 0.1298\n",
      "2024-10-20 21:28:41 [INFO]: Epoch 027 - training loss: 0.1720\n",
      "2024-10-20 21:28:41 [INFO]: Epoch 028 - training loss: 0.1673\n",
      "2024-10-20 21:28:41 [INFO]: Epoch 029 - training loss: 0.1669\n",
      "2024-10-20 21:28:41 [INFO]: Epoch 030 - training loss: 0.2563\n",
      "2024-10-20 21:28:41 [INFO]: Epoch 031 - training loss: 0.1799\n",
      "2024-10-20 21:28:42 [INFO]: Epoch 032 - training loss: 0.1240\n",
      "2024-10-20 21:28:42 [INFO]: Epoch 033 - training loss: 0.2998\n",
      "2024-10-20 21:28:42 [INFO]: Epoch 034 - training loss: 0.1874\n",
      "2024-10-20 21:28:42 [INFO]: Epoch 035 - training loss: 0.1494\n",
      "2024-10-20 21:28:42 [INFO]: Epoch 036 - training loss: 0.1602\n",
      "2024-10-20 21:28:42 [INFO]: Epoch 037 - training loss: 0.1664\n",
      "2024-10-20 21:28:42 [INFO]: Epoch 038 - training loss: 0.1729\n",
      "2024-10-20 21:28:43 [INFO]: Epoch 039 - training loss: 0.1413\n",
      "2024-10-20 21:28:43 [INFO]: Epoch 040 - training loss: 0.2045\n",
      "2024-10-20 21:28:43 [INFO]: Epoch 041 - training loss: 0.2046\n",
      "2024-10-20 21:28:43 [INFO]: Epoch 042 - training loss: 0.1853\n",
      "2024-10-20 21:28:43 [INFO]: Epoch 043 - training loss: 0.0956\n",
      "2024-10-20 21:28:43 [INFO]: Epoch 044 - training loss: 0.2280\n",
      "2024-10-20 21:28:44 [INFO]: Epoch 045 - training loss: 0.2271\n",
      "2024-10-20 21:28:44 [INFO]: Epoch 046 - training loss: 0.1715\n",
      "2024-10-20 21:28:44 [INFO]: Epoch 047 - training loss: 0.1713\n",
      "2024-10-20 21:28:44 [INFO]: Epoch 048 - training loss: 0.2634\n",
      "2024-10-20 21:28:44 [INFO]: Epoch 049 - training loss: 0.1604\n",
      "2024-10-20 21:28:44 [INFO]: Epoch 050 - training loss: 0.1199\n",
      "2024-10-20 21:28:44 [INFO]: Epoch 051 - training loss: 0.2297\n",
      "2024-10-20 21:28:45 [INFO]: Epoch 052 - training loss: 0.2319\n",
      "2024-10-20 21:28:45 [INFO]: Epoch 053 - training loss: 0.2624\n",
      "2024-10-20 21:28:45 [INFO]: Epoch 054 - training loss: 0.1890\n",
      "2024-10-20 21:28:45 [INFO]: Epoch 055 - training loss: 0.1558\n",
      "2024-10-20 21:28:45 [INFO]: Epoch 056 - training loss: 0.2058\n",
      "2024-10-20 21:28:45 [INFO]: Epoch 057 - training loss: 0.1854\n",
      "2024-10-20 21:28:45 [INFO]: Epoch 058 - training loss: 0.2132\n",
      "2024-10-20 21:28:46 [INFO]: Epoch 059 - training loss: 0.2402\n",
      "2024-10-20 21:28:46 [INFO]: Epoch 060 - training loss: 0.0961\n",
      "2024-10-20 21:28:46 [INFO]: Epoch 061 - training loss: 0.2001\n",
      "2024-10-20 21:28:46 [INFO]: Epoch 062 - training loss: 0.1608\n",
      "2024-10-20 21:28:46 [INFO]: Epoch 063 - training loss: 0.2191\n",
      "2024-10-20 21:28:46 [INFO]: Epoch 064 - training loss: 0.2493\n",
      "2024-10-20 21:28:46 [INFO]: Epoch 065 - training loss: 0.1447\n",
      "2024-10-20 21:28:47 [INFO]: Epoch 066 - training loss: 0.2260\n",
      "2024-10-20 21:28:47 [INFO]: Epoch 067 - training loss: 0.2150\n",
      "2024-10-20 21:28:47 [INFO]: Epoch 068 - training loss: 0.1297\n",
      "2024-10-20 21:28:47 [INFO]: Epoch 069 - training loss: 0.1930\n",
      "2024-10-20 21:28:47 [INFO]: Epoch 070 - training loss: 0.1413\n",
      "2024-10-20 21:28:47 [INFO]: Epoch 071 - training loss: 0.1784\n",
      "2024-10-20 21:28:48 [INFO]: Epoch 072 - training loss: 0.1715\n",
      "2024-10-20 21:28:48 [INFO]: Epoch 073 - training loss: 0.1280\n",
      "2024-10-20 21:28:48 [INFO]: Epoch 074 - training loss: 0.2009\n",
      "2024-10-20 21:28:48 [INFO]: Epoch 075 - training loss: 0.1419\n",
      "2024-10-20 21:28:48 [INFO]: Epoch 076 - training loss: 0.1262\n",
      "2024-10-20 21:28:48 [INFO]: Epoch 077 - training loss: 0.1891\n",
      "2024-10-20 21:28:48 [INFO]: Epoch 078 - training loss: 0.2697\n",
      "2024-10-20 21:28:49 [INFO]: Epoch 079 - training loss: 0.1485\n",
      "2024-10-20 21:28:49 [INFO]: Epoch 080 - training loss: 0.2170\n",
      "2024-10-20 21:28:49 [INFO]: Epoch 081 - training loss: 0.1467\n",
      "2024-10-20 21:28:49 [INFO]: Epoch 082 - training loss: 0.1634\n",
      "2024-10-20 21:28:49 [INFO]: Epoch 083 - training loss: 0.2247\n",
      "2024-10-20 21:28:49 [INFO]: Epoch 084 - training loss: 0.1931\n",
      "2024-10-20 21:28:49 [INFO]: Epoch 085 - training loss: 0.2238\n",
      "2024-10-20 21:28:50 [INFO]: Epoch 086 - training loss: 0.1112\n",
      "2024-10-20 21:28:50 [INFO]: Epoch 087 - training loss: 0.1788\n",
      "2024-10-20 21:28:50 [INFO]: Epoch 088 - training loss: 0.1867\n",
      "2024-10-20 21:28:50 [INFO]: Epoch 089 - training loss: 0.1840\n",
      "2024-10-20 21:28:50 [INFO]: Epoch 090 - training loss: 0.2543\n",
      "2024-10-20 21:28:50 [INFO]: Epoch 091 - training loss: 0.2223\n",
      "2024-10-20 21:28:51 [INFO]: Epoch 092 - training loss: 0.1677\n",
      "2024-10-20 21:28:51 [INFO]: Epoch 093 - training loss: 0.2166\n",
      "2024-10-20 21:28:51 [INFO]: Epoch 094 - training loss: 0.2043\n",
      "2024-10-20 21:28:51 [INFO]: Epoch 095 - training loss: 0.1467\n",
      "2024-10-20 21:28:51 [INFO]: Epoch 096 - training loss: 0.2707\n",
      "2024-10-20 21:28:51 [INFO]: Epoch 097 - training loss: 0.2082\n",
      "2024-10-20 21:28:51 [INFO]: Epoch 098 - training loss: 0.2219\n",
      "2024-10-20 21:28:52 [INFO]: Epoch 099 - training loss: 0.1633\n",
      "2024-10-20 21:28:52 [INFO]: Epoch 100 - training loss: 0.1732\n",
      "2024-10-20 21:28:52 [INFO]: Finished training. The best model is from epoch#43.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:32:41 [INFO]: Epoch 001 - training loss: 0.3164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 8/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:32:41 [INFO]: Epoch 002 - training loss: 0.1745\n",
      "2024-10-20 21:32:41 [INFO]: Epoch 003 - training loss: 0.3335\n",
      "2024-10-20 21:32:41 [INFO]: Epoch 004 - training loss: 0.1915\n",
      "2024-10-20 21:32:41 [INFO]: Epoch 005 - training loss: 0.1987\n",
      "2024-10-20 21:32:41 [INFO]: Epoch 006 - training loss: 0.2245\n",
      "2024-10-20 21:32:41 [INFO]: Epoch 007 - training loss: 0.1492\n",
      "2024-10-20 21:32:42 [INFO]: Epoch 008 - training loss: 0.1692\n",
      "2024-10-20 21:32:42 [INFO]: Epoch 009 - training loss: 0.2182\n",
      "2024-10-20 21:32:42 [INFO]: Epoch 010 - training loss: 0.1492\n",
      "2024-10-20 21:32:42 [INFO]: Epoch 011 - training loss: 0.2212\n",
      "2024-10-20 21:32:42 [INFO]: Epoch 012 - training loss: 0.1885\n",
      "2024-10-20 21:32:42 [INFO]: Epoch 013 - training loss: 0.0942\n",
      "2024-10-20 21:32:43 [INFO]: Epoch 014 - training loss: 0.1368\n",
      "2024-10-20 21:32:43 [INFO]: Epoch 015 - training loss: 0.2711\n",
      "2024-10-20 21:32:43 [INFO]: Epoch 016 - training loss: 0.1944\n",
      "2024-10-20 21:32:43 [INFO]: Epoch 017 - training loss: 0.1650\n",
      "2024-10-20 21:32:43 [INFO]: Epoch 018 - training loss: 0.1783\n",
      "2024-10-20 21:32:43 [INFO]: Epoch 019 - training loss: 0.1720\n",
      "2024-10-20 21:32:43 [INFO]: Epoch 020 - training loss: 0.2259\n",
      "2024-10-20 21:32:44 [INFO]: Epoch 021 - training loss: 0.2193\n",
      "2024-10-20 21:32:44 [INFO]: Epoch 022 - training loss: 0.1934\n",
      "2024-10-20 21:32:44 [INFO]: Epoch 023 - training loss: 0.2428\n",
      "2024-10-20 21:32:44 [INFO]: Epoch 024 - training loss: 0.2851\n",
      "2024-10-20 21:32:44 [INFO]: Epoch 025 - training loss: 0.2515\n",
      "2024-10-20 21:32:44 [INFO]: Epoch 026 - training loss: 0.1812\n",
      "2024-10-20 21:32:44 [INFO]: Epoch 027 - training loss: 0.1783\n",
      "2024-10-20 21:32:45 [INFO]: Epoch 028 - training loss: 0.2028\n",
      "2024-10-20 21:32:45 [INFO]: Epoch 029 - training loss: 0.1776\n",
      "2024-10-20 21:32:45 [INFO]: Epoch 030 - training loss: 0.1496\n",
      "2024-10-20 21:32:45 [INFO]: Epoch 031 - training loss: 0.1410\n",
      "2024-10-20 21:32:45 [INFO]: Epoch 032 - training loss: 0.1719\n",
      "2024-10-20 21:32:45 [INFO]: Epoch 033 - training loss: 0.1555\n",
      "2024-10-20 21:32:46 [INFO]: Epoch 034 - training loss: 0.1803\n",
      "2024-10-20 21:32:46 [INFO]: Epoch 035 - training loss: 0.1248\n",
      "2024-10-20 21:32:46 [INFO]: Epoch 036 - training loss: 0.1254\n",
      "2024-10-20 21:32:46 [INFO]: Epoch 037 - training loss: 0.1363\n",
      "2024-10-20 21:32:46 [INFO]: Epoch 038 - training loss: 0.1654\n",
      "2024-10-20 21:32:46 [INFO]: Epoch 039 - training loss: 0.1663\n",
      "2024-10-20 21:32:46 [INFO]: Epoch 040 - training loss: 0.1758\n",
      "2024-10-20 21:32:47 [INFO]: Epoch 041 - training loss: 0.1793\n",
      "2024-10-20 21:32:47 [INFO]: Epoch 042 - training loss: 0.1550\n",
      "2024-10-20 21:32:47 [INFO]: Epoch 043 - training loss: 0.1899\n",
      "2024-10-20 21:32:47 [INFO]: Epoch 044 - training loss: 0.1507\n",
      "2024-10-20 21:32:47 [INFO]: Epoch 045 - training loss: 0.0924\n",
      "2024-10-20 21:32:47 [INFO]: Epoch 046 - training loss: 0.0872\n",
      "2024-10-20 21:32:47 [INFO]: Epoch 047 - training loss: 0.1435\n",
      "2024-10-20 21:32:48 [INFO]: Epoch 048 - training loss: 0.1639\n",
      "2024-10-20 21:32:48 [INFO]: Epoch 049 - training loss: 0.2396\n",
      "2024-10-20 21:32:48 [INFO]: Epoch 050 - training loss: 0.1289\n",
      "2024-10-20 21:32:48 [INFO]: Epoch 051 - training loss: 0.1462\n",
      "2024-10-20 21:32:48 [INFO]: Epoch 052 - training loss: 0.1645\n",
      "2024-10-20 21:32:48 [INFO]: Epoch 053 - training loss: 0.1613\n",
      "2024-10-20 21:32:49 [INFO]: Epoch 054 - training loss: 0.1891\n",
      "2024-10-20 21:32:49 [INFO]: Epoch 055 - training loss: 0.1617\n",
      "2024-10-20 21:32:49 [INFO]: Epoch 056 - training loss: 0.1783\n",
      "2024-10-20 21:32:49 [INFO]: Epoch 057 - training loss: 0.2154\n",
      "2024-10-20 21:32:49 [INFO]: Epoch 058 - training loss: 0.1378\n",
      "2024-10-20 21:32:49 [INFO]: Epoch 059 - training loss: 0.1501\n",
      "2024-10-20 21:32:49 [INFO]: Epoch 060 - training loss: 0.2727\n",
      "2024-10-20 21:32:50 [INFO]: Epoch 061 - training loss: 0.1572\n",
      "2024-10-20 21:32:50 [INFO]: Epoch 062 - training loss: 0.2241\n",
      "2024-10-20 21:32:50 [INFO]: Epoch 063 - training loss: 0.1419\n",
      "2024-10-20 21:32:50 [INFO]: Epoch 064 - training loss: 0.1441\n",
      "2024-10-20 21:32:50 [INFO]: Epoch 065 - training loss: 0.1254\n",
      "2024-10-20 21:32:50 [INFO]: Epoch 066 - training loss: 0.2106\n",
      "2024-10-20 21:32:51 [INFO]: Epoch 067 - training loss: 0.1202\n",
      "2024-10-20 21:32:51 [INFO]: Epoch 068 - training loss: 0.1511\n",
      "2024-10-20 21:32:51 [INFO]: Epoch 069 - training loss: 0.1802\n",
      "2024-10-20 21:32:51 [INFO]: Epoch 070 - training loss: 0.2027\n",
      "2024-10-20 21:32:51 [INFO]: Epoch 071 - training loss: 0.1168\n",
      "2024-10-20 21:32:51 [INFO]: Epoch 072 - training loss: 0.1307\n",
      "2024-10-20 21:32:51 [INFO]: Epoch 073 - training loss: 0.1271\n",
      "2024-10-20 21:32:52 [INFO]: Epoch 074 - training loss: 0.1716\n",
      "2024-10-20 21:32:52 [INFO]: Epoch 075 - training loss: 0.1746\n",
      "2024-10-20 21:32:52 [INFO]: Epoch 076 - training loss: 0.1042\n",
      "2024-10-20 21:32:52 [INFO]: Epoch 077 - training loss: 0.1767\n",
      "2024-10-20 21:32:52 [INFO]: Epoch 078 - training loss: 0.1815\n",
      "2024-10-20 21:32:52 [INFO]: Epoch 079 - training loss: 0.1586\n",
      "2024-10-20 21:32:52 [INFO]: Epoch 080 - training loss: 0.1446\n",
      "2024-10-20 21:32:53 [INFO]: Epoch 081 - training loss: 0.1456\n",
      "2024-10-20 21:32:53 [INFO]: Epoch 082 - training loss: 0.2542\n",
      "2024-10-20 21:32:53 [INFO]: Epoch 083 - training loss: 0.1493\n",
      "2024-10-20 21:32:53 [INFO]: Epoch 084 - training loss: 0.1653\n",
      "2024-10-20 21:32:53 [INFO]: Epoch 085 - training loss: 0.1703\n",
      "2024-10-20 21:32:53 [INFO]: Epoch 086 - training loss: 0.1705\n",
      "2024-10-20 21:32:53 [INFO]: Epoch 087 - training loss: 0.1671\n",
      "2024-10-20 21:32:54 [INFO]: Epoch 088 - training loss: 0.1103\n",
      "2024-10-20 21:32:54 [INFO]: Epoch 089 - training loss: 0.1337\n",
      "2024-10-20 21:32:54 [INFO]: Epoch 090 - training loss: 0.2312\n",
      "2024-10-20 21:32:54 [INFO]: Epoch 091 - training loss: 0.1617\n",
      "2024-10-20 21:32:54 [INFO]: Epoch 092 - training loss: 0.1134\n",
      "2024-10-20 21:32:54 [INFO]: Epoch 093 - training loss: 0.1909\n",
      "2024-10-20 21:32:55 [INFO]: Epoch 094 - training loss: 0.2055\n",
      "2024-10-20 21:32:55 [INFO]: Epoch 095 - training loss: 0.3545\n",
      "2024-10-20 21:32:55 [INFO]: Epoch 096 - training loss: 0.1015\n",
      "2024-10-20 21:32:55 [INFO]: Epoch 097 - training loss: 0.1755\n",
      "2024-10-20 21:32:55 [INFO]: Epoch 098 - training loss: 0.1860\n",
      "2024-10-20 21:32:55 [INFO]: Epoch 099 - training loss: 0.2106\n",
      "2024-10-20 21:32:55 [INFO]: Epoch 100 - training loss: 0.1264\n",
      "2024-10-20 21:32:55 [INFO]: Finished training. The best model is from epoch#46.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:36:44 [INFO]: Epoch 001 - training loss: 0.2107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 9/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:36:44 [INFO]: Epoch 002 - training loss: 0.1429\n",
      "2024-10-20 21:36:45 [INFO]: Epoch 003 - training loss: 0.1432\n",
      "2024-10-20 21:36:45 [INFO]: Epoch 004 - training loss: 0.1899\n",
      "2024-10-20 21:36:45 [INFO]: Epoch 005 - training loss: 0.2230\n",
      "2024-10-20 21:36:45 [INFO]: Epoch 006 - training loss: 0.1473\n",
      "2024-10-20 21:36:45 [INFO]: Epoch 007 - training loss: 0.1768\n",
      "2024-10-20 21:36:45 [INFO]: Epoch 008 - training loss: 0.2353\n",
      "2024-10-20 21:36:46 [INFO]: Epoch 009 - training loss: 0.1923\n",
      "2024-10-20 21:36:46 [INFO]: Epoch 010 - training loss: 0.2836\n",
      "2024-10-20 21:36:46 [INFO]: Epoch 011 - training loss: 0.2124\n",
      "2024-10-20 21:36:46 [INFO]: Epoch 012 - training loss: 0.1317\n",
      "2024-10-20 21:36:46 [INFO]: Epoch 013 - training loss: 0.1408\n",
      "2024-10-20 21:36:46 [INFO]: Epoch 014 - training loss: 0.1921\n",
      "2024-10-20 21:36:46 [INFO]: Epoch 015 - training loss: 0.1911\n",
      "2024-10-20 21:36:47 [INFO]: Epoch 016 - training loss: 0.1642\n",
      "2024-10-20 21:36:47 [INFO]: Epoch 017 - training loss: 0.2640\n",
      "2024-10-20 21:36:47 [INFO]: Epoch 018 - training loss: 0.1068\n",
      "2024-10-20 21:36:47 [INFO]: Epoch 019 - training loss: 0.2187\n",
      "2024-10-20 21:36:47 [INFO]: Epoch 020 - training loss: 0.1341\n",
      "2024-10-20 21:36:47 [INFO]: Epoch 021 - training loss: 0.2074\n",
      "2024-10-20 21:36:47 [INFO]: Epoch 022 - training loss: 0.1470\n",
      "2024-10-20 21:36:48 [INFO]: Epoch 023 - training loss: 0.1601\n",
      "2024-10-20 21:36:48 [INFO]: Epoch 024 - training loss: 0.1594\n",
      "2024-10-20 21:36:48 [INFO]: Epoch 025 - training loss: 0.1705\n",
      "2024-10-20 21:36:48 [INFO]: Epoch 026 - training loss: 0.1877\n",
      "2024-10-20 21:36:48 [INFO]: Epoch 027 - training loss: 0.1982\n",
      "2024-10-20 21:36:48 [INFO]: Epoch 028 - training loss: 0.1218\n",
      "2024-10-20 21:36:49 [INFO]: Epoch 029 - training loss: 0.1106\n",
      "2024-10-20 21:36:49 [INFO]: Epoch 030 - training loss: 0.1757\n",
      "2024-10-20 21:36:49 [INFO]: Epoch 031 - training loss: 0.2118\n",
      "2024-10-20 21:36:49 [INFO]: Epoch 032 - training loss: 0.2892\n",
      "2024-10-20 21:36:49 [INFO]: Epoch 033 - training loss: 0.1400\n",
      "2024-10-20 21:36:49 [INFO]: Epoch 034 - training loss: 0.3057\n",
      "2024-10-20 21:36:49 [INFO]: Epoch 035 - training loss: 0.1832\n",
      "2024-10-20 21:36:50 [INFO]: Epoch 036 - training loss: 0.2298\n",
      "2024-10-20 21:36:50 [INFO]: Epoch 037 - training loss: 0.1709\n",
      "2024-10-20 21:36:50 [INFO]: Epoch 038 - training loss: 0.1583\n",
      "2024-10-20 21:36:50 [INFO]: Epoch 039 - training loss: 0.2593\n",
      "2024-10-20 21:36:50 [INFO]: Epoch 040 - training loss: 0.0881\n",
      "2024-10-20 21:36:50 [INFO]: Epoch 041 - training loss: 0.1260\n",
      "2024-10-20 21:36:50 [INFO]: Epoch 042 - training loss: 0.1632\n",
      "2024-10-20 21:36:51 [INFO]: Epoch 043 - training loss: 0.2419\n",
      "2024-10-20 21:36:51 [INFO]: Epoch 044 - training loss: 0.1618\n",
      "2024-10-20 21:36:51 [INFO]: Epoch 045 - training loss: 0.1386\n",
      "2024-10-20 21:36:51 [INFO]: Epoch 046 - training loss: 0.1325\n",
      "2024-10-20 21:36:51 [INFO]: Epoch 047 - training loss: 0.2488\n",
      "2024-10-20 21:36:51 [INFO]: Epoch 048 - training loss: 0.1401\n",
      "2024-10-20 21:36:52 [INFO]: Epoch 049 - training loss: 0.1507\n",
      "2024-10-20 21:36:52 [INFO]: Epoch 050 - training loss: 0.1895\n",
      "2024-10-20 21:36:52 [INFO]: Epoch 051 - training loss: 0.1513\n",
      "2024-10-20 21:36:52 [INFO]: Epoch 052 - training loss: 0.1670\n",
      "2024-10-20 21:36:52 [INFO]: Epoch 053 - training loss: 0.1310\n",
      "2024-10-20 21:36:52 [INFO]: Epoch 054 - training loss: 0.2164\n",
      "2024-10-20 21:36:52 [INFO]: Epoch 055 - training loss: 0.2031\n",
      "2024-10-20 21:36:53 [INFO]: Epoch 056 - training loss: 0.1576\n",
      "2024-10-20 21:36:53 [INFO]: Epoch 057 - training loss: 0.2175\n",
      "2024-10-20 21:36:53 [INFO]: Epoch 058 - training loss: 0.2103\n",
      "2024-10-20 21:36:53 [INFO]: Epoch 059 - training loss: 0.1974\n",
      "2024-10-20 21:36:53 [INFO]: Epoch 060 - training loss: 0.1821\n",
      "2024-10-20 21:36:53 [INFO]: Epoch 061 - training loss: 0.2043\n",
      "2024-10-20 21:36:54 [INFO]: Epoch 062 - training loss: 0.2477\n",
      "2024-10-20 21:36:54 [INFO]: Epoch 063 - training loss: 0.2862\n",
      "2024-10-20 21:36:54 [INFO]: Epoch 064 - training loss: 0.1787\n",
      "2024-10-20 21:36:54 [INFO]: Epoch 065 - training loss: 0.1634\n",
      "2024-10-20 21:36:54 [INFO]: Epoch 066 - training loss: 0.1296\n",
      "2024-10-20 21:36:54 [INFO]: Epoch 067 - training loss: 0.2027\n",
      "2024-10-20 21:36:54 [INFO]: Epoch 068 - training loss: 0.1722\n",
      "2024-10-20 21:36:55 [INFO]: Epoch 069 - training loss: 0.3184\n",
      "2024-10-20 21:36:55 [INFO]: Epoch 070 - training loss: 0.1840\n",
      "2024-10-20 21:36:55 [INFO]: Epoch 071 - training loss: 0.1719\n",
      "2024-10-20 21:36:55 [INFO]: Epoch 072 - training loss: 0.1251\n",
      "2024-10-20 21:36:55 [INFO]: Epoch 073 - training loss: 0.2051\n",
      "2024-10-20 21:36:55 [INFO]: Epoch 074 - training loss: 0.2389\n",
      "2024-10-20 21:36:55 [INFO]: Epoch 075 - training loss: 0.1995\n",
      "2024-10-20 21:36:56 [INFO]: Epoch 076 - training loss: 0.1822\n",
      "2024-10-20 21:36:56 [INFO]: Epoch 077 - training loss: 0.1492\n",
      "2024-10-20 21:36:56 [INFO]: Epoch 078 - training loss: 0.1991\n",
      "2024-10-20 21:36:56 [INFO]: Epoch 079 - training loss: 0.1255\n",
      "2024-10-20 21:36:56 [INFO]: Epoch 080 - training loss: 0.1379\n",
      "2024-10-20 21:36:56 [INFO]: Epoch 081 - training loss: 0.1698\n",
      "2024-10-20 21:36:57 [INFO]: Epoch 082 - training loss: 0.2770\n",
      "2024-10-20 21:36:57 [INFO]: Epoch 083 - training loss: 0.1500\n",
      "2024-10-20 21:36:57 [INFO]: Epoch 084 - training loss: 0.2074\n",
      "2024-10-20 21:36:57 [INFO]: Epoch 085 - training loss: 0.2249\n",
      "2024-10-20 21:36:57 [INFO]: Epoch 086 - training loss: 0.1324\n",
      "2024-10-20 21:36:57 [INFO]: Epoch 087 - training loss: 0.1380\n",
      "2024-10-20 21:36:57 [INFO]: Epoch 088 - training loss: 0.0966\n",
      "2024-10-20 21:36:58 [INFO]: Epoch 089 - training loss: 0.1293\n",
      "2024-10-20 21:36:58 [INFO]: Epoch 090 - training loss: 0.2273\n",
      "2024-10-20 21:36:58 [INFO]: Epoch 091 - training loss: 0.1000\n",
      "2024-10-20 21:36:58 [INFO]: Epoch 092 - training loss: 0.2162\n",
      "2024-10-20 21:36:58 [INFO]: Epoch 093 - training loss: 0.1822\n",
      "2024-10-20 21:36:58 [INFO]: Epoch 094 - training loss: 0.1036\n",
      "2024-10-20 21:36:58 [INFO]: Epoch 095 - training loss: 0.1114\n",
      "2024-10-20 21:36:59 [INFO]: Epoch 096 - training loss: 0.1539\n",
      "2024-10-20 21:36:59 [INFO]: Epoch 097 - training loss: 0.1497\n",
      "2024-10-20 21:36:59 [INFO]: Epoch 098 - training loss: 0.2697\n",
      "2024-10-20 21:36:59 [INFO]: Epoch 099 - training loss: 0.1917\n",
      "2024-10-20 21:36:59 [INFO]: Epoch 100 - training loss: 0.1944\n",
      "2024-10-20 21:36:59 [INFO]: Finished training. The best model is from epoch#40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:40:48 [INFO]: Epoch 001 - training loss: 0.1617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 10/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:40:48 [INFO]: Epoch 002 - training loss: 0.1153\n",
      "2024-10-20 21:40:48 [INFO]: Epoch 003 - training loss: 0.1142\n",
      "2024-10-20 21:40:48 [INFO]: Epoch 004 - training loss: 0.1037\n",
      "2024-10-20 21:40:49 [INFO]: Epoch 005 - training loss: 0.2055\n",
      "2024-10-20 21:40:49 [INFO]: Epoch 006 - training loss: 0.1415\n",
      "2024-10-20 21:40:49 [INFO]: Epoch 007 - training loss: 0.1653\n",
      "2024-10-20 21:40:49 [INFO]: Epoch 008 - training loss: 0.0994\n",
      "2024-10-20 21:40:49 [INFO]: Epoch 009 - training loss: 0.2701\n",
      "2024-10-20 21:40:49 [INFO]: Epoch 010 - training loss: 0.1939\n",
      "2024-10-20 21:40:50 [INFO]: Epoch 011 - training loss: 0.1934\n",
      "2024-10-20 21:40:50 [INFO]: Epoch 012 - training loss: 0.1998\n",
      "2024-10-20 21:40:50 [INFO]: Epoch 013 - training loss: 0.1957\n",
      "2024-10-20 21:40:50 [INFO]: Epoch 014 - training loss: 0.1875\n",
      "2024-10-20 21:40:50 [INFO]: Epoch 015 - training loss: 0.1440\n",
      "2024-10-20 21:40:50 [INFO]: Epoch 016 - training loss: 0.1655\n",
      "2024-10-20 21:40:50 [INFO]: Epoch 017 - training loss: 0.2171\n",
      "2024-10-20 21:40:51 [INFO]: Epoch 018 - training loss: 0.1441\n",
      "2024-10-20 21:40:51 [INFO]: Epoch 019 - training loss: 0.1898\n",
      "2024-10-20 21:40:51 [INFO]: Epoch 020 - training loss: 0.1505\n",
      "2024-10-20 21:40:51 [INFO]: Epoch 021 - training loss: 0.1842\n",
      "2024-10-20 21:40:51 [INFO]: Epoch 022 - training loss: 0.1552\n",
      "2024-10-20 21:40:51 [INFO]: Epoch 023 - training loss: 0.1206\n",
      "2024-10-20 21:40:51 [INFO]: Epoch 024 - training loss: 0.1265\n",
      "2024-10-20 21:40:52 [INFO]: Epoch 025 - training loss: 0.2812\n",
      "2024-10-20 21:40:52 [INFO]: Epoch 026 - training loss: 0.1544\n",
      "2024-10-20 21:40:52 [INFO]: Epoch 027 - training loss: 0.1484\n",
      "2024-10-20 21:40:52 [INFO]: Epoch 028 - training loss: 0.1471\n",
      "2024-10-20 21:40:52 [INFO]: Epoch 029 - training loss: 0.1924\n",
      "2024-10-20 21:40:52 [INFO]: Epoch 030 - training loss: 0.1914\n",
      "2024-10-20 21:40:53 [INFO]: Epoch 031 - training loss: 0.1899\n",
      "2024-10-20 21:40:53 [INFO]: Epoch 032 - training loss: 0.1601\n",
      "2024-10-20 21:40:53 [INFO]: Epoch 033 - training loss: 0.2108\n",
      "2024-10-20 21:40:53 [INFO]: Epoch 034 - training loss: 0.2402\n",
      "2024-10-20 21:40:53 [INFO]: Epoch 035 - training loss: 0.1595\n",
      "2024-10-20 21:40:53 [INFO]: Epoch 036 - training loss: 0.1914\n",
      "2024-10-20 21:40:53 [INFO]: Epoch 037 - training loss: 0.1467\n",
      "2024-10-20 21:40:54 [INFO]: Epoch 038 - training loss: 0.1479\n",
      "2024-10-20 21:40:54 [INFO]: Epoch 039 - training loss: 0.2469\n",
      "2024-10-20 21:40:54 [INFO]: Epoch 040 - training loss: 0.2005\n",
      "2024-10-20 21:40:54 [INFO]: Epoch 041 - training loss: 0.2671\n",
      "2024-10-20 21:40:54 [INFO]: Epoch 042 - training loss: 0.2323\n",
      "2024-10-20 21:40:54 [INFO]: Epoch 043 - training loss: 0.1852\n",
      "2024-10-20 21:40:54 [INFO]: Epoch 044 - training loss: 0.1944\n",
      "2024-10-20 21:40:55 [INFO]: Epoch 045 - training loss: 0.2680\n",
      "2024-10-20 21:40:55 [INFO]: Epoch 046 - training loss: 0.1799\n",
      "2024-10-20 21:40:55 [INFO]: Epoch 047 - training loss: 0.1396\n",
      "2024-10-20 21:40:55 [INFO]: Epoch 048 - training loss: 0.2363\n",
      "2024-10-20 21:40:55 [INFO]: Epoch 049 - training loss: 0.3015\n",
      "2024-10-20 21:40:55 [INFO]: Epoch 050 - training loss: 0.2440\n",
      "2024-10-20 21:40:56 [INFO]: Epoch 051 - training loss: 0.1346\n",
      "2024-10-20 21:40:56 [INFO]: Epoch 052 - training loss: 0.1665\n",
      "2024-10-20 21:40:56 [INFO]: Epoch 053 - training loss: 0.1956\n",
      "2024-10-20 21:40:56 [INFO]: Epoch 054 - training loss: 0.2140\n",
      "2024-10-20 21:40:56 [INFO]: Epoch 055 - training loss: 0.1637\n",
      "2024-10-20 21:40:56 [INFO]: Epoch 056 - training loss: 0.1586\n",
      "2024-10-20 21:40:56 [INFO]: Epoch 057 - training loss: 0.1139\n",
      "2024-10-20 21:40:57 [INFO]: Epoch 058 - training loss: 0.1839\n",
      "2024-10-20 21:40:57 [INFO]: Epoch 059 - training loss: 0.1170\n",
      "2024-10-20 21:40:57 [INFO]: Epoch 060 - training loss: 0.2040\n",
      "2024-10-20 21:40:57 [INFO]: Epoch 061 - training loss: 0.1192\n",
      "2024-10-20 21:40:57 [INFO]: Epoch 062 - training loss: 0.0884\n",
      "2024-10-20 21:40:57 [INFO]: Epoch 063 - training loss: 0.1761\n",
      "2024-10-20 21:40:58 [INFO]: Epoch 064 - training loss: 0.1632\n",
      "2024-10-20 21:40:58 [INFO]: Epoch 065 - training loss: 0.1718\n",
      "2024-10-20 21:40:58 [INFO]: Epoch 066 - training loss: 0.1727\n",
      "2024-10-20 21:40:58 [INFO]: Epoch 067 - training loss: 0.1196\n",
      "2024-10-20 21:40:58 [INFO]: Epoch 068 - training loss: 0.1163\n",
      "2024-10-20 21:40:58 [INFO]: Epoch 069 - training loss: 0.1598\n",
      "2024-10-20 21:40:58 [INFO]: Epoch 070 - training loss: 0.2547\n",
      "2024-10-20 21:40:59 [INFO]: Epoch 071 - training loss: 0.2044\n",
      "2024-10-20 21:40:59 [INFO]: Epoch 072 - training loss: 0.2689\n",
      "2024-10-20 21:40:59 [INFO]: Epoch 073 - training loss: 0.2748\n",
      "2024-10-20 21:40:59 [INFO]: Epoch 074 - training loss: 0.1445\n",
      "2024-10-20 21:40:59 [INFO]: Epoch 075 - training loss: 0.2961\n",
      "2024-10-20 21:40:59 [INFO]: Epoch 076 - training loss: 0.2731\n",
      "2024-10-20 21:40:59 [INFO]: Epoch 077 - training loss: 0.1675\n",
      "2024-10-20 21:41:00 [INFO]: Epoch 078 - training loss: 0.1868\n",
      "2024-10-20 21:41:00 [INFO]: Epoch 079 - training loss: 0.1510\n",
      "2024-10-20 21:41:00 [INFO]: Epoch 080 - training loss: 0.1682\n",
      "2024-10-20 21:41:00 [INFO]: Epoch 081 - training loss: 0.1120\n",
      "2024-10-20 21:41:00 [INFO]: Epoch 082 - training loss: 0.2130\n",
      "2024-10-20 21:41:00 [INFO]: Epoch 083 - training loss: 0.1442\n",
      "2024-10-20 21:41:00 [INFO]: Epoch 084 - training loss: 0.2450\n",
      "2024-10-20 21:41:01 [INFO]: Epoch 085 - training loss: 0.3169\n",
      "2024-10-20 21:41:01 [INFO]: Epoch 086 - training loss: 0.2063\n",
      "2024-10-20 21:41:01 [INFO]: Epoch 087 - training loss: 0.2041\n",
      "2024-10-20 21:41:01 [INFO]: Epoch 088 - training loss: 0.1787\n",
      "2024-10-20 21:41:01 [INFO]: Epoch 089 - training loss: 0.1528\n",
      "2024-10-20 21:41:01 [INFO]: Epoch 090 - training loss: 0.1932\n",
      "2024-10-20 21:41:02 [INFO]: Epoch 091 - training loss: 0.1535\n",
      "2024-10-20 21:41:02 [INFO]: Epoch 092 - training loss: 0.2167\n",
      "2024-10-20 21:41:02 [INFO]: Epoch 093 - training loss: 0.2374\n",
      "2024-10-20 21:41:02 [INFO]: Epoch 094 - training loss: 0.1700\n",
      "2024-10-20 21:41:02 [INFO]: Epoch 095 - training loss: 0.1452\n",
      "2024-10-20 21:41:02 [INFO]: Epoch 096 - training loss: 0.1746\n",
      "2024-10-20 21:41:02 [INFO]: Epoch 097 - training loss: 0.1783\n",
      "2024-10-20 21:41:03 [INFO]: Epoch 098 - training loss: 0.1214\n",
      "2024-10-20 21:41:03 [INFO]: Epoch 099 - training loss: 0.1754\n",
      "2024-10-20 21:41:03 [INFO]: Epoch 100 - training loss: 0.1695\n",
      "2024-10-20 21:41:03 [INFO]: Finished training. The best model is from epoch#62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:44:52 [INFO]: Epoch 001 - training loss: 0.0975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 11/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:44:52 [INFO]: Epoch 002 - training loss: 0.1167\n",
      "2024-10-20 21:44:52 [INFO]: Epoch 003 - training loss: 0.1707\n",
      "2024-10-20 21:44:52 [INFO]: Epoch 004 - training loss: 0.1940\n",
      "2024-10-20 21:44:52 [INFO]: Epoch 005 - training loss: 0.1691\n",
      "2024-10-20 21:44:53 [INFO]: Epoch 006 - training loss: 0.1854\n",
      "2024-10-20 21:44:53 [INFO]: Epoch 007 - training loss: 0.1893\n",
      "2024-10-20 21:44:53 [INFO]: Epoch 008 - training loss: 0.2214\n",
      "2024-10-20 21:44:53 [INFO]: Epoch 009 - training loss: 0.1906\n",
      "2024-10-20 21:44:53 [INFO]: Epoch 010 - training loss: 0.1312\n",
      "2024-10-20 21:44:53 [INFO]: Epoch 011 - training loss: 0.2288\n",
      "2024-10-20 21:44:54 [INFO]: Epoch 012 - training loss: 0.1532\n",
      "2024-10-20 21:44:54 [INFO]: Epoch 013 - training loss: 0.2839\n",
      "2024-10-20 21:44:54 [INFO]: Epoch 014 - training loss: 0.1485\n",
      "2024-10-20 21:44:54 [INFO]: Epoch 015 - training loss: 0.1734\n",
      "2024-10-20 21:44:54 [INFO]: Epoch 016 - training loss: 0.2314\n",
      "2024-10-20 21:44:54 [INFO]: Epoch 017 - training loss: 0.2829\n",
      "2024-10-20 21:44:54 [INFO]: Epoch 018 - training loss: 0.1193\n",
      "2024-10-20 21:44:55 [INFO]: Epoch 019 - training loss: 0.1613\n",
      "2024-10-20 21:44:55 [INFO]: Epoch 020 - training loss: 0.1777\n",
      "2024-10-20 21:44:55 [INFO]: Epoch 021 - training loss: 0.1170\n",
      "2024-10-20 21:44:55 [INFO]: Epoch 022 - training loss: 0.1374\n",
      "2024-10-20 21:44:55 [INFO]: Epoch 023 - training loss: 0.1570\n",
      "2024-10-20 21:44:55 [INFO]: Epoch 024 - training loss: 0.1654\n",
      "2024-10-20 21:44:55 [INFO]: Epoch 025 - training loss: 0.2391\n",
      "2024-10-20 21:44:56 [INFO]: Epoch 026 - training loss: 0.1475\n",
      "2024-10-20 21:44:56 [INFO]: Epoch 027 - training loss: 0.3289\n",
      "2024-10-20 21:44:56 [INFO]: Epoch 028 - training loss: 0.1783\n",
      "2024-10-20 21:44:56 [INFO]: Epoch 029 - training loss: 0.1396\n",
      "2024-10-20 21:44:56 [INFO]: Epoch 030 - training loss: 0.1431\n",
      "2024-10-20 21:44:56 [INFO]: Epoch 031 - training loss: 0.2021\n",
      "2024-10-20 21:44:57 [INFO]: Epoch 032 - training loss: 0.2449\n",
      "2024-10-20 21:44:57 [INFO]: Epoch 033 - training loss: 0.2392\n",
      "2024-10-20 21:44:57 [INFO]: Epoch 034 - training loss: 0.1374\n",
      "2024-10-20 21:44:57 [INFO]: Epoch 035 - training loss: 0.1325\n",
      "2024-10-20 21:44:57 [INFO]: Epoch 036 - training loss: 0.1475\n",
      "2024-10-20 21:44:57 [INFO]: Epoch 037 - training loss: 0.1798\n",
      "2024-10-20 21:44:57 [INFO]: Epoch 038 - training loss: 0.1656\n",
      "2024-10-20 21:44:58 [INFO]: Epoch 039 - training loss: 0.1839\n",
      "2024-10-20 21:44:58 [INFO]: Epoch 040 - training loss: 0.1429\n",
      "2024-10-20 21:44:58 [INFO]: Epoch 041 - training loss: 0.1684\n",
      "2024-10-20 21:44:58 [INFO]: Epoch 042 - training loss: 0.1367\n",
      "2024-10-20 21:44:58 [INFO]: Epoch 043 - training loss: 0.1035\n",
      "2024-10-20 21:44:58 [INFO]: Epoch 044 - training loss: 0.1591\n",
      "2024-10-20 21:44:58 [INFO]: Epoch 045 - training loss: 0.2177\n",
      "2024-10-20 21:44:59 [INFO]: Epoch 046 - training loss: 0.1238\n",
      "2024-10-20 21:44:59 [INFO]: Epoch 047 - training loss: 0.1984\n",
      "2024-10-20 21:44:59 [INFO]: Epoch 048 - training loss: 0.1639\n",
      "2024-10-20 21:44:59 [INFO]: Epoch 049 - training loss: 0.1720\n",
      "2024-10-20 21:44:59 [INFO]: Epoch 050 - training loss: 0.1460\n",
      "2024-10-20 21:44:59 [INFO]: Epoch 051 - training loss: 0.1028\n",
      "2024-10-20 21:44:59 [INFO]: Epoch 052 - training loss: 0.1328\n",
      "2024-10-20 21:45:00 [INFO]: Epoch 053 - training loss: 0.2202\n",
      "2024-10-20 21:45:00 [INFO]: Epoch 054 - training loss: 0.1626\n",
      "2024-10-20 21:45:00 [INFO]: Epoch 055 - training loss: 0.1992\n",
      "2024-10-20 21:45:00 [INFO]: Epoch 056 - training loss: 0.1533\n",
      "2024-10-20 21:45:00 [INFO]: Epoch 057 - training loss: 0.1615\n",
      "2024-10-20 21:45:00 [INFO]: Epoch 058 - training loss: 0.1408\n",
      "2024-10-20 21:45:01 [INFO]: Epoch 059 - training loss: 0.1378\n",
      "2024-10-20 21:45:01 [INFO]: Epoch 060 - training loss: 0.1288\n",
      "2024-10-20 21:45:01 [INFO]: Epoch 061 - training loss: 0.2017\n",
      "2024-10-20 21:45:01 [INFO]: Epoch 062 - training loss: 0.2381\n",
      "2024-10-20 21:45:01 [INFO]: Epoch 063 - training loss: 0.2284\n",
      "2024-10-20 21:45:01 [INFO]: Epoch 064 - training loss: 0.1049\n",
      "2024-10-20 21:45:01 [INFO]: Epoch 065 - training loss: 0.1898\n",
      "2024-10-20 21:45:02 [INFO]: Epoch 066 - training loss: 0.1577\n",
      "2024-10-20 21:45:02 [INFO]: Epoch 067 - training loss: 0.1870\n",
      "2024-10-20 21:45:02 [INFO]: Epoch 068 - training loss: 0.0861\n",
      "2024-10-20 21:45:02 [INFO]: Epoch 069 - training loss: 0.1514\n",
      "2024-10-20 21:45:02 [INFO]: Epoch 070 - training loss: 0.1711\n",
      "2024-10-20 21:45:02 [INFO]: Epoch 071 - training loss: 0.1203\n",
      "2024-10-20 21:45:02 [INFO]: Epoch 072 - training loss: 0.1992\n",
      "2024-10-20 21:45:03 [INFO]: Epoch 073 - training loss: 0.1820\n",
      "2024-10-20 21:45:03 [INFO]: Epoch 074 - training loss: 0.1505\n",
      "2024-10-20 21:45:03 [INFO]: Epoch 075 - training loss: 0.1692\n",
      "2024-10-20 21:45:03 [INFO]: Epoch 076 - training loss: 0.1459\n",
      "2024-10-20 21:45:03 [INFO]: Epoch 077 - training loss: 0.1471\n",
      "2024-10-20 21:45:03 [INFO]: Epoch 078 - training loss: 0.2136\n",
      "2024-10-20 21:45:04 [INFO]: Epoch 079 - training loss: 0.2105\n",
      "2024-10-20 21:45:04 [INFO]: Epoch 080 - training loss: 0.1483\n",
      "2024-10-20 21:45:04 [INFO]: Epoch 081 - training loss: 0.1469\n",
      "2024-10-20 21:45:04 [INFO]: Epoch 082 - training loss: 0.2185\n",
      "2024-10-20 21:45:04 [INFO]: Epoch 083 - training loss: 0.1739\n",
      "2024-10-20 21:45:04 [INFO]: Epoch 084 - training loss: 0.1338\n",
      "2024-10-20 21:45:04 [INFO]: Epoch 085 - training loss: 0.2583\n",
      "2024-10-20 21:45:05 [INFO]: Epoch 086 - training loss: 0.2260\n",
      "2024-10-20 21:45:05 [INFO]: Epoch 087 - training loss: 0.1516\n",
      "2024-10-20 21:45:05 [INFO]: Epoch 088 - training loss: 0.1500\n",
      "2024-10-20 21:45:05 [INFO]: Epoch 089 - training loss: 0.2514\n",
      "2024-10-20 21:45:05 [INFO]: Epoch 090 - training loss: 0.1172\n",
      "2024-10-20 21:45:05 [INFO]: Epoch 091 - training loss: 0.1053\n",
      "2024-10-20 21:45:05 [INFO]: Epoch 092 - training loss: 0.1849\n",
      "2024-10-20 21:45:06 [INFO]: Epoch 093 - training loss: 0.2162\n",
      "2024-10-20 21:45:06 [INFO]: Epoch 094 - training loss: 0.1976\n",
      "2024-10-20 21:45:06 [INFO]: Epoch 095 - training loss: 0.1871\n",
      "2024-10-20 21:45:06 [INFO]: Epoch 096 - training loss: 0.1336\n",
      "2024-10-20 21:45:06 [INFO]: Epoch 097 - training loss: 0.1953\n",
      "2024-10-20 21:45:06 [INFO]: Epoch 098 - training loss: 0.2306\n",
      "2024-10-20 21:45:07 [INFO]: Epoch 099 - training loss: 0.1443\n",
      "2024-10-20 21:45:07 [INFO]: Epoch 100 - training loss: 0.1623\n",
      "2024-10-20 21:45:07 [INFO]: Finished training. The best model is from epoch#68.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:48:56 [INFO]: Epoch 001 - training loss: 0.1840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 12/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:48:56 [INFO]: Epoch 002 - training loss: 0.1389\n",
      "2024-10-20 21:48:56 [INFO]: Epoch 003 - training loss: 0.1420\n",
      "2024-10-20 21:48:56 [INFO]: Epoch 004 - training loss: 0.1459\n",
      "2024-10-20 21:48:57 [INFO]: Epoch 005 - training loss: 0.1943\n",
      "2024-10-20 21:48:57 [INFO]: Epoch 006 - training loss: 0.1263\n",
      "2024-10-20 21:48:57 [INFO]: Epoch 007 - training loss: 0.1769\n",
      "2024-10-20 21:48:57 [INFO]: Epoch 008 - training loss: 0.1398\n",
      "2024-10-20 21:48:57 [INFO]: Epoch 009 - training loss: 0.1375\n",
      "2024-10-20 21:48:57 [INFO]: Epoch 010 - training loss: 0.1332\n",
      "2024-10-20 21:48:57 [INFO]: Epoch 011 - training loss: 0.2134\n",
      "2024-10-20 21:48:58 [INFO]: Epoch 012 - training loss: 0.1773\n",
      "2024-10-20 21:48:58 [INFO]: Epoch 013 - training loss: 0.1775\n",
      "2024-10-20 21:48:58 [INFO]: Epoch 014 - training loss: 0.1736\n",
      "2024-10-20 21:48:58 [INFO]: Epoch 015 - training loss: 0.2170\n",
      "2024-10-20 21:48:58 [INFO]: Epoch 016 - training loss: 0.1734\n",
      "2024-10-20 21:48:58 [INFO]: Epoch 017 - training loss: 0.1680\n",
      "2024-10-20 21:48:59 [INFO]: Epoch 018 - training loss: 0.1394\n",
      "2024-10-20 21:48:59 [INFO]: Epoch 019 - training loss: 0.2316\n",
      "2024-10-20 21:48:59 [INFO]: Epoch 020 - training loss: 0.1661\n",
      "2024-10-20 21:48:59 [INFO]: Epoch 021 - training loss: 0.2481\n",
      "2024-10-20 21:48:59 [INFO]: Epoch 022 - training loss: 0.1744\n",
      "2024-10-20 21:48:59 [INFO]: Epoch 023 - training loss: 0.2446\n",
      "2024-10-20 21:48:59 [INFO]: Epoch 024 - training loss: 0.1796\n",
      "2024-10-20 21:49:00 [INFO]: Epoch 025 - training loss: 0.1465\n",
      "2024-10-20 21:49:00 [INFO]: Epoch 026 - training loss: 0.1568\n",
      "2024-10-20 21:49:00 [INFO]: Epoch 027 - training loss: 0.1612\n",
      "2024-10-20 21:49:00 [INFO]: Epoch 028 - training loss: 0.1045\n",
      "2024-10-20 21:49:00 [INFO]: Epoch 029 - training loss: 0.1115\n",
      "2024-10-20 21:49:00 [INFO]: Epoch 030 - training loss: 0.1377\n",
      "2024-10-20 21:49:00 [INFO]: Epoch 031 - training loss: 0.1940\n",
      "2024-10-20 21:49:01 [INFO]: Epoch 032 - training loss: 0.1377\n",
      "2024-10-20 21:49:01 [INFO]: Epoch 033 - training loss: 0.1958\n",
      "2024-10-20 21:49:01 [INFO]: Epoch 034 - training loss: 0.1335\n",
      "2024-10-20 21:49:01 [INFO]: Epoch 035 - training loss: 0.1182\n",
      "2024-10-20 21:49:01 [INFO]: Epoch 036 - training loss: 0.2000\n",
      "2024-10-20 21:49:01 [INFO]: Epoch 037 - training loss: 0.1840\n",
      "2024-10-20 21:49:02 [INFO]: Epoch 038 - training loss: 0.1590\n",
      "2024-10-20 21:49:02 [INFO]: Epoch 039 - training loss: 0.1790\n",
      "2024-10-20 21:49:02 [INFO]: Epoch 040 - training loss: 0.1838\n",
      "2024-10-20 21:49:02 [INFO]: Epoch 041 - training loss: 0.1218\n",
      "2024-10-20 21:49:02 [INFO]: Epoch 042 - training loss: 0.1913\n",
      "2024-10-20 21:49:02 [INFO]: Epoch 043 - training loss: 0.1537\n",
      "2024-10-20 21:49:02 [INFO]: Epoch 044 - training loss: 0.1383\n",
      "2024-10-20 21:49:03 [INFO]: Epoch 045 - training loss: 0.1997\n",
      "2024-10-20 21:49:03 [INFO]: Epoch 046 - training loss: 0.2554\n",
      "2024-10-20 21:49:03 [INFO]: Epoch 047 - training loss: 0.1217\n",
      "2024-10-20 21:49:03 [INFO]: Epoch 048 - training loss: 0.1398\n",
      "2024-10-20 21:49:03 [INFO]: Epoch 049 - training loss: 0.1633\n",
      "2024-10-20 21:49:03 [INFO]: Epoch 050 - training loss: 0.2555\n",
      "2024-10-20 21:49:03 [INFO]: Epoch 051 - training loss: 0.2540\n",
      "2024-10-20 21:49:04 [INFO]: Epoch 052 - training loss: 0.1237\n",
      "2024-10-20 21:49:04 [INFO]: Epoch 053 - training loss: 0.1690\n",
      "2024-10-20 21:49:04 [INFO]: Epoch 054 - training loss: 0.1386\n",
      "2024-10-20 21:49:04 [INFO]: Epoch 055 - training loss: 0.1563\n",
      "2024-10-20 21:49:04 [INFO]: Epoch 056 - training loss: 0.1889\n",
      "2024-10-20 21:49:04 [INFO]: Epoch 057 - training loss: 0.1791\n",
      "2024-10-20 21:49:04 [INFO]: Epoch 058 - training loss: 0.3010\n",
      "2024-10-20 21:49:05 [INFO]: Epoch 059 - training loss: 0.1358\n",
      "2024-10-20 21:49:05 [INFO]: Epoch 060 - training loss: 0.1321\n",
      "2024-10-20 21:49:05 [INFO]: Epoch 061 - training loss: 0.1457\n",
      "2024-10-20 21:49:05 [INFO]: Epoch 062 - training loss: 0.1844\n",
      "2024-10-20 21:49:05 [INFO]: Epoch 063 - training loss: 0.1767\n",
      "2024-10-20 21:49:05 [INFO]: Epoch 064 - training loss: 0.1170\n",
      "2024-10-20 21:49:06 [INFO]: Epoch 065 - training loss: 0.1638\n",
      "2024-10-20 21:49:06 [INFO]: Epoch 066 - training loss: 0.1853\n",
      "2024-10-20 21:49:06 [INFO]: Epoch 067 - training loss: 0.1499\n",
      "2024-10-20 21:49:06 [INFO]: Epoch 068 - training loss: 0.1462\n",
      "2024-10-20 21:49:06 [INFO]: Epoch 069 - training loss: 0.1498\n",
      "2024-10-20 21:49:06 [INFO]: Epoch 070 - training loss: 0.1732\n",
      "2024-10-20 21:49:06 [INFO]: Epoch 071 - training loss: 0.1598\n",
      "2024-10-20 21:49:07 [INFO]: Epoch 072 - training loss: 0.1976\n",
      "2024-10-20 21:49:07 [INFO]: Epoch 073 - training loss: 0.1477\n",
      "2024-10-20 21:49:07 [INFO]: Epoch 074 - training loss: 0.1542\n",
      "2024-10-20 21:49:07 [INFO]: Epoch 075 - training loss: 0.1313\n",
      "2024-10-20 21:49:07 [INFO]: Epoch 076 - training loss: 0.1650\n",
      "2024-10-20 21:49:07 [INFO]: Epoch 077 - training loss: 0.2995\n",
      "2024-10-20 21:49:08 [INFO]: Epoch 078 - training loss: 0.1043\n",
      "2024-10-20 21:49:08 [INFO]: Epoch 079 - training loss: 0.1632\n",
      "2024-10-20 21:49:08 [INFO]: Epoch 080 - training loss: 0.2125\n",
      "2024-10-20 21:49:08 [INFO]: Epoch 081 - training loss: 0.1963\n",
      "2024-10-20 21:49:08 [INFO]: Epoch 082 - training loss: 0.1530\n",
      "2024-10-20 21:49:08 [INFO]: Epoch 083 - training loss: 0.0970\n",
      "2024-10-20 21:49:08 [INFO]: Epoch 084 - training loss: 0.1777\n",
      "2024-10-20 21:49:09 [INFO]: Epoch 085 - training loss: 0.1672\n",
      "2024-10-20 21:49:09 [INFO]: Epoch 086 - training loss: 0.1906\n",
      "2024-10-20 21:49:09 [INFO]: Epoch 087 - training loss: 0.2192\n",
      "2024-10-20 21:49:09 [INFO]: Epoch 088 - training loss: 0.1869\n",
      "2024-10-20 21:49:09 [INFO]: Epoch 089 - training loss: 0.1108\n",
      "2024-10-20 21:49:09 [INFO]: Epoch 090 - training loss: 0.2558\n",
      "2024-10-20 21:49:10 [INFO]: Epoch 091 - training loss: 0.3055\n",
      "2024-10-20 21:49:10 [INFO]: Epoch 092 - training loss: 0.1902\n",
      "2024-10-20 21:49:10 [INFO]: Epoch 093 - training loss: 0.1437\n",
      "2024-10-20 21:49:10 [INFO]: Epoch 094 - training loss: 0.1251\n",
      "2024-10-20 21:49:10 [INFO]: Epoch 095 - training loss: 0.1448\n",
      "2024-10-20 21:49:10 [INFO]: Epoch 096 - training loss: 0.1638\n",
      "2024-10-20 21:49:10 [INFO]: Epoch 097 - training loss: 0.1298\n",
      "2024-10-20 21:49:11 [INFO]: Epoch 098 - training loss: 0.1357\n",
      "2024-10-20 21:49:11 [INFO]: Epoch 099 - training loss: 0.1531\n",
      "2024-10-20 21:49:11 [INFO]: Epoch 100 - training loss: 0.2746\n",
      "2024-10-20 21:49:11 [INFO]: Finished training. The best model is from epoch#83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:53:01 [INFO]: Epoch 001 - training loss: 0.2649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 13/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:53:01 [INFO]: Epoch 002 - training loss: 0.2238\n",
      "2024-10-20 21:53:01 [INFO]: Epoch 003 - training loss: 0.2542\n",
      "2024-10-20 21:53:01 [INFO]: Epoch 004 - training loss: 0.2104\n",
      "2024-10-20 21:53:01 [INFO]: Epoch 005 - training loss: 0.1708\n",
      "2024-10-20 21:53:01 [INFO]: Epoch 006 - training loss: 0.1663\n",
      "2024-10-20 21:53:02 [INFO]: Epoch 007 - training loss: 0.1204\n",
      "2024-10-20 21:53:02 [INFO]: Epoch 008 - training loss: 0.2703\n",
      "2024-10-20 21:53:02 [INFO]: Epoch 009 - training loss: 0.2427\n",
      "2024-10-20 21:53:02 [INFO]: Epoch 010 - training loss: 0.2328\n",
      "2024-10-20 21:53:02 [INFO]: Epoch 011 - training loss: 0.1558\n",
      "2024-10-20 21:53:02 [INFO]: Epoch 012 - training loss: 0.1483\n",
      "2024-10-20 21:53:02 [INFO]: Epoch 013 - training loss: 0.1360\n",
      "2024-10-20 21:53:03 [INFO]: Epoch 014 - training loss: 0.2298\n",
      "2024-10-20 21:53:03 [INFO]: Epoch 015 - training loss: 0.1370\n",
      "2024-10-20 21:53:03 [INFO]: Epoch 016 - training loss: 0.2008\n",
      "2024-10-20 21:53:03 [INFO]: Epoch 017 - training loss: 0.1827\n",
      "2024-10-20 21:53:03 [INFO]: Epoch 018 - training loss: 0.1517\n",
      "2024-10-20 21:53:03 [INFO]: Epoch 019 - training loss: 0.1535\n",
      "2024-10-20 21:53:03 [INFO]: Epoch 020 - training loss: 0.1941\n",
      "2024-10-20 21:53:04 [INFO]: Epoch 021 - training loss: 0.1645\n",
      "2024-10-20 21:53:04 [INFO]: Epoch 022 - training loss: 0.1844\n",
      "2024-10-20 21:53:04 [INFO]: Epoch 023 - training loss: 0.1726\n",
      "2024-10-20 21:53:04 [INFO]: Epoch 024 - training loss: 0.1356\n",
      "2024-10-20 21:53:04 [INFO]: Epoch 025 - training loss: 0.1860\n",
      "2024-10-20 21:53:04 [INFO]: Epoch 026 - training loss: 0.2179\n",
      "2024-10-20 21:53:05 [INFO]: Epoch 027 - training loss: 0.1271\n",
      "2024-10-20 21:53:05 [INFO]: Epoch 028 - training loss: 0.1416\n",
      "2024-10-20 21:53:05 [INFO]: Epoch 029 - training loss: 0.1599\n",
      "2024-10-20 21:53:05 [INFO]: Epoch 030 - training loss: 0.1525\n",
      "2024-10-20 21:53:05 [INFO]: Epoch 031 - training loss: 0.1593\n",
      "2024-10-20 21:53:05 [INFO]: Epoch 032 - training loss: 0.2801\n",
      "2024-10-20 21:53:05 [INFO]: Epoch 033 - training loss: 0.1743\n",
      "2024-10-20 21:53:06 [INFO]: Epoch 034 - training loss: 0.1736\n",
      "2024-10-20 21:53:06 [INFO]: Epoch 035 - training loss: 0.1656\n",
      "2024-10-20 21:53:06 [INFO]: Epoch 036 - training loss: 0.2848\n",
      "2024-10-20 21:53:06 [INFO]: Epoch 037 - training loss: 0.1034\n",
      "2024-10-20 21:53:06 [INFO]: Epoch 038 - training loss: 0.2201\n",
      "2024-10-20 21:53:06 [INFO]: Epoch 039 - training loss: 0.1513\n",
      "2024-10-20 21:53:06 [INFO]: Epoch 040 - training loss: 0.2628\n",
      "2024-10-20 21:53:07 [INFO]: Epoch 041 - training loss: 0.1354\n",
      "2024-10-20 21:53:07 [INFO]: Epoch 042 - training loss: 0.1953\n",
      "2024-10-20 21:53:07 [INFO]: Epoch 043 - training loss: 0.1248\n",
      "2024-10-20 21:53:07 [INFO]: Epoch 044 - training loss: 0.1639\n",
      "2024-10-20 21:53:07 [INFO]: Epoch 045 - training loss: 0.1940\n",
      "2024-10-20 21:53:07 [INFO]: Epoch 046 - training loss: 0.1816\n",
      "2024-10-20 21:53:08 [INFO]: Epoch 047 - training loss: 0.1580\n",
      "2024-10-20 21:53:08 [INFO]: Epoch 048 - training loss: 0.1273\n",
      "2024-10-20 21:53:08 [INFO]: Epoch 049 - training loss: 0.1376\n",
      "2024-10-20 21:53:08 [INFO]: Epoch 050 - training loss: 0.1834\n",
      "2024-10-20 21:53:08 [INFO]: Epoch 051 - training loss: 0.1485\n",
      "2024-10-20 21:53:08 [INFO]: Epoch 052 - training loss: 0.1539\n",
      "2024-10-20 21:53:08 [INFO]: Epoch 053 - training loss: 0.1673\n",
      "2024-10-20 21:53:09 [INFO]: Epoch 054 - training loss: 0.0885\n",
      "2024-10-20 21:53:09 [INFO]: Epoch 055 - training loss: 0.1242\n",
      "2024-10-20 21:53:09 [INFO]: Epoch 056 - training loss: 0.1265\n",
      "2024-10-20 21:53:09 [INFO]: Epoch 057 - training loss: 0.1554\n",
      "2024-10-20 21:53:09 [INFO]: Epoch 058 - training loss: 0.1234\n",
      "2024-10-20 21:53:09 [INFO]: Epoch 059 - training loss: 0.1703\n",
      "2024-10-20 21:53:09 [INFO]: Epoch 060 - training loss: 0.2010\n",
      "2024-10-20 21:53:10 [INFO]: Epoch 061 - training loss: 0.1332\n",
      "2024-10-20 21:53:10 [INFO]: Epoch 062 - training loss: 0.2060\n",
      "2024-10-20 21:53:10 [INFO]: Epoch 063 - training loss: 0.2374\n",
      "2024-10-20 21:53:10 [INFO]: Epoch 064 - training loss: 0.1114\n",
      "2024-10-20 21:53:10 [INFO]: Epoch 065 - training loss: 0.1451\n",
      "2024-10-20 21:53:10 [INFO]: Epoch 066 - training loss: 0.1491\n",
      "2024-10-20 21:53:10 [INFO]: Epoch 067 - training loss: 0.1367\n",
      "2024-10-20 21:53:11 [INFO]: Epoch 068 - training loss: 0.1152\n",
      "2024-10-20 21:53:11 [INFO]: Epoch 069 - training loss: 0.1935\n",
      "2024-10-20 21:53:11 [INFO]: Epoch 070 - training loss: 0.1642\n",
      "2024-10-20 21:53:11 [INFO]: Epoch 071 - training loss: 0.1195\n",
      "2024-10-20 21:53:11 [INFO]: Epoch 072 - training loss: 0.1536\n",
      "2024-10-20 21:53:11 [INFO]: Epoch 073 - training loss: 0.2176\n",
      "2024-10-20 21:53:12 [INFO]: Epoch 074 - training loss: 0.2434\n",
      "2024-10-20 21:53:12 [INFO]: Epoch 075 - training loss: 0.2146\n",
      "2024-10-20 21:53:12 [INFO]: Epoch 076 - training loss: 0.1174\n",
      "2024-10-20 21:53:12 [INFO]: Epoch 077 - training loss: 0.1465\n",
      "2024-10-20 21:53:12 [INFO]: Epoch 078 - training loss: 0.1465\n",
      "2024-10-20 21:53:12 [INFO]: Epoch 079 - training loss: 0.2353\n",
      "2024-10-20 21:53:12 [INFO]: Epoch 080 - training loss: 0.1960\n",
      "2024-10-20 21:53:13 [INFO]: Epoch 081 - training loss: 0.1367\n",
      "2024-10-20 21:53:13 [INFO]: Epoch 082 - training loss: 0.0960\n",
      "2024-10-20 21:53:13 [INFO]: Epoch 083 - training loss: 0.1225\n",
      "2024-10-20 21:53:13 [INFO]: Epoch 084 - training loss: 0.1334\n",
      "2024-10-20 21:53:13 [INFO]: Epoch 085 - training loss: 0.2134\n",
      "2024-10-20 21:53:13 [INFO]: Epoch 086 - training loss: 0.1680\n",
      "2024-10-20 21:53:13 [INFO]: Epoch 087 - training loss: 0.1382\n",
      "2024-10-20 21:53:14 [INFO]: Epoch 088 - training loss: 0.1466\n",
      "2024-10-20 21:53:14 [INFO]: Epoch 089 - training loss: 0.1841\n",
      "2024-10-20 21:53:14 [INFO]: Epoch 090 - training loss: 0.1935\n",
      "2024-10-20 21:53:14 [INFO]: Epoch 091 - training loss: 0.1521\n",
      "2024-10-20 21:53:14 [INFO]: Epoch 092 - training loss: 0.1203\n",
      "2024-10-20 21:53:14 [INFO]: Epoch 093 - training loss: 0.1397\n",
      "2024-10-20 21:53:14 [INFO]: Epoch 094 - training loss: 0.1752\n",
      "2024-10-20 21:53:15 [INFO]: Epoch 095 - training loss: 0.1273\n",
      "2024-10-20 21:53:15 [INFO]: Epoch 096 - training loss: 0.1366\n",
      "2024-10-20 21:53:15 [INFO]: Epoch 097 - training loss: 0.2031\n",
      "2024-10-20 21:53:15 [INFO]: Epoch 098 - training loss: 0.1329\n",
      "2024-10-20 21:53:15 [INFO]: Epoch 099 - training loss: 0.1444\n",
      "2024-10-20 21:53:15 [INFO]: Epoch 100 - training loss: 0.0996\n",
      "2024-10-20 21:53:15 [INFO]: Finished training. The best model is from epoch#54.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:57:04 [INFO]: Epoch 001 - training loss: 0.1728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 14/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 21:57:04 [INFO]: Epoch 002 - training loss: 0.1694\n",
      "2024-10-20 21:57:04 [INFO]: Epoch 003 - training loss: 0.2671\n",
      "2024-10-20 21:57:05 [INFO]: Epoch 004 - training loss: 0.1881\n",
      "2024-10-20 21:57:05 [INFO]: Epoch 005 - training loss: 0.1733\n",
      "2024-10-20 21:57:05 [INFO]: Epoch 006 - training loss: 0.2170\n",
      "2024-10-20 21:57:05 [INFO]: Epoch 007 - training loss: 0.1256\n",
      "2024-10-20 21:57:05 [INFO]: Epoch 008 - training loss: 0.1338\n",
      "2024-10-20 21:57:05 [INFO]: Epoch 009 - training loss: 0.1332\n",
      "2024-10-20 21:57:05 [INFO]: Epoch 010 - training loss: 0.1858\n",
      "2024-10-20 21:57:06 [INFO]: Epoch 011 - training loss: 0.1733\n",
      "2024-10-20 21:57:06 [INFO]: Epoch 012 - training loss: 0.1419\n",
      "2024-10-20 21:57:06 [INFO]: Epoch 013 - training loss: 0.1861\n",
      "2024-10-20 21:57:06 [INFO]: Epoch 014 - training loss: 0.1889\n",
      "2024-10-20 21:57:06 [INFO]: Epoch 015 - training loss: 0.1730\n",
      "2024-10-20 21:57:06 [INFO]: Epoch 016 - training loss: 0.1684\n",
      "2024-10-20 21:57:06 [INFO]: Epoch 017 - training loss: 0.1273\n",
      "2024-10-20 21:57:07 [INFO]: Epoch 018 - training loss: 0.2061\n",
      "2024-10-20 21:57:07 [INFO]: Epoch 019 - training loss: 0.1964\n",
      "2024-10-20 21:57:07 [INFO]: Epoch 020 - training loss: 0.2149\n",
      "2024-10-20 21:57:07 [INFO]: Epoch 021 - training loss: 0.1383\n",
      "2024-10-20 21:57:07 [INFO]: Epoch 022 - training loss: 0.1093\n",
      "2024-10-20 21:57:07 [INFO]: Epoch 023 - training loss: 0.1598\n",
      "2024-10-20 21:57:08 [INFO]: Epoch 024 - training loss: 0.1605\n",
      "2024-10-20 21:57:08 [INFO]: Epoch 025 - training loss: 0.1717\n",
      "2024-10-20 21:57:08 [INFO]: Epoch 026 - training loss: 0.1149\n",
      "2024-10-20 21:57:08 [INFO]: Epoch 027 - training loss: 0.1471\n",
      "2024-10-20 21:57:08 [INFO]: Epoch 028 - training loss: 0.1890\n",
      "2024-10-20 21:57:08 [INFO]: Epoch 029 - training loss: 0.1044\n",
      "2024-10-20 21:57:08 [INFO]: Epoch 030 - training loss: 0.1636\n",
      "2024-10-20 21:57:09 [INFO]: Epoch 031 - training loss: 0.1174\n",
      "2024-10-20 21:57:09 [INFO]: Epoch 032 - training loss: 0.1136\n",
      "2024-10-20 21:57:09 [INFO]: Epoch 033 - training loss: 0.1340\n",
      "2024-10-20 21:57:09 [INFO]: Epoch 034 - training loss: 0.0932\n",
      "2024-10-20 21:57:09 [INFO]: Epoch 035 - training loss: 0.1412\n",
      "2024-10-20 21:57:09 [INFO]: Epoch 036 - training loss: 0.1499\n",
      "2024-10-20 21:57:09 [INFO]: Epoch 037 - training loss: 0.1857\n",
      "2024-10-20 21:57:10 [INFO]: Epoch 038 - training loss: 0.1119\n",
      "2024-10-20 21:57:10 [INFO]: Epoch 039 - training loss: 0.2235\n",
      "2024-10-20 21:57:10 [INFO]: Epoch 040 - training loss: 0.1233\n",
      "2024-10-20 21:57:10 [INFO]: Epoch 041 - training loss: 0.0973\n",
      "2024-10-20 21:57:10 [INFO]: Epoch 042 - training loss: 0.2105\n",
      "2024-10-20 21:57:10 [INFO]: Epoch 043 - training loss: 0.1939\n",
      "2024-10-20 21:57:10 [INFO]: Epoch 044 - training loss: 0.1320\n",
      "2024-10-20 21:57:11 [INFO]: Epoch 045 - training loss: 0.1813\n",
      "2024-10-20 21:57:11 [INFO]: Epoch 046 - training loss: 0.1723\n",
      "2024-10-20 21:57:11 [INFO]: Epoch 047 - training loss: 0.1979\n",
      "2024-10-20 21:57:11 [INFO]: Epoch 048 - training loss: 0.1685\n",
      "2024-10-20 21:57:11 [INFO]: Epoch 049 - training loss: 0.1437\n",
      "2024-10-20 21:57:11 [INFO]: Epoch 050 - training loss: 0.2441\n",
      "2024-10-20 21:57:12 [INFO]: Epoch 051 - training loss: 0.1782\n",
      "2024-10-20 21:57:12 [INFO]: Epoch 052 - training loss: 0.1612\n",
      "2024-10-20 21:57:12 [INFO]: Epoch 053 - training loss: 0.1916\n",
      "2024-10-20 21:57:12 [INFO]: Epoch 054 - training loss: 0.1682\n",
      "2024-10-20 21:57:12 [INFO]: Epoch 055 - training loss: 0.1702\n",
      "2024-10-20 21:57:12 [INFO]: Epoch 056 - training loss: 0.1394\n",
      "2024-10-20 21:57:12 [INFO]: Epoch 057 - training loss: 0.1803\n",
      "2024-10-20 21:57:13 [INFO]: Epoch 058 - training loss: 0.1193\n",
      "2024-10-20 21:57:13 [INFO]: Epoch 059 - training loss: 0.3081\n",
      "2024-10-20 21:57:13 [INFO]: Epoch 060 - training loss: 0.1511\n",
      "2024-10-20 21:57:13 [INFO]: Epoch 061 - training loss: 0.1336\n",
      "2024-10-20 21:57:13 [INFO]: Epoch 062 - training loss: 0.1299\n",
      "2024-10-20 21:57:13 [INFO]: Epoch 063 - training loss: 0.2032\n",
      "2024-10-20 21:57:13 [INFO]: Epoch 064 - training loss: 0.1139\n",
      "2024-10-20 21:57:14 [INFO]: Epoch 065 - training loss: 0.1556\n",
      "2024-10-20 21:57:14 [INFO]: Epoch 066 - training loss: 0.1413\n",
      "2024-10-20 21:57:14 [INFO]: Epoch 067 - training loss: 0.1908\n",
      "2024-10-20 21:57:14 [INFO]: Epoch 068 - training loss: 0.1355\n",
      "2024-10-20 21:57:14 [INFO]: Epoch 069 - training loss: 0.2123\n",
      "2024-10-20 21:57:14 [INFO]: Epoch 070 - training loss: 0.0952\n",
      "2024-10-20 21:57:15 [INFO]: Epoch 071 - training loss: 0.1299\n",
      "2024-10-20 21:57:15 [INFO]: Epoch 072 - training loss: 0.1629\n",
      "2024-10-20 21:57:15 [INFO]: Epoch 073 - training loss: 0.2078\n",
      "2024-10-20 21:57:15 [INFO]: Epoch 074 - training loss: 0.2218\n",
      "2024-10-20 21:57:15 [INFO]: Epoch 075 - training loss: 0.1638\n",
      "2024-10-20 21:57:15 [INFO]: Epoch 076 - training loss: 0.1595\n",
      "2024-10-20 21:57:15 [INFO]: Epoch 077 - training loss: 0.1667\n",
      "2024-10-20 21:57:16 [INFO]: Epoch 078 - training loss: 0.1150\n",
      "2024-10-20 21:57:16 [INFO]: Epoch 079 - training loss: 0.1254\n",
      "2024-10-20 21:57:16 [INFO]: Epoch 080 - training loss: 0.0881\n",
      "2024-10-20 21:57:16 [INFO]: Epoch 081 - training loss: 0.1767\n",
      "2024-10-20 21:57:16 [INFO]: Epoch 082 - training loss: 0.1845\n",
      "2024-10-20 21:57:16 [INFO]: Epoch 083 - training loss: 0.1677\n",
      "2024-10-20 21:57:16 [INFO]: Epoch 084 - training loss: 0.1418\n",
      "2024-10-20 21:57:17 [INFO]: Epoch 085 - training loss: 0.1495\n",
      "2024-10-20 21:57:17 [INFO]: Epoch 086 - training loss: 0.1641\n",
      "2024-10-20 21:57:17 [INFO]: Epoch 087 - training loss: 0.1020\n",
      "2024-10-20 21:57:17 [INFO]: Epoch 088 - training loss: 0.1509\n",
      "2024-10-20 21:57:17 [INFO]: Epoch 089 - training loss: 0.1613\n",
      "2024-10-20 21:57:17 [INFO]: Epoch 090 - training loss: 0.1230\n",
      "2024-10-20 21:57:18 [INFO]: Epoch 091 - training loss: 0.2132\n",
      "2024-10-20 21:57:18 [INFO]: Epoch 092 - training loss: 0.1320\n",
      "2024-10-20 21:57:18 [INFO]: Epoch 093 - training loss: 0.1002\n",
      "2024-10-20 21:57:18 [INFO]: Epoch 094 - training loss: 0.1374\n",
      "2024-10-20 21:57:18 [INFO]: Epoch 095 - training loss: 0.2371\n",
      "2024-10-20 21:57:18 [INFO]: Epoch 096 - training loss: 0.1943\n",
      "2024-10-20 21:57:18 [INFO]: Epoch 097 - training loss: 0.0729\n",
      "2024-10-20 21:57:19 [INFO]: Epoch 098 - training loss: 0.1322\n",
      "2024-10-20 21:57:19 [INFO]: Epoch 099 - training loss: 0.1450\n",
      "2024-10-20 21:57:19 [INFO]: Epoch 100 - training loss: 0.1828\n",
      "2024-10-20 21:57:19 [INFO]: Finished training. The best model is from epoch#97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:01:08 [INFO]: Epoch 001 - training loss: 0.1067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 15/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:01:08 [INFO]: Epoch 002 - training loss: 0.1989\n",
      "2024-10-20 22:01:08 [INFO]: Epoch 003 - training loss: 0.1192\n",
      "2024-10-20 22:01:08 [INFO]: Epoch 004 - training loss: 0.2546\n",
      "2024-10-20 22:01:09 [INFO]: Epoch 005 - training loss: 0.1602\n",
      "2024-10-20 22:01:09 [INFO]: Epoch 006 - training loss: 0.2218\n",
      "2024-10-20 22:01:09 [INFO]: Epoch 007 - training loss: 0.2151\n",
      "2024-10-20 22:01:09 [INFO]: Epoch 008 - training loss: 0.1522\n",
      "2024-10-20 22:01:09 [INFO]: Epoch 009 - training loss: 0.1760\n",
      "2024-10-20 22:01:09 [INFO]: Epoch 010 - training loss: 0.2190\n",
      "2024-10-20 22:01:10 [INFO]: Epoch 011 - training loss: 0.1187\n",
      "2024-10-20 22:01:10 [INFO]: Epoch 012 - training loss: 0.1394\n",
      "2024-10-20 22:01:10 [INFO]: Epoch 013 - training loss: 0.2133\n",
      "2024-10-20 22:01:10 [INFO]: Epoch 014 - training loss: 0.2565\n",
      "2024-10-20 22:01:10 [INFO]: Epoch 015 - training loss: 0.1382\n",
      "2024-10-20 22:01:10 [INFO]: Epoch 016 - training loss: 0.1426\n",
      "2024-10-20 22:01:10 [INFO]: Epoch 017 - training loss: 0.2094\n",
      "2024-10-20 22:01:11 [INFO]: Epoch 018 - training loss: 0.1753\n",
      "2024-10-20 22:01:11 [INFO]: Epoch 019 - training loss: 0.1795\n",
      "2024-10-20 22:01:11 [INFO]: Epoch 020 - training loss: 0.1497\n",
      "2024-10-20 22:01:11 [INFO]: Epoch 021 - training loss: 0.1976\n",
      "2024-10-20 22:01:11 [INFO]: Epoch 022 - training loss: 0.2295\n",
      "2024-10-20 22:01:11 [INFO]: Epoch 023 - training loss: 0.1732\n",
      "2024-10-20 22:01:12 [INFO]: Epoch 024 - training loss: 0.1669\n",
      "2024-10-20 22:01:12 [INFO]: Epoch 025 - training loss: 0.2546\n",
      "2024-10-20 22:01:12 [INFO]: Epoch 026 - training loss: 0.2400\n",
      "2024-10-20 22:01:12 [INFO]: Epoch 027 - training loss: 0.1654\n",
      "2024-10-20 22:01:12 [INFO]: Epoch 028 - training loss: 0.1850\n",
      "2024-10-20 22:01:12 [INFO]: Epoch 029 - training loss: 0.1340\n",
      "2024-10-20 22:01:12 [INFO]: Epoch 030 - training loss: 0.1700\n",
      "2024-10-20 22:01:13 [INFO]: Epoch 031 - training loss: 0.2737\n",
      "2024-10-20 22:01:13 [INFO]: Epoch 032 - training loss: 0.1125\n",
      "2024-10-20 22:01:13 [INFO]: Epoch 033 - training loss: 0.1341\n",
      "2024-10-20 22:01:13 [INFO]: Epoch 034 - training loss: 0.1458\n",
      "2024-10-20 22:01:13 [INFO]: Epoch 035 - training loss: 0.1852\n",
      "2024-10-20 22:01:13 [INFO]: Epoch 036 - training loss: 0.0961\n",
      "2024-10-20 22:01:14 [INFO]: Epoch 037 - training loss: 0.1160\n",
      "2024-10-20 22:01:14 [INFO]: Epoch 038 - training loss: 0.2632\n",
      "2024-10-20 22:01:14 [INFO]: Epoch 039 - training loss: 0.2258\n",
      "2024-10-20 22:01:14 [INFO]: Epoch 040 - training loss: 0.1292\n",
      "2024-10-20 22:01:14 [INFO]: Epoch 041 - training loss: 0.2258\n",
      "2024-10-20 22:01:14 [INFO]: Epoch 042 - training loss: 0.2209\n",
      "2024-10-20 22:01:14 [INFO]: Epoch 043 - training loss: 0.1466\n",
      "2024-10-20 22:01:15 [INFO]: Epoch 044 - training loss: 0.0861\n",
      "2024-10-20 22:01:15 [INFO]: Epoch 045 - training loss: 0.1419\n",
      "2024-10-20 22:01:15 [INFO]: Epoch 046 - training loss: 0.1990\n",
      "2024-10-20 22:01:15 [INFO]: Epoch 047 - training loss: 0.2734\n",
      "2024-10-20 22:01:15 [INFO]: Epoch 048 - training loss: 0.1224\n",
      "2024-10-20 22:01:15 [INFO]: Epoch 049 - training loss: 0.1648\n",
      "2024-10-20 22:01:16 [INFO]: Epoch 050 - training loss: 0.1612\n",
      "2024-10-20 22:01:16 [INFO]: Epoch 051 - training loss: 0.1556\n",
      "2024-10-20 22:01:16 [INFO]: Epoch 052 - training loss: 0.1397\n",
      "2024-10-20 22:01:16 [INFO]: Epoch 053 - training loss: 0.1151\n",
      "2024-10-20 22:01:16 [INFO]: Epoch 054 - training loss: 0.2293\n",
      "2024-10-20 22:01:16 [INFO]: Epoch 055 - training loss: 0.1021\n",
      "2024-10-20 22:01:16 [INFO]: Epoch 056 - training loss: 0.1158\n",
      "2024-10-20 22:01:17 [INFO]: Epoch 057 - training loss: 0.1436\n",
      "2024-10-20 22:01:17 [INFO]: Epoch 058 - training loss: 0.2054\n",
      "2024-10-20 22:01:17 [INFO]: Epoch 059 - training loss: 0.2166\n",
      "2024-10-20 22:01:17 [INFO]: Epoch 060 - training loss: 0.1793\n",
      "2024-10-20 22:01:17 [INFO]: Epoch 061 - training loss: 0.1592\n",
      "2024-10-20 22:01:17 [INFO]: Epoch 062 - training loss: 0.1566\n",
      "2024-10-20 22:01:18 [INFO]: Epoch 063 - training loss: 0.1338\n",
      "2024-10-20 22:01:18 [INFO]: Epoch 064 - training loss: 0.2807\n",
      "2024-10-20 22:01:18 [INFO]: Epoch 065 - training loss: 0.2028\n",
      "2024-10-20 22:01:18 [INFO]: Epoch 066 - training loss: 0.1648\n",
      "2024-10-20 22:01:18 [INFO]: Epoch 067 - training loss: 0.2459\n",
      "2024-10-20 22:01:18 [INFO]: Epoch 068 - training loss: 0.1202\n",
      "2024-10-20 22:01:18 [INFO]: Epoch 069 - training loss: 0.3433\n",
      "2024-10-20 22:01:19 [INFO]: Epoch 070 - training loss: 0.1857\n",
      "2024-10-20 22:01:19 [INFO]: Epoch 071 - training loss: 0.1143\n",
      "2024-10-20 22:01:19 [INFO]: Epoch 072 - training loss: 0.1353\n",
      "2024-10-20 22:01:19 [INFO]: Epoch 073 - training loss: 0.1543\n",
      "2024-10-20 22:01:19 [INFO]: Epoch 074 - training loss: 0.1676\n",
      "2024-10-20 22:01:19 [INFO]: Epoch 075 - training loss: 0.1716\n",
      "2024-10-20 22:01:19 [INFO]: Epoch 076 - training loss: 0.1186\n",
      "2024-10-20 22:01:20 [INFO]: Epoch 077 - training loss: 0.1261\n",
      "2024-10-20 22:01:20 [INFO]: Epoch 078 - training loss: 0.1649\n",
      "2024-10-20 22:01:20 [INFO]: Epoch 079 - training loss: 0.2140\n",
      "2024-10-20 22:01:20 [INFO]: Epoch 080 - training loss: 0.2036\n",
      "2024-10-20 22:01:20 [INFO]: Epoch 081 - training loss: 0.1368\n",
      "2024-10-20 22:01:20 [INFO]: Epoch 082 - training loss: 0.1621\n",
      "2024-10-20 22:01:21 [INFO]: Epoch 083 - training loss: 0.1414\n",
      "2024-10-20 22:01:21 [INFO]: Epoch 084 - training loss: 0.1081\n",
      "2024-10-20 22:01:21 [INFO]: Epoch 085 - training loss: 0.2367\n",
      "2024-10-20 22:01:21 [INFO]: Epoch 086 - training loss: 0.2016\n",
      "2024-10-20 22:01:21 [INFO]: Epoch 087 - training loss: 0.1703\n",
      "2024-10-20 22:01:21 [INFO]: Epoch 088 - training loss: 0.1430\n",
      "2024-10-20 22:01:21 [INFO]: Epoch 089 - training loss: 0.1662\n",
      "2024-10-20 22:01:22 [INFO]: Epoch 090 - training loss: 0.2621\n",
      "2024-10-20 22:01:22 [INFO]: Epoch 091 - training loss: 0.2011\n",
      "2024-10-20 22:01:22 [INFO]: Epoch 092 - training loss: 0.2667\n",
      "2024-10-20 22:01:22 [INFO]: Epoch 093 - training loss: 0.2002\n",
      "2024-10-20 22:01:22 [INFO]: Epoch 094 - training loss: 0.2607\n",
      "2024-10-20 22:01:22 [INFO]: Epoch 095 - training loss: 0.1715\n",
      "2024-10-20 22:01:22 [INFO]: Epoch 096 - training loss: 0.1303\n",
      "2024-10-20 22:01:23 [INFO]: Epoch 097 - training loss: 0.1802\n",
      "2024-10-20 22:01:23 [INFO]: Epoch 098 - training loss: 0.1862\n",
      "2024-10-20 22:01:23 [INFO]: Epoch 099 - training loss: 0.1648\n",
      "2024-10-20 22:01:23 [INFO]: Epoch 100 - training loss: 0.1937\n",
      "2024-10-20 22:01:23 [INFO]: Finished training. The best model is from epoch#44.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:05:12 [INFO]: Epoch 001 - training loss: 0.2019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 16/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:05:12 [INFO]: Epoch 002 - training loss: 0.1799\n",
      "2024-10-20 22:05:13 [INFO]: Epoch 003 - training loss: 0.1611\n",
      "2024-10-20 22:05:13 [INFO]: Epoch 004 - training loss: 0.2254\n",
      "2024-10-20 22:05:13 [INFO]: Epoch 005 - training loss: 0.2020\n",
      "2024-10-20 22:05:13 [INFO]: Epoch 006 - training loss: 0.3139\n",
      "2024-10-20 22:05:13 [INFO]: Epoch 007 - training loss: 0.1372\n",
      "2024-10-20 22:05:13 [INFO]: Epoch 008 - training loss: 0.1531\n",
      "2024-10-20 22:05:13 [INFO]: Epoch 009 - training loss: 0.1391\n",
      "2024-10-20 22:05:14 [INFO]: Epoch 010 - training loss: 0.1522\n",
      "2024-10-20 22:05:14 [INFO]: Epoch 011 - training loss: 0.1761\n",
      "2024-10-20 22:05:14 [INFO]: Epoch 012 - training loss: 0.1070\n",
      "2024-10-20 22:05:14 [INFO]: Epoch 013 - training loss: 0.0966\n",
      "2024-10-20 22:05:14 [INFO]: Epoch 014 - training loss: 0.0951\n",
      "2024-10-20 22:05:14 [INFO]: Epoch 015 - training loss: 0.1598\n",
      "2024-10-20 22:05:14 [INFO]: Epoch 016 - training loss: 0.1563\n",
      "2024-10-20 22:05:15 [INFO]: Epoch 017 - training loss: 0.1348\n",
      "2024-10-20 22:05:15 [INFO]: Epoch 018 - training loss: 0.1545\n",
      "2024-10-20 22:05:15 [INFO]: Epoch 019 - training loss: 0.1779\n",
      "2024-10-20 22:05:15 [INFO]: Epoch 020 - training loss: 0.1865\n",
      "2024-10-20 22:05:15 [INFO]: Epoch 021 - training loss: 0.1501\n",
      "2024-10-20 22:05:15 [INFO]: Epoch 022 - training loss: 0.1241\n",
      "2024-10-20 22:05:16 [INFO]: Epoch 023 - training loss: 0.2413\n",
      "2024-10-20 22:05:16 [INFO]: Epoch 024 - training loss: 0.1712\n",
      "2024-10-20 22:05:16 [INFO]: Epoch 025 - training loss: 0.1603\n",
      "2024-10-20 22:05:16 [INFO]: Epoch 026 - training loss: 0.2473\n",
      "2024-10-20 22:05:16 [INFO]: Epoch 027 - training loss: 0.1975\n",
      "2024-10-20 22:05:16 [INFO]: Epoch 028 - training loss: 0.1301\n",
      "2024-10-20 22:05:16 [INFO]: Epoch 029 - training loss: 0.2985\n",
      "2024-10-20 22:05:17 [INFO]: Epoch 030 - training loss: 0.1519\n",
      "2024-10-20 22:05:17 [INFO]: Epoch 031 - training loss: 0.1372\n",
      "2024-10-20 22:05:17 [INFO]: Epoch 032 - training loss: 0.1814\n",
      "2024-10-20 22:05:17 [INFO]: Epoch 033 - training loss: 0.1266\n",
      "2024-10-20 22:05:17 [INFO]: Epoch 034 - training loss: 0.1998\n",
      "2024-10-20 22:05:17 [INFO]: Epoch 035 - training loss: 0.1451\n",
      "2024-10-20 22:05:17 [INFO]: Epoch 036 - training loss: 0.1435\n",
      "2024-10-20 22:05:18 [INFO]: Epoch 037 - training loss: 0.1468\n",
      "2024-10-20 22:05:18 [INFO]: Epoch 038 - training loss: 0.1659\n",
      "2024-10-20 22:05:18 [INFO]: Epoch 039 - training loss: 0.2419\n",
      "2024-10-20 22:05:18 [INFO]: Epoch 040 - training loss: 0.1250\n",
      "2024-10-20 22:05:18 [INFO]: Epoch 041 - training loss: 0.1322\n",
      "2024-10-20 22:05:18 [INFO]: Epoch 042 - training loss: 0.1409\n",
      "2024-10-20 22:05:18 [INFO]: Epoch 043 - training loss: 0.1797\n",
      "2024-10-20 22:05:19 [INFO]: Epoch 044 - training loss: 0.1671\n",
      "2024-10-20 22:05:19 [INFO]: Epoch 045 - training loss: 0.1418\n",
      "2024-10-20 22:05:19 [INFO]: Epoch 046 - training loss: 0.1448\n",
      "2024-10-20 22:05:19 [INFO]: Epoch 047 - training loss: 0.1438\n",
      "2024-10-20 22:05:19 [INFO]: Epoch 048 - training loss: 0.1180\n",
      "2024-10-20 22:05:19 [INFO]: Epoch 049 - training loss: 0.1664\n",
      "2024-10-20 22:05:20 [INFO]: Epoch 050 - training loss: 0.1622\n",
      "2024-10-20 22:05:20 [INFO]: Epoch 051 - training loss: 0.1305\n",
      "2024-10-20 22:05:20 [INFO]: Epoch 052 - training loss: 0.2135\n",
      "2024-10-20 22:05:20 [INFO]: Epoch 053 - training loss: 0.1849\n",
      "2024-10-20 22:05:20 [INFO]: Epoch 054 - training loss: 0.1725\n",
      "2024-10-20 22:05:20 [INFO]: Epoch 055 - training loss: 0.2189\n",
      "2024-10-20 22:05:20 [INFO]: Epoch 056 - training loss: 0.2438\n",
      "2024-10-20 22:05:21 [INFO]: Epoch 057 - training loss: 0.1618\n",
      "2024-10-20 22:05:21 [INFO]: Epoch 058 - training loss: 0.2255\n",
      "2024-10-20 22:05:21 [INFO]: Epoch 059 - training loss: 0.1421\n",
      "2024-10-20 22:05:21 [INFO]: Epoch 060 - training loss: 0.1400\n",
      "2024-10-20 22:05:21 [INFO]: Epoch 061 - training loss: 0.1821\n",
      "2024-10-20 22:05:21 [INFO]: Epoch 062 - training loss: 0.1560\n",
      "2024-10-20 22:05:21 [INFO]: Epoch 063 - training loss: 0.1924\n",
      "2024-10-20 22:05:22 [INFO]: Epoch 064 - training loss: 0.1429\n",
      "2024-10-20 22:05:22 [INFO]: Epoch 065 - training loss: 0.2775\n",
      "2024-10-20 22:05:22 [INFO]: Epoch 066 - training loss: 0.1429\n",
      "2024-10-20 22:05:22 [INFO]: Epoch 067 - training loss: 0.2706\n",
      "2024-10-20 22:05:22 [INFO]: Epoch 068 - training loss: 0.1277\n",
      "2024-10-20 22:05:22 [INFO]: Epoch 069 - training loss: 0.2661\n",
      "2024-10-20 22:05:22 [INFO]: Epoch 070 - training loss: 0.2528\n",
      "2024-10-20 22:05:23 [INFO]: Epoch 071 - training loss: 0.1311\n",
      "2024-10-20 22:05:23 [INFO]: Epoch 072 - training loss: 0.1680\n",
      "2024-10-20 22:05:23 [INFO]: Epoch 073 - training loss: 0.1252\n",
      "2024-10-20 22:05:23 [INFO]: Epoch 074 - training loss: 0.1325\n",
      "2024-10-20 22:05:23 [INFO]: Epoch 075 - training loss: 0.1578\n",
      "2024-10-20 22:05:23 [INFO]: Epoch 076 - training loss: 0.1326\n",
      "2024-10-20 22:05:24 [INFO]: Epoch 077 - training loss: 0.1829\n",
      "2024-10-20 22:05:24 [INFO]: Epoch 078 - training loss: 0.1329\n",
      "2024-10-20 22:05:24 [INFO]: Epoch 079 - training loss: 0.1249\n",
      "2024-10-20 22:05:24 [INFO]: Epoch 080 - training loss: 0.1312\n",
      "2024-10-20 22:05:24 [INFO]: Epoch 081 - training loss: 0.1644\n",
      "2024-10-20 22:05:24 [INFO]: Epoch 082 - training loss: 0.1784\n",
      "2024-10-20 22:05:24 [INFO]: Epoch 083 - training loss: 0.1465\n",
      "2024-10-20 22:05:25 [INFO]: Epoch 084 - training loss: 0.1550\n",
      "2024-10-20 22:05:25 [INFO]: Epoch 085 - training loss: 0.1788\n",
      "2024-10-20 22:05:25 [INFO]: Epoch 086 - training loss: 0.1496\n",
      "2024-10-20 22:05:25 [INFO]: Epoch 087 - training loss: 0.1772\n",
      "2024-10-20 22:05:25 [INFO]: Epoch 088 - training loss: 0.0945\n",
      "2024-10-20 22:05:25 [INFO]: Epoch 089 - training loss: 0.1111\n",
      "2024-10-20 22:05:25 [INFO]: Epoch 090 - training loss: 0.1440\n",
      "2024-10-20 22:05:26 [INFO]: Epoch 091 - training loss: 0.1738\n",
      "2024-10-20 22:05:26 [INFO]: Epoch 092 - training loss: 0.1462\n",
      "2024-10-20 22:05:26 [INFO]: Epoch 093 - training loss: 0.1254\n",
      "2024-10-20 22:05:26 [INFO]: Epoch 094 - training loss: 0.1694\n",
      "2024-10-20 22:05:26 [INFO]: Epoch 095 - training loss: 0.1548\n",
      "2024-10-20 22:05:26 [INFO]: Epoch 096 - training loss: 0.1834\n",
      "2024-10-20 22:05:26 [INFO]: Epoch 097 - training loss: 0.2038\n",
      "2024-10-20 22:05:27 [INFO]: Epoch 098 - training loss: 0.1810\n",
      "2024-10-20 22:05:27 [INFO]: Epoch 099 - training loss: 0.2442\n",
      "2024-10-20 22:05:27 [INFO]: Epoch 100 - training loss: 0.1562\n",
      "2024-10-20 22:05:27 [INFO]: Finished training. The best model is from epoch#88.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:09:16 [INFO]: Epoch 001 - training loss: 0.1190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 17/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:09:16 [INFO]: Epoch 002 - training loss: 0.1671\n",
      "2024-10-20 22:09:16 [INFO]: Epoch 003 - training loss: 0.2084\n",
      "2024-10-20 22:09:16 [INFO]: Epoch 004 - training loss: 0.1524\n",
      "2024-10-20 22:09:17 [INFO]: Epoch 005 - training loss: 0.1477\n",
      "2024-10-20 22:09:17 [INFO]: Epoch 006 - training loss: 0.2913\n",
      "2024-10-20 22:09:17 [INFO]: Epoch 007 - training loss: 0.1494\n",
      "2024-10-20 22:09:17 [INFO]: Epoch 008 - training loss: 0.1591\n",
      "2024-10-20 22:09:17 [INFO]: Epoch 009 - training loss: 0.1143\n",
      "2024-10-20 22:09:17 [INFO]: Epoch 010 - training loss: 0.2173\n",
      "2024-10-20 22:09:18 [INFO]: Epoch 011 - training loss: 0.1941\n",
      "2024-10-20 22:09:18 [INFO]: Epoch 012 - training loss: 0.1305\n",
      "2024-10-20 22:09:18 [INFO]: Epoch 013 - training loss: 0.2034\n",
      "2024-10-20 22:09:18 [INFO]: Epoch 014 - training loss: 0.2747\n",
      "2024-10-20 22:09:18 [INFO]: Epoch 015 - training loss: 0.1293\n",
      "2024-10-20 22:09:18 [INFO]: Epoch 016 - training loss: 0.1607\n",
      "2024-10-20 22:09:18 [INFO]: Epoch 017 - training loss: 0.1738\n",
      "2024-10-20 22:09:19 [INFO]: Epoch 018 - training loss: 0.1248\n",
      "2024-10-20 22:09:19 [INFO]: Epoch 019 - training loss: 0.1525\n",
      "2024-10-20 22:09:19 [INFO]: Epoch 020 - training loss: 0.1576\n",
      "2024-10-20 22:09:19 [INFO]: Epoch 021 - training loss: 0.1930\n",
      "2024-10-20 22:09:19 [INFO]: Epoch 022 - training loss: 0.1506\n",
      "2024-10-20 22:09:19 [INFO]: Epoch 023 - training loss: 0.1697\n",
      "2024-10-20 22:09:19 [INFO]: Epoch 024 - training loss: 0.2665\n",
      "2024-10-20 22:09:20 [INFO]: Epoch 025 - training loss: 0.2048\n",
      "2024-10-20 22:09:20 [INFO]: Epoch 026 - training loss: 0.1695\n",
      "2024-10-20 22:09:20 [INFO]: Epoch 027 - training loss: 0.2391\n",
      "2024-10-20 22:09:20 [INFO]: Epoch 028 - training loss: 0.1819\n",
      "2024-10-20 22:09:20 [INFO]: Epoch 029 - training loss: 0.2296\n",
      "2024-10-20 22:09:20 [INFO]: Epoch 030 - training loss: 0.2123\n",
      "2024-10-20 22:09:20 [INFO]: Epoch 031 - training loss: 0.1280\n",
      "2024-10-20 22:09:21 [INFO]: Epoch 032 - training loss: 0.1923\n",
      "2024-10-20 22:09:21 [INFO]: Epoch 033 - training loss: 0.2160\n",
      "2024-10-20 22:09:21 [INFO]: Epoch 034 - training loss: 0.0959\n",
      "2024-10-20 22:09:21 [INFO]: Epoch 035 - training loss: 0.1690\n",
      "2024-10-20 22:09:21 [INFO]: Epoch 036 - training loss: 0.1494\n",
      "2024-10-20 22:09:21 [INFO]: Epoch 037 - training loss: 0.1280\n",
      "2024-10-20 22:09:22 [INFO]: Epoch 038 - training loss: 0.1687\n",
      "2024-10-20 22:09:22 [INFO]: Epoch 039 - training loss: 0.3314\n",
      "2024-10-20 22:09:22 [INFO]: Epoch 040 - training loss: 0.1255\n",
      "2024-10-20 22:09:22 [INFO]: Epoch 041 - training loss: 0.1471\n",
      "2024-10-20 22:09:22 [INFO]: Epoch 042 - training loss: 0.1468\n",
      "2024-10-20 22:09:22 [INFO]: Epoch 043 - training loss: 0.2153\n",
      "2024-10-20 22:09:22 [INFO]: Epoch 044 - training loss: 0.1439\n",
      "2024-10-20 22:09:23 [INFO]: Epoch 045 - training loss: 0.2144\n",
      "2024-10-20 22:09:23 [INFO]: Epoch 046 - training loss: 0.1601\n",
      "2024-10-20 22:09:23 [INFO]: Epoch 047 - training loss: 0.1135\n",
      "2024-10-20 22:09:23 [INFO]: Epoch 048 - training loss: 0.1792\n",
      "2024-10-20 22:09:23 [INFO]: Epoch 049 - training loss: 0.1563\n",
      "2024-10-20 22:09:23 [INFO]: Epoch 050 - training loss: 0.2059\n",
      "2024-10-20 22:09:23 [INFO]: Epoch 051 - training loss: 0.2055\n",
      "2024-10-20 22:09:24 [INFO]: Epoch 052 - training loss: 0.2893\n",
      "2024-10-20 22:09:24 [INFO]: Epoch 053 - training loss: 0.1865\n",
      "2024-10-20 22:09:24 [INFO]: Epoch 054 - training loss: 0.1256\n",
      "2024-10-20 22:09:24 [INFO]: Epoch 055 - training loss: 0.1341\n",
      "2024-10-20 22:09:24 [INFO]: Epoch 056 - training loss: 0.1640\n",
      "2024-10-20 22:09:24 [INFO]: Epoch 057 - training loss: 0.1648\n",
      "2024-10-20 22:09:24 [INFO]: Epoch 058 - training loss: 0.1095\n",
      "2024-10-20 22:09:25 [INFO]: Epoch 059 - training loss: 0.2362\n",
      "2024-10-20 22:09:25 [INFO]: Epoch 060 - training loss: 0.1319\n",
      "2024-10-20 22:09:25 [INFO]: Epoch 061 - training loss: 0.1547\n",
      "2024-10-20 22:09:25 [INFO]: Epoch 062 - training loss: 0.1385\n",
      "2024-10-20 22:09:25 [INFO]: Epoch 063 - training loss: 0.1079\n",
      "2024-10-20 22:09:25 [INFO]: Epoch 064 - training loss: 0.1797\n",
      "2024-10-20 22:09:25 [INFO]: Epoch 065 - training loss: 0.0972\n",
      "2024-10-20 22:09:26 [INFO]: Epoch 066 - training loss: 0.1693\n",
      "2024-10-20 22:09:26 [INFO]: Epoch 067 - training loss: 0.1895\n",
      "2024-10-20 22:09:26 [INFO]: Epoch 068 - training loss: 0.2146\n",
      "2024-10-20 22:09:26 [INFO]: Epoch 069 - training loss: 0.1579\n",
      "2024-10-20 22:09:26 [INFO]: Epoch 070 - training loss: 0.1176\n",
      "2024-10-20 22:09:26 [INFO]: Epoch 071 - training loss: 0.1855\n",
      "2024-10-20 22:09:27 [INFO]: Epoch 072 - training loss: 0.1244\n",
      "2024-10-20 22:09:27 [INFO]: Epoch 073 - training loss: 0.1110\n",
      "2024-10-20 22:09:27 [INFO]: Epoch 074 - training loss: 0.1272\n",
      "2024-10-20 22:09:27 [INFO]: Epoch 075 - training loss: 0.1226\n",
      "2024-10-20 22:09:27 [INFO]: Epoch 076 - training loss: 0.1223\n",
      "2024-10-20 22:09:27 [INFO]: Epoch 077 - training loss: 0.1026\n",
      "2024-10-20 22:09:27 [INFO]: Epoch 078 - training loss: 0.0991\n",
      "2024-10-20 22:09:28 [INFO]: Epoch 079 - training loss: 0.1913\n",
      "2024-10-20 22:09:28 [INFO]: Epoch 080 - training loss: 0.1210\n",
      "2024-10-20 22:09:28 [INFO]: Epoch 081 - training loss: 0.1390\n",
      "2024-10-20 22:09:28 [INFO]: Epoch 082 - training loss: 0.2280\n",
      "2024-10-20 22:09:28 [INFO]: Epoch 083 - training loss: 0.1329\n",
      "2024-10-20 22:09:28 [INFO]: Epoch 084 - training loss: 0.1345\n",
      "2024-10-20 22:09:28 [INFO]: Epoch 085 - training loss: 0.1780\n",
      "2024-10-20 22:09:29 [INFO]: Epoch 086 - training loss: 0.1796\n",
      "2024-10-20 22:09:29 [INFO]: Epoch 087 - training loss: 0.1477\n",
      "2024-10-20 22:09:29 [INFO]: Epoch 088 - training loss: 0.1344\n",
      "2024-10-20 22:09:29 [INFO]: Epoch 089 - training loss: 0.1415\n",
      "2024-10-20 22:09:29 [INFO]: Epoch 090 - training loss: 0.1478\n",
      "2024-10-20 22:09:29 [INFO]: Epoch 091 - training loss: 0.1332\n",
      "2024-10-20 22:09:29 [INFO]: Epoch 092 - training loss: 0.1483\n",
      "2024-10-20 22:09:30 [INFO]: Epoch 093 - training loss: 0.1149\n",
      "2024-10-20 22:09:30 [INFO]: Epoch 094 - training loss: 0.2233\n",
      "2024-10-20 22:09:30 [INFO]: Epoch 095 - training loss: 0.1360\n",
      "2024-10-20 22:09:30 [INFO]: Epoch 096 - training loss: 0.1752\n",
      "2024-10-20 22:09:30 [INFO]: Epoch 097 - training loss: 0.1443\n",
      "2024-10-20 22:09:30 [INFO]: Epoch 098 - training loss: 0.2231\n",
      "2024-10-20 22:09:31 [INFO]: Epoch 099 - training loss: 0.1279\n",
      "2024-10-20 22:09:31 [INFO]: Epoch 100 - training loss: 0.1157\n",
      "2024-10-20 22:09:31 [INFO]: Finished training. The best model is from epoch#34.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:13:20 [INFO]: Epoch 001 - training loss: 0.1333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 18/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:13:20 [INFO]: Epoch 002 - training loss: 0.1155\n",
      "2024-10-20 22:13:20 [INFO]: Epoch 003 - training loss: 0.1950\n",
      "2024-10-20 22:13:20 [INFO]: Epoch 004 - training loss: 0.1378\n",
      "2024-10-20 22:13:20 [INFO]: Epoch 005 - training loss: 0.2004\n",
      "2024-10-20 22:13:20 [INFO]: Epoch 006 - training loss: 0.2666\n",
      "2024-10-20 22:13:20 [INFO]: Epoch 007 - training loss: 0.1898\n",
      "2024-10-20 22:13:21 [INFO]: Epoch 008 - training loss: 0.1750\n",
      "2024-10-20 22:13:21 [INFO]: Epoch 009 - training loss: 0.0913\n",
      "2024-10-20 22:13:21 [INFO]: Epoch 010 - training loss: 0.1661\n",
      "2024-10-20 22:13:21 [INFO]: Epoch 011 - training loss: 0.0710\n",
      "2024-10-20 22:13:21 [INFO]: Epoch 012 - training loss: 0.1231\n",
      "2024-10-20 22:13:21 [INFO]: Epoch 013 - training loss: 0.1167\n",
      "2024-10-20 22:13:21 [INFO]: Epoch 014 - training loss: 0.1180\n",
      "2024-10-20 22:13:22 [INFO]: Epoch 015 - training loss: 0.2302\n",
      "2024-10-20 22:13:22 [INFO]: Epoch 016 - training loss: 0.2328\n",
      "2024-10-20 22:13:22 [INFO]: Epoch 017 - training loss: 0.1842\n",
      "2024-10-20 22:13:22 [INFO]: Epoch 018 - training loss: 0.1747\n",
      "2024-10-20 22:13:22 [INFO]: Epoch 019 - training loss: 0.1960\n",
      "2024-10-20 22:13:22 [INFO]: Epoch 020 - training loss: 0.1433\n",
      "2024-10-20 22:13:23 [INFO]: Epoch 021 - training loss: 0.1705\n",
      "2024-10-20 22:13:23 [INFO]: Epoch 022 - training loss: 0.0908\n",
      "2024-10-20 22:13:23 [INFO]: Epoch 023 - training loss: 0.3278\n",
      "2024-10-20 22:13:23 [INFO]: Epoch 024 - training loss: 0.3097\n",
      "2024-10-20 22:13:23 [INFO]: Epoch 025 - training loss: 0.1405\n",
      "2024-10-20 22:13:23 [INFO]: Epoch 026 - training loss: 0.2125\n",
      "2024-10-20 22:13:23 [INFO]: Epoch 027 - training loss: 0.1356\n",
      "2024-10-20 22:13:24 [INFO]: Epoch 028 - training loss: 0.1300\n",
      "2024-10-20 22:13:24 [INFO]: Epoch 029 - training loss: 0.1335\n",
      "2024-10-20 22:13:24 [INFO]: Epoch 030 - training loss: 0.1117\n",
      "2024-10-20 22:13:24 [INFO]: Epoch 031 - training loss: 0.1407\n",
      "2024-10-20 22:13:24 [INFO]: Epoch 032 - training loss: 0.1199\n",
      "2024-10-20 22:13:24 [INFO]: Epoch 033 - training loss: 0.2780\n",
      "2024-10-20 22:13:24 [INFO]: Epoch 034 - training loss: 0.1445\n",
      "2024-10-20 22:13:25 [INFO]: Epoch 035 - training loss: 0.1765\n",
      "2024-10-20 22:13:25 [INFO]: Epoch 036 - training loss: 0.1293\n",
      "2024-10-20 22:13:25 [INFO]: Epoch 037 - training loss: 0.1865\n",
      "2024-10-20 22:13:25 [INFO]: Epoch 038 - training loss: 0.2567\n",
      "2024-10-20 22:13:25 [INFO]: Epoch 039 - training loss: 0.2648\n",
      "2024-10-20 22:13:25 [INFO]: Epoch 040 - training loss: 0.1202\n",
      "2024-10-20 22:13:25 [INFO]: Epoch 041 - training loss: 0.1695\n",
      "2024-10-20 22:13:26 [INFO]: Epoch 042 - training loss: 0.1016\n",
      "2024-10-20 22:13:26 [INFO]: Epoch 043 - training loss: 0.1365\n",
      "2024-10-20 22:13:26 [INFO]: Epoch 044 - training loss: 0.1242\n",
      "2024-10-20 22:13:26 [INFO]: Epoch 045 - training loss: 0.1294\n",
      "2024-10-20 22:13:26 [INFO]: Epoch 046 - training loss: 0.2781\n",
      "2024-10-20 22:13:26 [INFO]: Epoch 047 - training loss: 0.1988\n",
      "2024-10-20 22:13:27 [INFO]: Epoch 048 - training loss: 0.1042\n",
      "2024-10-20 22:13:27 [INFO]: Epoch 049 - training loss: 0.1933\n",
      "2024-10-20 22:13:27 [INFO]: Epoch 050 - training loss: 0.2035\n",
      "2024-10-20 22:13:27 [INFO]: Epoch 051 - training loss: 0.1587\n",
      "2024-10-20 22:13:27 [INFO]: Epoch 052 - training loss: 0.2097\n",
      "2024-10-20 22:13:27 [INFO]: Epoch 053 - training loss: 0.1691\n",
      "2024-10-20 22:13:27 [INFO]: Epoch 054 - training loss: 0.1629\n",
      "2024-10-20 22:13:28 [INFO]: Epoch 055 - training loss: 0.1637\n",
      "2024-10-20 22:13:28 [INFO]: Epoch 056 - training loss: 0.2430\n",
      "2024-10-20 22:13:28 [INFO]: Epoch 057 - training loss: 0.1743\n",
      "2024-10-20 22:13:28 [INFO]: Epoch 058 - training loss: 0.1362\n",
      "2024-10-20 22:13:28 [INFO]: Epoch 059 - training loss: 0.1492\n",
      "2024-10-20 22:13:28 [INFO]: Epoch 060 - training loss: 0.1474\n",
      "2024-10-20 22:13:28 [INFO]: Epoch 061 - training loss: 0.1344\n",
      "2024-10-20 22:13:29 [INFO]: Epoch 062 - training loss: 0.1928\n",
      "2024-10-20 22:13:29 [INFO]: Epoch 063 - training loss: 0.1291\n",
      "2024-10-20 22:13:29 [INFO]: Epoch 064 - training loss: 0.1205\n",
      "2024-10-20 22:13:29 [INFO]: Epoch 065 - training loss: 0.1850\n",
      "2024-10-20 22:13:29 [INFO]: Epoch 066 - training loss: 0.2232\n",
      "2024-10-20 22:13:29 [INFO]: Epoch 067 - training loss: 0.1382\n",
      "2024-10-20 22:13:29 [INFO]: Epoch 068 - training loss: 0.1438\n",
      "2024-10-20 22:13:30 [INFO]: Epoch 069 - training loss: 0.2283\n",
      "2024-10-20 22:13:30 [INFO]: Epoch 070 - training loss: 0.1259\n",
      "2024-10-20 22:13:30 [INFO]: Epoch 071 - training loss: 0.2225\n",
      "2024-10-20 22:13:30 [INFO]: Epoch 072 - training loss: 0.2349\n",
      "2024-10-20 22:13:30 [INFO]: Epoch 073 - training loss: 0.1821\n",
      "2024-10-20 22:13:30 [INFO]: Epoch 074 - training loss: 0.1505\n",
      "2024-10-20 22:13:31 [INFO]: Epoch 075 - training loss: 0.0912\n",
      "2024-10-20 22:13:31 [INFO]: Epoch 076 - training loss: 0.2998\n",
      "2024-10-20 22:13:31 [INFO]: Epoch 077 - training loss: 0.1660\n",
      "2024-10-20 22:13:31 [INFO]: Epoch 078 - training loss: 0.1307\n",
      "2024-10-20 22:13:31 [INFO]: Epoch 079 - training loss: 0.1787\n",
      "2024-10-20 22:13:31 [INFO]: Epoch 080 - training loss: 0.1402\n",
      "2024-10-20 22:13:31 [INFO]: Epoch 081 - training loss: 0.2274\n",
      "2024-10-20 22:13:32 [INFO]: Epoch 082 - training loss: 0.1115\n",
      "2024-10-20 22:13:32 [INFO]: Epoch 083 - training loss: 0.2265\n",
      "2024-10-20 22:13:32 [INFO]: Epoch 084 - training loss: 0.1511\n",
      "2024-10-20 22:13:32 [INFO]: Epoch 085 - training loss: 0.1445\n",
      "2024-10-20 22:13:32 [INFO]: Epoch 086 - training loss: 0.1130\n",
      "2024-10-20 22:13:32 [INFO]: Epoch 087 - training loss: 0.1334\n",
      "2024-10-20 22:13:32 [INFO]: Epoch 088 - training loss: 0.1317\n",
      "2024-10-20 22:13:33 [INFO]: Epoch 089 - training loss: 0.1067\n",
      "2024-10-20 22:13:33 [INFO]: Epoch 090 - training loss: 0.1355\n",
      "2024-10-20 22:13:33 [INFO]: Epoch 091 - training loss: 0.1550\n",
      "2024-10-20 22:13:33 [INFO]: Epoch 092 - training loss: 0.2205\n",
      "2024-10-20 22:13:33 [INFO]: Epoch 093 - training loss: 0.2634\n",
      "2024-10-20 22:13:33 [INFO]: Epoch 094 - training loss: 0.1529\n",
      "2024-10-20 22:13:34 [INFO]: Epoch 095 - training loss: 0.1604\n",
      "2024-10-20 22:13:34 [INFO]: Epoch 096 - training loss: 0.2837\n",
      "2024-10-20 22:13:34 [INFO]: Epoch 097 - training loss: 0.2029\n",
      "2024-10-20 22:13:34 [INFO]: Epoch 098 - training loss: 0.1404\n",
      "2024-10-20 22:13:34 [INFO]: Epoch 099 - training loss: 0.1612\n",
      "2024-10-20 22:13:34 [INFO]: Epoch 100 - training loss: 0.2132\n",
      "2024-10-20 22:13:34 [INFO]: Finished training. The best model is from epoch#11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:17:24 [INFO]: Epoch 001 - training loss: 0.2342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 19/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:17:24 [INFO]: Epoch 002 - training loss: 0.1524\n",
      "2024-10-20 22:17:24 [INFO]: Epoch 003 - training loss: 0.2025\n",
      "2024-10-20 22:17:24 [INFO]: Epoch 004 - training loss: 0.2758\n",
      "2024-10-20 22:17:24 [INFO]: Epoch 005 - training loss: 0.1586\n",
      "2024-10-20 22:17:24 [INFO]: Epoch 006 - training loss: 0.1530\n",
      "2024-10-20 22:17:25 [INFO]: Epoch 007 - training loss: 0.1281\n",
      "2024-10-20 22:17:25 [INFO]: Epoch 008 - training loss: 0.1348\n",
      "2024-10-20 22:17:25 [INFO]: Epoch 009 - training loss: 0.1095\n",
      "2024-10-20 22:17:25 [INFO]: Epoch 010 - training loss: 0.1930\n",
      "2024-10-20 22:17:25 [INFO]: Epoch 011 - training loss: 0.1297\n",
      "2024-10-20 22:17:25 [INFO]: Epoch 012 - training loss: 0.1077\n",
      "2024-10-20 22:17:25 [INFO]: Epoch 013 - training loss: 0.2125\n",
      "2024-10-20 22:17:26 [INFO]: Epoch 014 - training loss: 0.1765\n",
      "2024-10-20 22:17:26 [INFO]: Epoch 015 - training loss: 0.2007\n",
      "2024-10-20 22:17:26 [INFO]: Epoch 016 - training loss: 0.1831\n",
      "2024-10-20 22:17:26 [INFO]: Epoch 017 - training loss: 0.1225\n",
      "2024-10-20 22:17:26 [INFO]: Epoch 018 - training loss: 0.2972\n",
      "2024-10-20 22:17:26 [INFO]: Epoch 019 - training loss: 0.1139\n",
      "2024-10-20 22:17:27 [INFO]: Epoch 020 - training loss: 0.1682\n",
      "2024-10-20 22:17:27 [INFO]: Epoch 021 - training loss: 0.1288\n",
      "2024-10-20 22:17:27 [INFO]: Epoch 022 - training loss: 0.1835\n",
      "2024-10-20 22:17:27 [INFO]: Epoch 023 - training loss: 0.2116\n",
      "2024-10-20 22:17:27 [INFO]: Epoch 024 - training loss: 0.1462\n",
      "2024-10-20 22:17:27 [INFO]: Epoch 025 - training loss: 0.1323\n",
      "2024-10-20 22:17:27 [INFO]: Epoch 026 - training loss: 0.2950\n",
      "2024-10-20 22:17:28 [INFO]: Epoch 027 - training loss: 0.2497\n",
      "2024-10-20 22:17:28 [INFO]: Epoch 028 - training loss: 0.1658\n",
      "2024-10-20 22:17:28 [INFO]: Epoch 029 - training loss: 0.2606\n",
      "2024-10-20 22:17:28 [INFO]: Epoch 030 - training loss: 0.1394\n",
      "2024-10-20 22:17:28 [INFO]: Epoch 031 - training loss: 0.1409\n",
      "2024-10-20 22:17:28 [INFO]: Epoch 032 - training loss: 0.1826\n",
      "2024-10-20 22:17:28 [INFO]: Epoch 033 - training loss: 0.1417\n",
      "2024-10-20 22:17:29 [INFO]: Epoch 034 - training loss: 0.2420\n",
      "2024-10-20 22:17:29 [INFO]: Epoch 035 - training loss: 0.1489\n",
      "2024-10-20 22:17:29 [INFO]: Epoch 036 - training loss: 0.1516\n",
      "2024-10-20 22:17:29 [INFO]: Epoch 037 - training loss: 0.1192\n",
      "2024-10-20 22:17:29 [INFO]: Epoch 038 - training loss: 0.1271\n",
      "2024-10-20 22:17:29 [INFO]: Epoch 039 - training loss: 0.2791\n",
      "2024-10-20 22:17:29 [INFO]: Epoch 040 - training loss: 0.1720\n",
      "2024-10-20 22:17:30 [INFO]: Epoch 041 - training loss: 0.1598\n",
      "2024-10-20 22:17:30 [INFO]: Epoch 042 - training loss: 0.1651\n",
      "2024-10-20 22:17:30 [INFO]: Epoch 043 - training loss: 0.1802\n",
      "2024-10-20 22:17:30 [INFO]: Epoch 044 - training loss: 0.1468\n",
      "2024-10-20 22:17:30 [INFO]: Epoch 045 - training loss: 0.1608\n",
      "2024-10-20 22:17:30 [INFO]: Epoch 046 - training loss: 0.1378\n",
      "2024-10-20 22:17:31 [INFO]: Epoch 047 - training loss: 0.1946\n",
      "2024-10-20 22:17:31 [INFO]: Epoch 048 - training loss: 0.2746\n",
      "2024-10-20 22:17:31 [INFO]: Epoch 049 - training loss: 0.1567\n",
      "2024-10-20 22:17:31 [INFO]: Epoch 050 - training loss: 0.1663\n",
      "2024-10-20 22:17:31 [INFO]: Epoch 051 - training loss: 0.1561\n",
      "2024-10-20 22:17:31 [INFO]: Epoch 052 - training loss: 0.2150\n",
      "2024-10-20 22:17:31 [INFO]: Epoch 053 - training loss: 0.2834\n",
      "2024-10-20 22:17:32 [INFO]: Epoch 054 - training loss: 0.1830\n",
      "2024-10-20 22:17:32 [INFO]: Epoch 055 - training loss: 0.1655\n",
      "2024-10-20 22:17:32 [INFO]: Epoch 056 - training loss: 0.2059\n",
      "2024-10-20 22:17:32 [INFO]: Epoch 057 - training loss: 0.1382\n",
      "2024-10-20 22:17:32 [INFO]: Epoch 058 - training loss: 0.1613\n",
      "2024-10-20 22:17:32 [INFO]: Epoch 059 - training loss: 0.1886\n",
      "2024-10-20 22:17:32 [INFO]: Epoch 060 - training loss: 0.2420\n",
      "2024-10-20 22:17:33 [INFO]: Epoch 061 - training loss: 0.1377\n",
      "2024-10-20 22:17:33 [INFO]: Epoch 062 - training loss: 0.1133\n",
      "2024-10-20 22:17:33 [INFO]: Epoch 063 - training loss: 0.2350\n",
      "2024-10-20 22:17:33 [INFO]: Epoch 064 - training loss: 0.1846\n",
      "2024-10-20 22:17:33 [INFO]: Epoch 065 - training loss: 0.1341\n",
      "2024-10-20 22:17:33 [INFO]: Epoch 066 - training loss: 0.1628\n",
      "2024-10-20 22:17:34 [INFO]: Epoch 067 - training loss: 0.2066\n",
      "2024-10-20 22:17:34 [INFO]: Epoch 068 - training loss: 0.1517\n",
      "2024-10-20 22:17:34 [INFO]: Epoch 069 - training loss: 0.1277\n",
      "2024-10-20 22:17:34 [INFO]: Epoch 070 - training loss: 0.1069\n",
      "2024-10-20 22:17:34 [INFO]: Epoch 071 - training loss: 0.1941\n",
      "2024-10-20 22:17:34 [INFO]: Epoch 072 - training loss: 0.1687\n",
      "2024-10-20 22:17:34 [INFO]: Epoch 073 - training loss: 0.1166\n",
      "2024-10-20 22:17:35 [INFO]: Epoch 074 - training loss: 0.1989\n",
      "2024-10-20 22:17:35 [INFO]: Epoch 075 - training loss: 0.2289\n",
      "2024-10-20 22:17:35 [INFO]: Epoch 076 - training loss: 0.1780\n",
      "2024-10-20 22:17:35 [INFO]: Epoch 077 - training loss: 0.1622\n",
      "2024-10-20 22:17:35 [INFO]: Epoch 078 - training loss: 0.2616\n",
      "2024-10-20 22:17:35 [INFO]: Epoch 079 - training loss: 0.2532\n",
      "2024-10-20 22:17:35 [INFO]: Epoch 080 - training loss: 0.1748\n",
      "2024-10-20 22:17:36 [INFO]: Epoch 081 - training loss: 0.2424\n",
      "2024-10-20 22:17:36 [INFO]: Epoch 082 - training loss: 0.1378\n",
      "2024-10-20 22:17:36 [INFO]: Epoch 083 - training loss: 0.1612\n",
      "2024-10-20 22:17:36 [INFO]: Epoch 084 - training loss: 0.1601\n",
      "2024-10-20 22:17:36 [INFO]: Epoch 085 - training loss: 0.1773\n",
      "2024-10-20 22:17:36 [INFO]: Epoch 086 - training loss: 0.1213\n",
      "2024-10-20 22:17:37 [INFO]: Epoch 087 - training loss: 0.1449\n",
      "2024-10-20 22:17:37 [INFO]: Epoch 088 - training loss: 0.1291\n",
      "2024-10-20 22:17:37 [INFO]: Epoch 089 - training loss: 0.1380\n",
      "2024-10-20 22:17:37 [INFO]: Epoch 090 - training loss: 0.1547\n",
      "2024-10-20 22:17:37 [INFO]: Epoch 091 - training loss: 0.1013\n",
      "2024-10-20 22:17:37 [INFO]: Epoch 092 - training loss: 0.1521\n",
      "2024-10-20 22:17:37 [INFO]: Epoch 093 - training loss: 0.1295\n",
      "2024-10-20 22:17:38 [INFO]: Epoch 094 - training loss: 0.1270\n",
      "2024-10-20 22:17:38 [INFO]: Epoch 095 - training loss: 0.1573\n",
      "2024-10-20 22:17:38 [INFO]: Epoch 096 - training loss: 0.1215\n",
      "2024-10-20 22:17:38 [INFO]: Epoch 097 - training loss: 0.1244\n",
      "2024-10-20 22:17:38 [INFO]: Epoch 098 - training loss: 0.1040\n",
      "2024-10-20 22:17:38 [INFO]: Epoch 099 - training loss: 0.1669\n",
      "2024-10-20 22:17:38 [INFO]: Epoch 100 - training loss: 0.1328\n",
      "2024-10-20 22:17:38 [INFO]: Finished training. The best model is from epoch#91.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:21:27 [INFO]: Epoch 001 - training loss: 0.2255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 20/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:21:27 [INFO]: Epoch 002 - training loss: 0.1411\n",
      "2024-10-20 22:21:28 [INFO]: Epoch 003 - training loss: 0.1595\n",
      "2024-10-20 22:21:28 [INFO]: Epoch 004 - training loss: 0.1888\n",
      "2024-10-20 22:21:28 [INFO]: Epoch 005 - training loss: 0.1435\n",
      "2024-10-20 22:21:28 [INFO]: Epoch 006 - training loss: 0.1873\n",
      "2024-10-20 22:21:28 [INFO]: Epoch 007 - training loss: 0.1211\n",
      "2024-10-20 22:21:28 [INFO]: Epoch 008 - training loss: 0.1770\n",
      "2024-10-20 22:21:28 [INFO]: Epoch 009 - training loss: 0.1469\n",
      "2024-10-20 22:21:29 [INFO]: Epoch 010 - training loss: 0.1575\n",
      "2024-10-20 22:21:29 [INFO]: Epoch 011 - training loss: 0.1137\n",
      "2024-10-20 22:21:29 [INFO]: Epoch 012 - training loss: 0.1063\n",
      "2024-10-20 22:21:29 [INFO]: Epoch 013 - training loss: 0.1741\n",
      "2024-10-20 22:21:29 [INFO]: Epoch 014 - training loss: 0.1450\n",
      "2024-10-20 22:21:29 [INFO]: Epoch 015 - training loss: 0.2209\n",
      "2024-10-20 22:21:29 [INFO]: Epoch 016 - training loss: 0.2196\n",
      "2024-10-20 22:21:30 [INFO]: Epoch 017 - training loss: 0.1329\n",
      "2024-10-20 22:21:30 [INFO]: Epoch 018 - training loss: 0.1645\n",
      "2024-10-20 22:21:30 [INFO]: Epoch 019 - training loss: 0.1800\n",
      "2024-10-20 22:21:30 [INFO]: Epoch 020 - training loss: 0.2083\n",
      "2024-10-20 22:21:30 [INFO]: Epoch 021 - training loss: 0.2203\n",
      "2024-10-20 22:21:30 [INFO]: Epoch 022 - training loss: 0.2310\n",
      "2024-10-20 22:21:31 [INFO]: Epoch 023 - training loss: 0.1240\n",
      "2024-10-20 22:21:31 [INFO]: Epoch 024 - training loss: 0.1162\n",
      "2024-10-20 22:21:31 [INFO]: Epoch 025 - training loss: 0.1468\n",
      "2024-10-20 22:21:31 [INFO]: Epoch 026 - training loss: 0.1062\n",
      "2024-10-20 22:21:31 [INFO]: Epoch 027 - training loss: 0.1616\n",
      "2024-10-20 22:21:31 [INFO]: Epoch 028 - training loss: 0.1054\n",
      "2024-10-20 22:21:31 [INFO]: Epoch 029 - training loss: 0.1161\n",
      "2024-10-20 22:21:32 [INFO]: Epoch 030 - training loss: 0.1752\n",
      "2024-10-20 22:21:32 [INFO]: Epoch 031 - training loss: 0.1131\n",
      "2024-10-20 22:21:32 [INFO]: Epoch 032 - training loss: 0.0965\n",
      "2024-10-20 22:21:32 [INFO]: Epoch 033 - training loss: 0.2135\n",
      "2024-10-20 22:21:32 [INFO]: Epoch 034 - training loss: 0.1642\n",
      "2024-10-20 22:21:32 [INFO]: Epoch 035 - training loss: 0.1440\n",
      "2024-10-20 22:21:32 [INFO]: Epoch 036 - training loss: 0.1470\n",
      "2024-10-20 22:21:33 [INFO]: Epoch 037 - training loss: 0.2314\n",
      "2024-10-20 22:21:33 [INFO]: Epoch 038 - training loss: 0.1662\n",
      "2024-10-20 22:21:33 [INFO]: Epoch 039 - training loss: 0.1727\n",
      "2024-10-20 22:21:33 [INFO]: Epoch 040 - training loss: 0.1720\n",
      "2024-10-20 22:21:33 [INFO]: Epoch 041 - training loss: 0.1999\n",
      "2024-10-20 22:21:33 [INFO]: Epoch 042 - training loss: 0.0987\n",
      "2024-10-20 22:21:33 [INFO]: Epoch 043 - training loss: 0.1757\n",
      "2024-10-20 22:21:34 [INFO]: Epoch 044 - training loss: 0.1290\n",
      "2024-10-20 22:21:34 [INFO]: Epoch 045 - training loss: 0.1531\n",
      "2024-10-20 22:21:34 [INFO]: Epoch 046 - training loss: 0.1726\n",
      "2024-10-20 22:21:34 [INFO]: Epoch 047 - training loss: 0.2875\n",
      "2024-10-20 22:21:34 [INFO]: Epoch 048 - training loss: 0.1878\n",
      "2024-10-20 22:21:34 [INFO]: Epoch 049 - training loss: 0.1606\n",
      "2024-10-20 22:21:35 [INFO]: Epoch 050 - training loss: 0.1160\n",
      "2024-10-20 22:21:35 [INFO]: Epoch 051 - training loss: 0.3252\n",
      "2024-10-20 22:21:35 [INFO]: Epoch 052 - training loss: 0.1279\n",
      "2024-10-20 22:21:35 [INFO]: Epoch 053 - training loss: 0.1279\n",
      "2024-10-20 22:21:35 [INFO]: Epoch 054 - training loss: 0.1356\n",
      "2024-10-20 22:21:35 [INFO]: Epoch 055 - training loss: 0.2603\n",
      "2024-10-20 22:21:35 [INFO]: Epoch 056 - training loss: 0.2236\n",
      "2024-10-20 22:21:36 [INFO]: Epoch 057 - training loss: 0.1259\n",
      "2024-10-20 22:21:36 [INFO]: Epoch 058 - training loss: 0.1333\n",
      "2024-10-20 22:21:36 [INFO]: Epoch 059 - training loss: 0.1274\n",
      "2024-10-20 22:21:36 [INFO]: Epoch 060 - training loss: 0.2005\n",
      "2024-10-20 22:21:36 [INFO]: Epoch 061 - training loss: 0.1674\n",
      "2024-10-20 22:21:36 [INFO]: Epoch 062 - training loss: 0.1785\n",
      "2024-10-20 22:21:36 [INFO]: Epoch 063 - training loss: 0.1477\n",
      "2024-10-20 22:21:37 [INFO]: Epoch 064 - training loss: 0.1205\n",
      "2024-10-20 22:21:37 [INFO]: Epoch 065 - training loss: 0.1454\n",
      "2024-10-20 22:21:37 [INFO]: Epoch 066 - training loss: 0.1547\n",
      "2024-10-20 22:21:37 [INFO]: Epoch 067 - training loss: 0.2125\n",
      "2024-10-20 22:21:37 [INFO]: Epoch 068 - training loss: 0.1724\n",
      "2024-10-20 22:21:37 [INFO]: Epoch 069 - training loss: 0.1630\n",
      "2024-10-20 22:21:38 [INFO]: Epoch 070 - training loss: 0.1336\n",
      "2024-10-20 22:21:38 [INFO]: Epoch 071 - training loss: 0.1434\n",
      "2024-10-20 22:21:38 [INFO]: Epoch 072 - training loss: 0.1271\n",
      "2024-10-20 22:21:38 [INFO]: Epoch 073 - training loss: 0.1156\n",
      "2024-10-20 22:21:38 [INFO]: Epoch 074 - training loss: 0.2554\n",
      "2024-10-20 22:21:38 [INFO]: Epoch 075 - training loss: 0.1495\n",
      "2024-10-20 22:21:38 [INFO]: Epoch 076 - training loss: 0.1219\n",
      "2024-10-20 22:21:39 [INFO]: Epoch 077 - training loss: 0.2746\n",
      "2024-10-20 22:21:39 [INFO]: Epoch 078 - training loss: 0.2424\n",
      "2024-10-20 22:21:39 [INFO]: Epoch 079 - training loss: 0.2022\n",
      "2024-10-20 22:21:39 [INFO]: Epoch 080 - training loss: 0.1163\n",
      "2024-10-20 22:21:39 [INFO]: Epoch 081 - training loss: 0.1470\n",
      "2024-10-20 22:21:39 [INFO]: Epoch 082 - training loss: 0.1244\n",
      "2024-10-20 22:21:39 [INFO]: Epoch 083 - training loss: 0.1681\n",
      "2024-10-20 22:21:40 [INFO]: Epoch 084 - training loss: 0.1397\n",
      "2024-10-20 22:21:40 [INFO]: Epoch 085 - training loss: 0.1079\n",
      "2024-10-20 22:21:40 [INFO]: Epoch 086 - training loss: 0.1577\n",
      "2024-10-20 22:21:40 [INFO]: Epoch 087 - training loss: 0.1367\n",
      "2024-10-20 22:21:40 [INFO]: Epoch 088 - training loss: 0.1759\n",
      "2024-10-20 22:21:40 [INFO]: Epoch 089 - training loss: 0.1696\n",
      "2024-10-20 22:21:41 [INFO]: Epoch 090 - training loss: 0.1747\n",
      "2024-10-20 22:21:41 [INFO]: Epoch 091 - training loss: 0.1956\n",
      "2024-10-20 22:21:41 [INFO]: Epoch 092 - training loss: 0.1571\n",
      "2024-10-20 22:21:41 [INFO]: Epoch 093 - training loss: 0.1801\n",
      "2024-10-20 22:21:41 [INFO]: Epoch 094 - training loss: 0.1095\n",
      "2024-10-20 22:21:41 [INFO]: Epoch 095 - training loss: 0.1576\n",
      "2024-10-20 22:21:41 [INFO]: Epoch 096 - training loss: 0.1648\n",
      "2024-10-20 22:21:42 [INFO]: Epoch 097 - training loss: 0.0798\n",
      "2024-10-20 22:21:42 [INFO]: Epoch 098 - training loss: 0.1332\n",
      "2024-10-20 22:21:42 [INFO]: Epoch 099 - training loss: 0.2155\n",
      "2024-10-20 22:21:42 [INFO]: Epoch 100 - training loss: 0.1491\n",
      "2024-10-20 22:21:42 [INFO]: Finished training. The best model is from epoch#97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:25:31 [INFO]: Epoch 001 - training loss: 0.1175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 21/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:25:31 [INFO]: Epoch 002 - training loss: 0.2357\n",
      "2024-10-20 22:25:31 [INFO]: Epoch 003 - training loss: 0.1303\n",
      "2024-10-20 22:25:32 [INFO]: Epoch 004 - training loss: 0.1347\n",
      "2024-10-20 22:25:32 [INFO]: Epoch 005 - training loss: 0.1349\n",
      "2024-10-20 22:25:32 [INFO]: Epoch 006 - training loss: 0.1469\n",
      "2024-10-20 22:25:32 [INFO]: Epoch 007 - training loss: 0.1382\n",
      "2024-10-20 22:25:32 [INFO]: Epoch 008 - training loss: 0.1297\n",
      "2024-10-20 22:25:32 [INFO]: Epoch 009 - training loss: 0.1285\n",
      "2024-10-20 22:25:33 [INFO]: Epoch 010 - training loss: 0.1269\n",
      "2024-10-20 22:25:33 [INFO]: Epoch 011 - training loss: 0.0828\n",
      "2024-10-20 22:25:33 [INFO]: Epoch 012 - training loss: 0.0976\n",
      "2024-10-20 22:25:33 [INFO]: Epoch 013 - training loss: 0.1090\n",
      "2024-10-20 22:25:33 [INFO]: Epoch 014 - training loss: 0.2596\n",
      "2024-10-20 22:25:33 [INFO]: Epoch 015 - training loss: 0.1739\n",
      "2024-10-20 22:25:33 [INFO]: Epoch 016 - training loss: 0.2228\n",
      "2024-10-20 22:25:34 [INFO]: Epoch 017 - training loss: 0.0694\n",
      "2024-10-20 22:25:34 [INFO]: Epoch 018 - training loss: 0.1496\n",
      "2024-10-20 22:25:34 [INFO]: Epoch 019 - training loss: 0.1540\n",
      "2024-10-20 22:25:34 [INFO]: Epoch 020 - training loss: 0.1184\n",
      "2024-10-20 22:25:34 [INFO]: Epoch 021 - training loss: 0.1967\n",
      "2024-10-20 22:25:34 [INFO]: Epoch 022 - training loss: 0.1588\n",
      "2024-10-20 22:25:34 [INFO]: Epoch 023 - training loss: 0.1859\n",
      "2024-10-20 22:25:35 [INFO]: Epoch 024 - training loss: 0.1295\n",
      "2024-10-20 22:25:35 [INFO]: Epoch 025 - training loss: 0.0974\n",
      "2024-10-20 22:25:35 [INFO]: Epoch 026 - training loss: 0.1373\n",
      "2024-10-20 22:25:35 [INFO]: Epoch 027 - training loss: 0.1497\n",
      "2024-10-20 22:25:35 [INFO]: Epoch 028 - training loss: 0.2101\n",
      "2024-10-20 22:25:35 [INFO]: Epoch 029 - training loss: 0.1211\n",
      "2024-10-20 22:25:36 [INFO]: Epoch 030 - training loss: 0.2092\n",
      "2024-10-20 22:25:36 [INFO]: Epoch 031 - training loss: 0.1959\n",
      "2024-10-20 22:25:36 [INFO]: Epoch 032 - training loss: 0.1078\n",
      "2024-10-20 22:25:36 [INFO]: Epoch 033 - training loss: 0.2114\n",
      "2024-10-20 22:25:36 [INFO]: Epoch 034 - training loss: 0.1629\n",
      "2024-10-20 22:25:36 [INFO]: Epoch 035 - training loss: 0.1387\n",
      "2024-10-20 22:25:36 [INFO]: Epoch 036 - training loss: 0.1666\n",
      "2024-10-20 22:25:37 [INFO]: Epoch 037 - training loss: 0.1359\n",
      "2024-10-20 22:25:37 [INFO]: Epoch 038 - training loss: 0.0959\n",
      "2024-10-20 22:25:37 [INFO]: Epoch 039 - training loss: 0.1901\n",
      "2024-10-20 22:25:37 [INFO]: Epoch 040 - training loss: 0.1240\n",
      "2024-10-20 22:25:37 [INFO]: Epoch 041 - training loss: 0.1050\n",
      "2024-10-20 22:25:37 [INFO]: Epoch 042 - training loss: 0.1814\n",
      "2024-10-20 22:25:37 [INFO]: Epoch 043 - training loss: 0.1506\n",
      "2024-10-20 22:25:38 [INFO]: Epoch 044 - training loss: 0.1512\n",
      "2024-10-20 22:25:38 [INFO]: Epoch 045 - training loss: 0.2061\n",
      "2024-10-20 22:25:38 [INFO]: Epoch 046 - training loss: 0.1459\n",
      "2024-10-20 22:25:38 [INFO]: Epoch 047 - training loss: 0.1583\n",
      "2024-10-20 22:25:38 [INFO]: Epoch 048 - training loss: 0.1481\n",
      "2024-10-20 22:25:38 [INFO]: Epoch 049 - training loss: 0.1750\n",
      "2024-10-20 22:25:39 [INFO]: Epoch 050 - training loss: 0.1441\n",
      "2024-10-20 22:25:39 [INFO]: Epoch 051 - training loss: 0.1275\n",
      "2024-10-20 22:25:39 [INFO]: Epoch 052 - training loss: 0.1488\n",
      "2024-10-20 22:25:39 [INFO]: Epoch 053 - training loss: 0.2440\n",
      "2024-10-20 22:25:39 [INFO]: Epoch 054 - training loss: 0.2243\n",
      "2024-10-20 22:25:39 [INFO]: Epoch 055 - training loss: 0.1579\n",
      "2024-10-20 22:25:39 [INFO]: Epoch 056 - training loss: 0.1579\n",
      "2024-10-20 22:25:40 [INFO]: Epoch 057 - training loss: 0.2257\n",
      "2024-10-20 22:25:40 [INFO]: Epoch 058 - training loss: 0.1533\n",
      "2024-10-20 22:25:40 [INFO]: Epoch 059 - training loss: 0.1461\n",
      "2024-10-20 22:25:40 [INFO]: Epoch 060 - training loss: 0.1178\n",
      "2024-10-20 22:25:40 [INFO]: Epoch 061 - training loss: 0.1455\n",
      "2024-10-20 22:25:40 [INFO]: Epoch 062 - training loss: 0.1518\n",
      "2024-10-20 22:25:41 [INFO]: Epoch 063 - training loss: 0.1163\n",
      "2024-10-20 22:25:41 [INFO]: Epoch 064 - training loss: 0.1072\n",
      "2024-10-20 22:25:41 [INFO]: Epoch 065 - training loss: 0.1611\n",
      "2024-10-20 22:25:41 [INFO]: Epoch 066 - training loss: 0.1939\n",
      "2024-10-20 22:25:41 [INFO]: Epoch 067 - training loss: 0.1453\n",
      "2024-10-20 22:25:41 [INFO]: Epoch 068 - training loss: 0.1308\n",
      "2024-10-20 22:25:41 [INFO]: Epoch 069 - training loss: 0.1236\n",
      "2024-10-20 22:25:42 [INFO]: Epoch 070 - training loss: 0.2021\n",
      "2024-10-20 22:25:42 [INFO]: Epoch 071 - training loss: 0.0823\n",
      "2024-10-20 22:25:42 [INFO]: Epoch 072 - training loss: 0.1845\n",
      "2024-10-20 22:25:42 [INFO]: Epoch 073 - training loss: 0.1396\n",
      "2024-10-20 22:25:42 [INFO]: Epoch 074 - training loss: 0.1445\n",
      "2024-10-20 22:25:42 [INFO]: Epoch 075 - training loss: 0.1283\n",
      "2024-10-20 22:25:43 [INFO]: Epoch 076 - training loss: 0.1759\n",
      "2024-10-20 22:25:43 [INFO]: Epoch 077 - training loss: 0.1877\n",
      "2024-10-20 22:25:43 [INFO]: Epoch 078 - training loss: 0.1987\n",
      "2024-10-20 22:25:43 [INFO]: Epoch 079 - training loss: 0.3131\n",
      "2024-10-20 22:25:43 [INFO]: Epoch 080 - training loss: 0.0996\n",
      "2024-10-20 22:25:43 [INFO]: Epoch 081 - training loss: 0.1104\n",
      "2024-10-20 22:25:43 [INFO]: Epoch 082 - training loss: 0.1298\n",
      "2024-10-20 22:25:44 [INFO]: Epoch 083 - training loss: 0.1131\n",
      "2024-10-20 22:25:44 [INFO]: Epoch 084 - training loss: 0.1587\n",
      "2024-10-20 22:25:44 [INFO]: Epoch 085 - training loss: 0.1896\n",
      "2024-10-20 22:25:44 [INFO]: Epoch 086 - training loss: 0.0882\n",
      "2024-10-20 22:25:44 [INFO]: Epoch 087 - training loss: 0.0758\n",
      "2024-10-20 22:25:44 [INFO]: Epoch 088 - training loss: 0.2203\n",
      "2024-10-20 22:25:44 [INFO]: Epoch 089 - training loss: 0.1251\n",
      "2024-10-20 22:25:45 [INFO]: Epoch 090 - training loss: 0.1312\n",
      "2024-10-20 22:25:45 [INFO]: Epoch 091 - training loss: 0.1327\n",
      "2024-10-20 22:25:45 [INFO]: Epoch 092 - training loss: 0.1388\n",
      "2024-10-20 22:25:45 [INFO]: Epoch 093 - training loss: 0.1511\n",
      "2024-10-20 22:25:45 [INFO]: Epoch 094 - training loss: 0.1436\n",
      "2024-10-20 22:25:45 [INFO]: Epoch 095 - training loss: 0.1684\n",
      "2024-10-20 22:25:45 [INFO]: Epoch 096 - training loss: 0.1402\n",
      "2024-10-20 22:25:46 [INFO]: Epoch 097 - training loss: 0.1204\n",
      "2024-10-20 22:25:46 [INFO]: Epoch 098 - training loss: 0.1041\n",
      "2024-10-20 22:25:46 [INFO]: Epoch 099 - training loss: 0.1442\n",
      "2024-10-20 22:25:46 [INFO]: Epoch 100 - training loss: 0.2123\n",
      "2024-10-20 22:25:46 [INFO]: Finished training. The best model is from epoch#17.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:29:35 [INFO]: Epoch 001 - training loss: 0.1005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 22/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:29:35 [INFO]: Epoch 002 - training loss: 0.1124\n",
      "2024-10-20 22:29:35 [INFO]: Epoch 003 - training loss: 0.2325\n",
      "2024-10-20 22:29:36 [INFO]: Epoch 004 - training loss: 0.3201\n",
      "2024-10-20 22:29:36 [INFO]: Epoch 005 - training loss: 0.1465\n",
      "2024-10-20 22:29:36 [INFO]: Epoch 006 - training loss: 0.2507\n",
      "2024-10-20 22:29:36 [INFO]: Epoch 007 - training loss: 0.2212\n",
      "2024-10-20 22:29:36 [INFO]: Epoch 008 - training loss: 0.1603\n",
      "2024-10-20 22:29:36 [INFO]: Epoch 009 - training loss: 0.1521\n",
      "2024-10-20 22:29:36 [INFO]: Epoch 010 - training loss: 0.1360\n",
      "2024-10-20 22:29:37 [INFO]: Epoch 011 - training loss: 0.1034\n",
      "2024-10-20 22:29:37 [INFO]: Epoch 012 - training loss: 0.1734\n",
      "2024-10-20 22:29:37 [INFO]: Epoch 013 - training loss: 0.1716\n",
      "2024-10-20 22:29:37 [INFO]: Epoch 014 - training loss: 0.1935\n",
      "2024-10-20 22:29:37 [INFO]: Epoch 015 - training loss: 0.1686\n",
      "2024-10-20 22:29:37 [INFO]: Epoch 016 - training loss: 0.0943\n",
      "2024-10-20 22:29:37 [INFO]: Epoch 017 - training loss: 0.1632\n",
      "2024-10-20 22:29:38 [INFO]: Epoch 018 - training loss: 0.1660\n",
      "2024-10-20 22:29:38 [INFO]: Epoch 019 - training loss: 0.1166\n",
      "2024-10-20 22:29:38 [INFO]: Epoch 020 - training loss: 0.1131\n",
      "2024-10-20 22:29:38 [INFO]: Epoch 021 - training loss: 0.1085\n",
      "2024-10-20 22:29:38 [INFO]: Epoch 022 - training loss: 0.1918\n",
      "2024-10-20 22:29:38 [INFO]: Epoch 023 - training loss: 0.2023\n",
      "2024-10-20 22:29:39 [INFO]: Epoch 024 - training loss: 0.1487\n",
      "2024-10-20 22:29:39 [INFO]: Epoch 025 - training loss: 0.0915\n",
      "2024-10-20 22:29:39 [INFO]: Epoch 026 - training loss: 0.0922\n",
      "2024-10-20 22:29:39 [INFO]: Epoch 027 - training loss: 0.1187\n",
      "2024-10-20 22:29:39 [INFO]: Epoch 028 - training loss: 0.0961\n",
      "2024-10-20 22:29:39 [INFO]: Epoch 029 - training loss: 0.1274\n",
      "2024-10-20 22:29:39 [INFO]: Epoch 030 - training loss: 0.1974\n",
      "2024-10-20 22:29:40 [INFO]: Epoch 031 - training loss: 0.2137\n",
      "2024-10-20 22:29:40 [INFO]: Epoch 032 - training loss: 0.1653\n",
      "2024-10-20 22:29:40 [INFO]: Epoch 033 - training loss: 0.1491\n",
      "2024-10-20 22:29:40 [INFO]: Epoch 034 - training loss: 0.1092\n",
      "2024-10-20 22:29:40 [INFO]: Epoch 035 - training loss: 0.1280\n",
      "2024-10-20 22:29:40 [INFO]: Epoch 036 - training loss: 0.1149\n",
      "2024-10-20 22:29:40 [INFO]: Epoch 037 - training loss: 0.1101\n",
      "2024-10-20 22:29:41 [INFO]: Epoch 038 - training loss: 0.1504\n",
      "2024-10-20 22:29:41 [INFO]: Epoch 039 - training loss: 0.1788\n",
      "2024-10-20 22:29:41 [INFO]: Epoch 040 - training loss: 0.1438\n",
      "2024-10-20 22:29:41 [INFO]: Epoch 041 - training loss: 0.1211\n",
      "2024-10-20 22:29:41 [INFO]: Epoch 042 - training loss: 0.1173\n",
      "2024-10-20 22:29:41 [INFO]: Epoch 043 - training loss: 0.1864\n",
      "2024-10-20 22:29:42 [INFO]: Epoch 044 - training loss: 0.2891\n",
      "2024-10-20 22:29:42 [INFO]: Epoch 045 - training loss: 0.2202\n",
      "2024-10-20 22:29:42 [INFO]: Epoch 046 - training loss: 0.1600\n",
      "2024-10-20 22:29:42 [INFO]: Epoch 047 - training loss: 0.1268\n",
      "2024-10-20 22:29:42 [INFO]: Epoch 048 - training loss: 0.2336\n",
      "2024-10-20 22:29:42 [INFO]: Epoch 049 - training loss: 0.1207\n",
      "2024-10-20 22:29:42 [INFO]: Epoch 050 - training loss: 0.2075\n",
      "2024-10-20 22:29:43 [INFO]: Epoch 051 - training loss: 0.2230\n",
      "2024-10-20 22:29:43 [INFO]: Epoch 052 - training loss: 0.2083\n",
      "2024-10-20 22:29:43 [INFO]: Epoch 053 - training loss: 0.1596\n",
      "2024-10-20 22:29:43 [INFO]: Epoch 054 - training loss: 0.2408\n",
      "2024-10-20 22:29:43 [INFO]: Epoch 055 - training loss: 0.2221\n",
      "2024-10-20 22:29:43 [INFO]: Epoch 056 - training loss: 0.1446\n",
      "2024-10-20 22:29:44 [INFO]: Epoch 057 - training loss: 0.1523\n",
      "2024-10-20 22:29:44 [INFO]: Epoch 058 - training loss: 0.1545\n",
      "2024-10-20 22:29:44 [INFO]: Epoch 059 - training loss: 0.1704\n",
      "2024-10-20 22:29:44 [INFO]: Epoch 060 - training loss: 0.1369\n",
      "2024-10-20 22:29:44 [INFO]: Epoch 061 - training loss: 0.1146\n",
      "2024-10-20 22:29:44 [INFO]: Epoch 062 - training loss: 0.1786\n",
      "2024-10-20 22:29:44 [INFO]: Epoch 063 - training loss: 0.2354\n",
      "2024-10-20 22:29:45 [INFO]: Epoch 064 - training loss: 0.2228\n",
      "2024-10-20 22:29:45 [INFO]: Epoch 065 - training loss: 0.1120\n",
      "2024-10-20 22:29:45 [INFO]: Epoch 066 - training loss: 0.2373\n",
      "2024-10-20 22:29:45 [INFO]: Epoch 067 - training loss: 0.1729\n",
      "2024-10-20 22:29:45 [INFO]: Epoch 068 - training loss: 0.1899\n",
      "2024-10-20 22:29:45 [INFO]: Epoch 069 - training loss: 0.1159\n",
      "2024-10-20 22:29:45 [INFO]: Epoch 070 - training loss: 0.1425\n",
      "2024-10-20 22:29:46 [INFO]: Epoch 071 - training loss: 0.1639\n",
      "2024-10-20 22:29:46 [INFO]: Epoch 072 - training loss: 0.2448\n",
      "2024-10-20 22:29:46 [INFO]: Epoch 073 - training loss: 0.1628\n",
      "2024-10-20 22:29:46 [INFO]: Epoch 074 - training loss: 0.1315\n",
      "2024-10-20 22:29:46 [INFO]: Epoch 075 - training loss: 0.2453\n",
      "2024-10-20 22:29:46 [INFO]: Epoch 076 - training loss: 0.2018\n",
      "2024-10-20 22:29:47 [INFO]: Epoch 077 - training loss: 0.1485\n",
      "2024-10-20 22:29:47 [INFO]: Epoch 078 - training loss: 0.2887\n",
      "2024-10-20 22:29:47 [INFO]: Epoch 079 - training loss: 0.2036\n",
      "2024-10-20 22:29:47 [INFO]: Epoch 080 - training loss: 0.1857\n",
      "2024-10-20 22:29:47 [INFO]: Epoch 081 - training loss: 0.1823\n",
      "2024-10-20 22:29:47 [INFO]: Epoch 082 - training loss: 0.1540\n",
      "2024-10-20 22:29:47 [INFO]: Epoch 083 - training loss: 0.1389\n",
      "2024-10-20 22:29:48 [INFO]: Epoch 084 - training loss: 0.2328\n",
      "2024-10-20 22:29:48 [INFO]: Epoch 085 - training loss: 0.1411\n",
      "2024-10-20 22:29:48 [INFO]: Epoch 086 - training loss: 0.1691\n",
      "2024-10-20 22:29:48 [INFO]: Epoch 087 - training loss: 0.2692\n",
      "2024-10-20 22:29:48 [INFO]: Epoch 088 - training loss: 0.1364\n",
      "2024-10-20 22:29:48 [INFO]: Epoch 089 - training loss: 0.2041\n",
      "2024-10-20 22:29:48 [INFO]: Epoch 090 - training loss: 0.1597\n",
      "2024-10-20 22:29:49 [INFO]: Epoch 091 - training loss: 0.1054\n",
      "2024-10-20 22:29:49 [INFO]: Epoch 092 - training loss: 0.1814\n",
      "2024-10-20 22:29:49 [INFO]: Epoch 093 - training loss: 0.1916\n",
      "2024-10-20 22:29:49 [INFO]: Epoch 094 - training loss: 0.1257\n",
      "2024-10-20 22:29:49 [INFO]: Epoch 095 - training loss: 0.2006\n",
      "2024-10-20 22:29:49 [INFO]: Epoch 096 - training loss: 0.1677\n",
      "2024-10-20 22:29:50 [INFO]: Epoch 097 - training loss: 0.1450\n",
      "2024-10-20 22:29:50 [INFO]: Epoch 098 - training loss: 0.1448\n",
      "2024-10-20 22:29:50 [INFO]: Epoch 099 - training loss: 0.1450\n",
      "2024-10-20 22:29:50 [INFO]: Epoch 100 - training loss: 0.3449\n",
      "2024-10-20 22:29:50 [INFO]: Finished training. The best model is from epoch#25.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:33:39 [INFO]: Epoch 001 - training loss: 0.2053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 23/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:33:39 [INFO]: Epoch 002 - training loss: 0.1880\n",
      "2024-10-20 22:33:39 [INFO]: Epoch 003 - training loss: 0.1740\n",
      "2024-10-20 22:33:39 [INFO]: Epoch 004 - training loss: 0.3316\n",
      "2024-10-20 22:33:39 [INFO]: Epoch 005 - training loss: 0.1222\n",
      "2024-10-20 22:33:39 [INFO]: Epoch 006 - training loss: 0.2347\n",
      "2024-10-20 22:33:40 [INFO]: Epoch 007 - training loss: 0.1447\n",
      "2024-10-20 22:33:40 [INFO]: Epoch 008 - training loss: 0.1264\n",
      "2024-10-20 22:33:40 [INFO]: Epoch 009 - training loss: 0.1206\n",
      "2024-10-20 22:33:40 [INFO]: Epoch 010 - training loss: 0.1443\n",
      "2024-10-20 22:33:40 [INFO]: Epoch 011 - training loss: 0.1151\n",
      "2024-10-20 22:33:40 [INFO]: Epoch 012 - training loss: 0.1521\n",
      "2024-10-20 22:33:41 [INFO]: Epoch 013 - training loss: 0.1662\n",
      "2024-10-20 22:33:41 [INFO]: Epoch 014 - training loss: 0.1060\n",
      "2024-10-20 22:33:41 [INFO]: Epoch 015 - training loss: 0.0909\n",
      "2024-10-20 22:33:41 [INFO]: Epoch 016 - training loss: 0.1230\n",
      "2024-10-20 22:33:41 [INFO]: Epoch 017 - training loss: 0.1437\n",
      "2024-10-20 22:33:41 [INFO]: Epoch 018 - training loss: 0.1517\n",
      "2024-10-20 22:33:41 [INFO]: Epoch 019 - training loss: 0.2106\n",
      "2024-10-20 22:33:42 [INFO]: Epoch 020 - training loss: 0.1926\n",
      "2024-10-20 22:33:42 [INFO]: Epoch 021 - training loss: 0.1537\n",
      "2024-10-20 22:33:42 [INFO]: Epoch 022 - training loss: 0.1425\n",
      "2024-10-20 22:33:42 [INFO]: Epoch 023 - training loss: 0.1096\n",
      "2024-10-20 22:33:42 [INFO]: Epoch 024 - training loss: 0.1192\n",
      "2024-10-20 22:33:42 [INFO]: Epoch 025 - training loss: 0.1989\n",
      "2024-10-20 22:33:42 [INFO]: Epoch 026 - training loss: 0.1500\n",
      "2024-10-20 22:33:43 [INFO]: Epoch 027 - training loss: 0.0611\n",
      "2024-10-20 22:33:43 [INFO]: Epoch 028 - training loss: 0.2625\n",
      "2024-10-20 22:33:43 [INFO]: Epoch 029 - training loss: 0.1856\n",
      "2024-10-20 22:33:43 [INFO]: Epoch 030 - training loss: 0.1052\n",
      "2024-10-20 22:33:43 [INFO]: Epoch 031 - training loss: 0.1492\n",
      "2024-10-20 22:33:43 [INFO]: Epoch 032 - training loss: 0.1736\n",
      "2024-10-20 22:33:43 [INFO]: Epoch 033 - training loss: 0.2129\n",
      "2024-10-20 22:33:44 [INFO]: Epoch 034 - training loss: 0.1178\n",
      "2024-10-20 22:33:44 [INFO]: Epoch 035 - training loss: 0.1407\n",
      "2024-10-20 22:33:44 [INFO]: Epoch 036 - training loss: 0.1733\n",
      "2024-10-20 22:33:44 [INFO]: Epoch 037 - training loss: 0.2160\n",
      "2024-10-20 22:33:44 [INFO]: Epoch 038 - training loss: 0.1113\n",
      "2024-10-20 22:33:44 [INFO]: Epoch 039 - training loss: 0.1273\n",
      "2024-10-20 22:33:45 [INFO]: Epoch 040 - training loss: 0.2352\n",
      "2024-10-20 22:33:45 [INFO]: Epoch 041 - training loss: 0.1265\n",
      "2024-10-20 22:33:45 [INFO]: Epoch 042 - training loss: 0.1655\n",
      "2024-10-20 22:33:45 [INFO]: Epoch 043 - training loss: 0.1402\n",
      "2024-10-20 22:33:45 [INFO]: Epoch 044 - training loss: 0.1560\n",
      "2024-10-20 22:33:45 [INFO]: Epoch 045 - training loss: 0.1674\n",
      "2024-10-20 22:33:45 [INFO]: Epoch 046 - training loss: 0.1432\n",
      "2024-10-20 22:33:46 [INFO]: Epoch 047 - training loss: 0.2414\n",
      "2024-10-20 22:33:46 [INFO]: Epoch 048 - training loss: 0.2028\n",
      "2024-10-20 22:33:46 [INFO]: Epoch 049 - training loss: 0.2175\n",
      "2024-10-20 22:33:46 [INFO]: Epoch 050 - training loss: 0.1803\n",
      "2024-10-20 22:33:46 [INFO]: Epoch 051 - training loss: 0.1817\n",
      "2024-10-20 22:33:46 [INFO]: Epoch 052 - training loss: 0.2222\n",
      "2024-10-20 22:33:46 [INFO]: Epoch 053 - training loss: 0.1793\n",
      "2024-10-20 22:33:47 [INFO]: Epoch 054 - training loss: 0.1796\n",
      "2024-10-20 22:33:47 [INFO]: Epoch 055 - training loss: 0.2897\n",
      "2024-10-20 22:33:47 [INFO]: Epoch 056 - training loss: 0.1239\n",
      "2024-10-20 22:33:47 [INFO]: Epoch 057 - training loss: 0.3132\n",
      "2024-10-20 22:33:47 [INFO]: Epoch 058 - training loss: 0.1312\n",
      "2024-10-20 22:33:47 [INFO]: Epoch 059 - training loss: 0.1530\n",
      "2024-10-20 22:33:48 [INFO]: Epoch 060 - training loss: 0.1237\n",
      "2024-10-20 22:33:48 [INFO]: Epoch 061 - training loss: 0.1715\n",
      "2024-10-20 22:33:48 [INFO]: Epoch 062 - training loss: 0.1196\n",
      "2024-10-20 22:33:48 [INFO]: Epoch 063 - training loss: 0.1659\n",
      "2024-10-20 22:33:48 [INFO]: Epoch 064 - training loss: 0.2103\n",
      "2024-10-20 22:33:48 [INFO]: Epoch 065 - training loss: 0.1387\n",
      "2024-10-20 22:33:48 [INFO]: Epoch 066 - training loss: 0.2146\n",
      "2024-10-20 22:33:49 [INFO]: Epoch 067 - training loss: 0.2139\n",
      "2024-10-20 22:33:49 [INFO]: Epoch 068 - training loss: 0.1783\n",
      "2024-10-20 22:33:49 [INFO]: Epoch 069 - training loss: 0.1597\n",
      "2024-10-20 22:33:49 [INFO]: Epoch 070 - training loss: 0.2104\n",
      "2024-10-20 22:33:49 [INFO]: Epoch 071 - training loss: 0.1796\n",
      "2024-10-20 22:33:49 [INFO]: Epoch 072 - training loss: 0.2048\n",
      "2024-10-20 22:33:49 [INFO]: Epoch 073 - training loss: 0.1653\n",
      "2024-10-20 22:33:50 [INFO]: Epoch 074 - training loss: 0.1504\n",
      "2024-10-20 22:33:50 [INFO]: Epoch 075 - training loss: 0.1854\n",
      "2024-10-20 22:33:50 [INFO]: Epoch 076 - training loss: 0.1488\n",
      "2024-10-20 22:33:50 [INFO]: Epoch 077 - training loss: 0.2330\n",
      "2024-10-20 22:33:50 [INFO]: Epoch 078 - training loss: 0.1168\n",
      "2024-10-20 22:33:50 [INFO]: Epoch 079 - training loss: 0.2020\n",
      "2024-10-20 22:33:51 [INFO]: Epoch 080 - training loss: 0.1353\n",
      "2024-10-20 22:33:51 [INFO]: Epoch 081 - training loss: 0.0931\n",
      "2024-10-20 22:33:51 [INFO]: Epoch 082 - training loss: 0.2770\n",
      "2024-10-20 22:33:51 [INFO]: Epoch 083 - training loss: 0.1654\n",
      "2024-10-20 22:33:51 [INFO]: Epoch 084 - training loss: 0.1984\n",
      "2024-10-20 22:33:51 [INFO]: Epoch 085 - training loss: 0.2522\n",
      "2024-10-20 22:33:51 [INFO]: Epoch 086 - training loss: 0.1659\n",
      "2024-10-20 22:33:52 [INFO]: Epoch 087 - training loss: 0.1909\n",
      "2024-10-20 22:33:52 [INFO]: Epoch 088 - training loss: 0.1611\n",
      "2024-10-20 22:33:52 [INFO]: Epoch 089 - training loss: 0.1542\n",
      "2024-10-20 22:33:52 [INFO]: Epoch 090 - training loss: 0.1475\n",
      "2024-10-20 22:33:52 [INFO]: Epoch 091 - training loss: 0.1600\n",
      "2024-10-20 22:33:52 [INFO]: Epoch 092 - training loss: 0.1294\n",
      "2024-10-20 22:33:52 [INFO]: Epoch 093 - training loss: 0.1938\n",
      "2024-10-20 22:33:53 [INFO]: Epoch 094 - training loss: 0.3045\n",
      "2024-10-20 22:33:53 [INFO]: Epoch 095 - training loss: 0.1278\n",
      "2024-10-20 22:33:53 [INFO]: Epoch 096 - training loss: 0.1244\n",
      "2024-10-20 22:33:53 [INFO]: Epoch 097 - training loss: 0.1877\n",
      "2024-10-20 22:33:53 [INFO]: Epoch 098 - training loss: 0.1484\n",
      "2024-10-20 22:33:53 [INFO]: Epoch 099 - training loss: 0.1455\n",
      "2024-10-20 22:33:53 [INFO]: Epoch 100 - training loss: 0.1322\n",
      "2024-10-20 22:33:53 [INFO]: Finished training. The best model is from epoch#27.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:37:42 [INFO]: Epoch 001 - training loss: 0.1137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 24/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:37:42 [INFO]: Epoch 002 - training loss: 0.4214\n",
      "2024-10-20 22:37:43 [INFO]: Epoch 003 - training loss: 0.1934\n",
      "2024-10-20 22:37:43 [INFO]: Epoch 004 - training loss: 0.2191\n",
      "2024-10-20 22:37:43 [INFO]: Epoch 005 - training loss: 0.1497\n",
      "2024-10-20 22:37:43 [INFO]: Epoch 006 - training loss: 0.1555\n",
      "2024-10-20 22:37:43 [INFO]: Epoch 007 - training loss: 0.0913\n",
      "2024-10-20 22:37:43 [INFO]: Epoch 008 - training loss: 0.1391\n",
      "2024-10-20 22:37:43 [INFO]: Epoch 009 - training loss: 0.3650\n",
      "2024-10-20 22:37:44 [INFO]: Epoch 010 - training loss: 0.2022\n",
      "2024-10-20 22:37:44 [INFO]: Epoch 011 - training loss: 0.2644\n",
      "2024-10-20 22:37:44 [INFO]: Epoch 012 - training loss: 0.1431\n",
      "2024-10-20 22:37:44 [INFO]: Epoch 013 - training loss: 0.1666\n",
      "2024-10-20 22:37:44 [INFO]: Epoch 014 - training loss: 0.1233\n",
      "2024-10-20 22:37:44 [INFO]: Epoch 015 - training loss: 0.1447\n",
      "2024-10-20 22:37:45 [INFO]: Epoch 016 - training loss: 0.1650\n",
      "2024-10-20 22:37:45 [INFO]: Epoch 017 - training loss: 0.1338\n",
      "2024-10-20 22:37:45 [INFO]: Epoch 018 - training loss: 0.1240\n",
      "2024-10-20 22:37:45 [INFO]: Epoch 019 - training loss: 0.1146\n",
      "2024-10-20 22:37:45 [INFO]: Epoch 020 - training loss: 0.1577\n",
      "2024-10-20 22:37:45 [INFO]: Epoch 021 - training loss: 0.2080\n",
      "2024-10-20 22:37:45 [INFO]: Epoch 022 - training loss: 0.1921\n",
      "2024-10-20 22:37:46 [INFO]: Epoch 023 - training loss: 0.1376\n",
      "2024-10-20 22:37:46 [INFO]: Epoch 024 - training loss: 0.2889\n",
      "2024-10-20 22:37:46 [INFO]: Epoch 025 - training loss: 0.1912\n",
      "2024-10-20 22:37:46 [INFO]: Epoch 026 - training loss: 0.1445\n",
      "2024-10-20 22:37:46 [INFO]: Epoch 027 - training loss: 0.0837\n",
      "2024-10-20 22:37:46 [INFO]: Epoch 028 - training loss: 0.1745\n",
      "2024-10-20 22:37:46 [INFO]: Epoch 029 - training loss: 0.1686\n",
      "2024-10-20 22:37:47 [INFO]: Epoch 030 - training loss: 0.1159\n",
      "2024-10-20 22:37:47 [INFO]: Epoch 031 - training loss: 0.1129\n",
      "2024-10-20 22:37:47 [INFO]: Epoch 032 - training loss: 0.2790\n",
      "2024-10-20 22:37:47 [INFO]: Epoch 033 - training loss: 0.1582\n",
      "2024-10-20 22:37:47 [INFO]: Epoch 034 - training loss: 0.1459\n",
      "2024-10-20 22:37:47 [INFO]: Epoch 035 - training loss: 0.1473\n",
      "2024-10-20 22:37:48 [INFO]: Epoch 036 - training loss: 0.1791\n",
      "2024-10-20 22:37:48 [INFO]: Epoch 037 - training loss: 0.1982\n",
      "2024-10-20 22:37:48 [INFO]: Epoch 038 - training loss: 0.2424\n",
      "2024-10-20 22:37:48 [INFO]: Epoch 039 - training loss: 0.1271\n",
      "2024-10-20 22:37:48 [INFO]: Epoch 040 - training loss: 0.1712\n",
      "2024-10-20 22:37:48 [INFO]: Epoch 041 - training loss: 0.0934\n",
      "2024-10-20 22:37:48 [INFO]: Epoch 042 - training loss: 0.1831\n",
      "2024-10-20 22:37:49 [INFO]: Epoch 043 - training loss: 0.1650\n",
      "2024-10-20 22:37:49 [INFO]: Epoch 044 - training loss: 0.2233\n",
      "2024-10-20 22:37:49 [INFO]: Epoch 045 - training loss: 0.1800\n",
      "2024-10-20 22:37:49 [INFO]: Epoch 046 - training loss: 0.1790\n",
      "2024-10-20 22:37:49 [INFO]: Epoch 047 - training loss: 0.1276\n",
      "2024-10-20 22:37:49 [INFO]: Epoch 048 - training loss: 0.1169\n",
      "2024-10-20 22:37:49 [INFO]: Epoch 049 - training loss: 0.1743\n",
      "2024-10-20 22:37:50 [INFO]: Epoch 050 - training loss: 0.2112\n",
      "2024-10-20 22:37:50 [INFO]: Epoch 051 - training loss: 0.1513\n",
      "2024-10-20 22:37:50 [INFO]: Epoch 052 - training loss: 0.1968\n",
      "2024-10-20 22:37:50 [INFO]: Epoch 053 - training loss: 0.2265\n",
      "2024-10-20 22:37:50 [INFO]: Epoch 054 - training loss: 0.1168\n",
      "2024-10-20 22:37:50 [INFO]: Epoch 055 - training loss: 0.1547\n",
      "2024-10-20 22:37:51 [INFO]: Epoch 056 - training loss: 0.1884\n",
      "2024-10-20 22:37:51 [INFO]: Epoch 057 - training loss: 0.1977\n",
      "2024-10-20 22:37:51 [INFO]: Epoch 058 - training loss: 0.2479\n",
      "2024-10-20 22:37:51 [INFO]: Epoch 059 - training loss: 0.2368\n",
      "2024-10-20 22:37:51 [INFO]: Epoch 060 - training loss: 0.1442\n",
      "2024-10-20 22:37:51 [INFO]: Epoch 061 - training loss: 0.2034\n",
      "2024-10-20 22:37:51 [INFO]: Epoch 062 - training loss: 0.1358\n",
      "2024-10-20 22:37:52 [INFO]: Epoch 063 - training loss: 0.1226\n",
      "2024-10-20 22:37:52 [INFO]: Epoch 064 - training loss: 0.2933\n",
      "2024-10-20 22:37:52 [INFO]: Epoch 065 - training loss: 0.1058\n",
      "2024-10-20 22:37:52 [INFO]: Epoch 066 - training loss: 0.2384\n",
      "2024-10-20 22:37:52 [INFO]: Epoch 067 - training loss: 0.1311\n",
      "2024-10-20 22:37:52 [INFO]: Epoch 068 - training loss: 0.1120\n",
      "2024-10-20 22:37:52 [INFO]: Epoch 069 - training loss: 0.1250\n",
      "2024-10-20 22:37:53 [INFO]: Epoch 070 - training loss: 0.1840\n",
      "2024-10-20 22:37:53 [INFO]: Epoch 071 - training loss: 0.1468\n",
      "2024-10-20 22:37:53 [INFO]: Epoch 072 - training loss: 0.2236\n",
      "2024-10-20 22:37:53 [INFO]: Epoch 073 - training loss: 0.1919\n",
      "2024-10-20 22:37:53 [INFO]: Epoch 074 - training loss: 0.1857\n",
      "2024-10-20 22:37:53 [INFO]: Epoch 075 - training loss: 0.0915\n",
      "2024-10-20 22:37:53 [INFO]: Epoch 076 - training loss: 0.2189\n",
      "2024-10-20 22:37:54 [INFO]: Epoch 077 - training loss: 0.1142\n",
      "2024-10-20 22:37:54 [INFO]: Epoch 078 - training loss: 0.1345\n",
      "2024-10-20 22:37:54 [INFO]: Epoch 079 - training loss: 0.1770\n",
      "2024-10-20 22:37:54 [INFO]: Epoch 080 - training loss: 0.2101\n",
      "2024-10-20 22:37:54 [INFO]: Epoch 081 - training loss: 0.1139\n",
      "2024-10-20 22:37:54 [INFO]: Epoch 082 - training loss: 0.1963\n",
      "2024-10-20 22:37:55 [INFO]: Epoch 083 - training loss: 0.1828\n",
      "2024-10-20 22:37:55 [INFO]: Epoch 084 - training loss: 0.1300\n",
      "2024-10-20 22:37:55 [INFO]: Epoch 085 - training loss: 0.2563\n",
      "2024-10-20 22:37:55 [INFO]: Epoch 086 - training loss: 0.1550\n",
      "2024-10-20 22:37:55 [INFO]: Epoch 087 - training loss: 0.1314\n",
      "2024-10-20 22:37:55 [INFO]: Epoch 088 - training loss: 0.1857\n",
      "2024-10-20 22:37:55 [INFO]: Epoch 089 - training loss: 0.2682\n",
      "2024-10-20 22:37:56 [INFO]: Epoch 090 - training loss: 0.1227\n",
      "2024-10-20 22:37:56 [INFO]: Epoch 091 - training loss: 0.1547\n",
      "2024-10-20 22:37:56 [INFO]: Epoch 092 - training loss: 0.0985\n",
      "2024-10-20 22:37:56 [INFO]: Epoch 093 - training loss: 0.1672\n",
      "2024-10-20 22:37:56 [INFO]: Epoch 094 - training loss: 0.2018\n",
      "2024-10-20 22:37:56 [INFO]: Epoch 095 - training loss: 0.1462\n",
      "2024-10-20 22:37:56 [INFO]: Epoch 096 - training loss: 0.0809\n",
      "2024-10-20 22:37:57 [INFO]: Epoch 097 - training loss: 0.1103\n",
      "2024-10-20 22:37:57 [INFO]: Epoch 098 - training loss: 0.2015\n",
      "2024-10-20 22:37:57 [INFO]: Epoch 099 - training loss: 0.1538\n",
      "2024-10-20 22:37:57 [INFO]: Epoch 100 - training loss: 0.2299\n",
      "2024-10-20 22:37:57 [INFO]: Finished training. The best model is from epoch#96.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:41:46 [INFO]: Epoch 001 - training loss: 0.1241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 25/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:41:46 [INFO]: Epoch 002 - training loss: 0.2013\n",
      "2024-10-20 22:41:46 [INFO]: Epoch 003 - training loss: 0.1061\n",
      "2024-10-20 22:41:46 [INFO]: Epoch 004 - training loss: 0.1290\n",
      "2024-10-20 22:41:47 [INFO]: Epoch 005 - training loss: 0.1951\n",
      "2024-10-20 22:41:47 [INFO]: Epoch 006 - training loss: 0.1485\n",
      "2024-10-20 22:41:47 [INFO]: Epoch 007 - training loss: 0.1796\n",
      "2024-10-20 22:41:47 [INFO]: Epoch 008 - training loss: 0.1456\n",
      "2024-10-20 22:41:47 [INFO]: Epoch 009 - training loss: 0.2045\n",
      "2024-10-20 22:41:47 [INFO]: Epoch 010 - training loss: 0.1279\n",
      "2024-10-20 22:41:48 [INFO]: Epoch 011 - training loss: 0.1053\n",
      "2024-10-20 22:41:48 [INFO]: Epoch 012 - training loss: 0.1533\n",
      "2024-10-20 22:41:48 [INFO]: Epoch 013 - training loss: 0.1760\n",
      "2024-10-20 22:41:48 [INFO]: Epoch 014 - training loss: 0.1103\n",
      "2024-10-20 22:41:48 [INFO]: Epoch 015 - training loss: 0.1486\n",
      "2024-10-20 22:41:48 [INFO]: Epoch 016 - training loss: 0.1605\n",
      "2024-10-20 22:41:48 [INFO]: Epoch 017 - training loss: 0.1806\n",
      "2024-10-20 22:41:49 [INFO]: Epoch 018 - training loss: 0.1086\n",
      "2024-10-20 22:41:49 [INFO]: Epoch 019 - training loss: 0.1039\n",
      "2024-10-20 22:41:49 [INFO]: Epoch 020 - training loss: 0.2984\n",
      "2024-10-20 22:41:49 [INFO]: Epoch 021 - training loss: 0.1107\n",
      "2024-10-20 22:41:49 [INFO]: Epoch 022 - training loss: 0.1555\n",
      "2024-10-20 22:41:49 [INFO]: Epoch 023 - training loss: 0.2710\n",
      "2024-10-20 22:41:49 [INFO]: Epoch 024 - training loss: 0.1837\n",
      "2024-10-20 22:41:50 [INFO]: Epoch 025 - training loss: 0.1238\n",
      "2024-10-20 22:41:50 [INFO]: Epoch 026 - training loss: 0.2388\n",
      "2024-10-20 22:41:50 [INFO]: Epoch 027 - training loss: 0.1602\n",
      "2024-10-20 22:41:50 [INFO]: Epoch 028 - training loss: 0.3298\n",
      "2024-10-20 22:41:50 [INFO]: Epoch 029 - training loss: 0.2577\n",
      "2024-10-20 22:41:50 [INFO]: Epoch 030 - training loss: 0.2006\n",
      "2024-10-20 22:41:51 [INFO]: Epoch 031 - training loss: 0.1153\n",
      "2024-10-20 22:41:51 [INFO]: Epoch 032 - training loss: 0.2760\n",
      "2024-10-20 22:41:51 [INFO]: Epoch 033 - training loss: 0.1462\n",
      "2024-10-20 22:41:51 [INFO]: Epoch 034 - training loss: 0.1760\n",
      "2024-10-20 22:41:51 [INFO]: Epoch 035 - training loss: 0.1466\n",
      "2024-10-20 22:41:51 [INFO]: Epoch 036 - training loss: 0.1065\n",
      "2024-10-20 22:41:51 [INFO]: Epoch 037 - training loss: 0.2710\n",
      "2024-10-20 22:41:52 [INFO]: Epoch 038 - training loss: 0.2040\n",
      "2024-10-20 22:41:52 [INFO]: Epoch 039 - training loss: 0.1820\n",
      "2024-10-20 22:41:52 [INFO]: Epoch 040 - training loss: 0.1061\n",
      "2024-10-20 22:41:52 [INFO]: Epoch 041 - training loss: 0.1778\n",
      "2024-10-20 22:41:52 [INFO]: Epoch 042 - training loss: 0.2094\n",
      "2024-10-20 22:41:52 [INFO]: Epoch 043 - training loss: 0.3044\n",
      "2024-10-20 22:41:52 [INFO]: Epoch 044 - training loss: 0.1472\n",
      "2024-10-20 22:41:53 [INFO]: Epoch 045 - training loss: 0.1560\n",
      "2024-10-20 22:41:53 [INFO]: Epoch 046 - training loss: 0.2427\n",
      "2024-10-20 22:41:53 [INFO]: Epoch 047 - training loss: 0.3467\n",
      "2024-10-20 22:41:53 [INFO]: Epoch 048 - training loss: 0.1721\n",
      "2024-10-20 22:41:53 [INFO]: Epoch 049 - training loss: 0.1314\n",
      "2024-10-20 22:41:53 [INFO]: Epoch 050 - training loss: 0.1850\n",
      "2024-10-20 22:41:53 [INFO]: Epoch 051 - training loss: 0.0746\n",
      "2024-10-20 22:41:54 [INFO]: Epoch 052 - training loss: 0.1946\n",
      "2024-10-20 22:41:54 [INFO]: Epoch 053 - training loss: 0.1706\n",
      "2024-10-20 22:41:54 [INFO]: Epoch 054 - training loss: 0.1787\n",
      "2024-10-20 22:41:54 [INFO]: Epoch 055 - training loss: 0.2236\n",
      "2024-10-20 22:41:54 [INFO]: Epoch 056 - training loss: 0.2767\n",
      "2024-10-20 22:41:54 [INFO]: Epoch 057 - training loss: 0.1158\n",
      "2024-10-20 22:41:55 [INFO]: Epoch 058 - training loss: 0.1310\n",
      "2024-10-20 22:41:55 [INFO]: Epoch 059 - training loss: 0.1559\n",
      "2024-10-20 22:41:55 [INFO]: Epoch 060 - training loss: 0.2416\n",
      "2024-10-20 22:41:55 [INFO]: Epoch 061 - training loss: 0.0802\n",
      "2024-10-20 22:41:55 [INFO]: Epoch 062 - training loss: 0.1854\n",
      "2024-10-20 22:41:55 [INFO]: Epoch 063 - training loss: 0.1565\n",
      "2024-10-20 22:41:55 [INFO]: Epoch 064 - training loss: 0.2479\n",
      "2024-10-20 22:41:56 [INFO]: Epoch 065 - training loss: 0.2486\n",
      "2024-10-20 22:41:56 [INFO]: Epoch 066 - training loss: 0.1913\n",
      "2024-10-20 22:41:56 [INFO]: Epoch 067 - training loss: 0.1415\n",
      "2024-10-20 22:41:56 [INFO]: Epoch 068 - training loss: 0.0847\n",
      "2024-10-20 22:41:56 [INFO]: Epoch 069 - training loss: 0.2066\n",
      "2024-10-20 22:41:56 [INFO]: Epoch 070 - training loss: 0.1919\n",
      "2024-10-20 22:41:56 [INFO]: Epoch 071 - training loss: 0.1182\n",
      "2024-10-20 22:41:57 [INFO]: Epoch 072 - training loss: 0.1529\n",
      "2024-10-20 22:41:57 [INFO]: Epoch 073 - training loss: 0.1341\n",
      "2024-10-20 22:41:57 [INFO]: Epoch 074 - training loss: 0.0892\n",
      "2024-10-20 22:41:57 [INFO]: Epoch 075 - training loss: 0.1252\n",
      "2024-10-20 22:41:57 [INFO]: Epoch 076 - training loss: 0.2622\n",
      "2024-10-20 22:41:57 [INFO]: Epoch 077 - training loss: 0.0845\n",
      "2024-10-20 22:41:58 [INFO]: Epoch 078 - training loss: 0.1738\n",
      "2024-10-20 22:41:58 [INFO]: Epoch 079 - training loss: 0.1326\n",
      "2024-10-20 22:41:58 [INFO]: Epoch 080 - training loss: 0.1997\n",
      "2024-10-20 22:41:58 [INFO]: Epoch 081 - training loss: 0.1626\n",
      "2024-10-20 22:41:58 [INFO]: Epoch 082 - training loss: 0.1787\n",
      "2024-10-20 22:41:58 [INFO]: Epoch 083 - training loss: 0.0888\n",
      "2024-10-20 22:41:58 [INFO]: Epoch 084 - training loss: 0.1253\n",
      "2024-10-20 22:41:59 [INFO]: Epoch 085 - training loss: 0.1984\n",
      "2024-10-20 22:41:59 [INFO]: Epoch 086 - training loss: 0.2452\n",
      "2024-10-20 22:41:59 [INFO]: Epoch 087 - training loss: 0.1080\n",
      "2024-10-20 22:41:59 [INFO]: Epoch 088 - training loss: 0.1729\n",
      "2024-10-20 22:41:59 [INFO]: Epoch 089 - training loss: 0.1553\n",
      "2024-10-20 22:41:59 [INFO]: Epoch 090 - training loss: 0.1656\n",
      "2024-10-20 22:41:59 [INFO]: Epoch 091 - training loss: 0.2378\n",
      "2024-10-20 22:42:00 [INFO]: Epoch 092 - training loss: 0.1862\n",
      "2024-10-20 22:42:00 [INFO]: Epoch 093 - training loss: 0.1274\n",
      "2024-10-20 22:42:00 [INFO]: Epoch 094 - training loss: 0.1407\n",
      "2024-10-20 22:42:00 [INFO]: Epoch 095 - training loss: 0.1035\n",
      "2024-10-20 22:42:00 [INFO]: Epoch 096 - training loss: 0.2042\n",
      "2024-10-20 22:42:00 [INFO]: Epoch 097 - training loss: 0.1030\n",
      "2024-10-20 22:42:00 [INFO]: Epoch 098 - training loss: 0.1209\n",
      "2024-10-20 22:42:01 [INFO]: Epoch 099 - training loss: 0.2398\n",
      "2024-10-20 22:42:01 [INFO]: Epoch 100 - training loss: 0.1721\n",
      "2024-10-20 22:42:01 [INFO]: Finished training. The best model is from epoch#51.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:45:50 [INFO]: Epoch 001 - training loss: 0.1985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 26/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:45:50 [INFO]: Epoch 002 - training loss: 0.1093\n",
      "2024-10-20 22:45:50 [INFO]: Epoch 003 - training loss: 0.2285\n",
      "2024-10-20 22:45:50 [INFO]: Epoch 004 - training loss: 0.1538\n",
      "2024-10-20 22:45:50 [INFO]: Epoch 005 - training loss: 0.1310\n",
      "2024-10-20 22:45:51 [INFO]: Epoch 006 - training loss: 0.1346\n",
      "2024-10-20 22:45:51 [INFO]: Epoch 007 - training loss: 0.1419\n",
      "2024-10-20 22:45:51 [INFO]: Epoch 008 - training loss: 0.1515\n",
      "2024-10-20 22:45:51 [INFO]: Epoch 009 - training loss: 0.1361\n",
      "2024-10-20 22:45:51 [INFO]: Epoch 010 - training loss: 0.1346\n",
      "2024-10-20 22:45:51 [INFO]: Epoch 011 - training loss: 0.1342\n",
      "2024-10-20 22:45:51 [INFO]: Epoch 012 - training loss: 0.1882\n",
      "2024-10-20 22:45:52 [INFO]: Epoch 013 - training loss: 0.0765\n",
      "2024-10-20 22:45:52 [INFO]: Epoch 014 - training loss: 0.1116\n",
      "2024-10-20 22:45:52 [INFO]: Epoch 015 - training loss: 0.2055\n",
      "2024-10-20 22:45:52 [INFO]: Epoch 016 - training loss: 0.1203\n",
      "2024-10-20 22:45:52 [INFO]: Epoch 017 - training loss: 0.1523\n",
      "2024-10-20 22:45:52 [INFO]: Epoch 018 - training loss: 0.1347\n",
      "2024-10-20 22:45:53 [INFO]: Epoch 019 - training loss: 0.2889\n",
      "2024-10-20 22:45:53 [INFO]: Epoch 020 - training loss: 0.0945\n",
      "2024-10-20 22:45:53 [INFO]: Epoch 021 - training loss: 0.1618\n",
      "2024-10-20 22:45:53 [INFO]: Epoch 022 - training loss: 0.2694\n",
      "2024-10-20 22:45:53 [INFO]: Epoch 023 - training loss: 0.1282\n",
      "2024-10-20 22:45:53 [INFO]: Epoch 024 - training loss: 0.1888\n",
      "2024-10-20 22:45:53 [INFO]: Epoch 025 - training loss: 0.1525\n",
      "2024-10-20 22:45:54 [INFO]: Epoch 026 - training loss: 0.1805\n",
      "2024-10-20 22:45:54 [INFO]: Epoch 027 - training loss: 0.1860\n",
      "2024-10-20 22:45:54 [INFO]: Epoch 028 - training loss: 0.1733\n",
      "2024-10-20 22:45:54 [INFO]: Epoch 029 - training loss: 0.1431\n",
      "2024-10-20 22:45:54 [INFO]: Epoch 030 - training loss: 0.1402\n",
      "2024-10-20 22:45:54 [INFO]: Epoch 031 - training loss: 0.1229\n",
      "2024-10-20 22:45:55 [INFO]: Epoch 032 - training loss: 0.2656\n",
      "2024-10-20 22:45:55 [INFO]: Epoch 033 - training loss: 0.2042\n",
      "2024-10-20 22:45:55 [INFO]: Epoch 034 - training loss: 0.1647\n",
      "2024-10-20 22:45:55 [INFO]: Epoch 035 - training loss: 0.1353\n",
      "2024-10-20 22:45:55 [INFO]: Epoch 036 - training loss: 0.2426\n",
      "2024-10-20 22:45:55 [INFO]: Epoch 037 - training loss: 0.1182\n",
      "2024-10-20 22:45:55 [INFO]: Epoch 038 - training loss: 0.1335\n",
      "2024-10-20 22:45:56 [INFO]: Epoch 039 - training loss: 0.1064\n",
      "2024-10-20 22:45:56 [INFO]: Epoch 040 - training loss: 0.2001\n",
      "2024-10-20 22:45:56 [INFO]: Epoch 041 - training loss: 0.1672\n",
      "2024-10-20 22:45:56 [INFO]: Epoch 042 - training loss: 0.1592\n",
      "2024-10-20 22:45:56 [INFO]: Epoch 043 - training loss: 0.1488\n",
      "2024-10-20 22:45:56 [INFO]: Epoch 044 - training loss: 0.1582\n",
      "2024-10-20 22:45:56 [INFO]: Epoch 045 - training loss: 0.1713\n",
      "2024-10-20 22:45:57 [INFO]: Epoch 046 - training loss: 0.1203\n",
      "2024-10-20 22:45:57 [INFO]: Epoch 047 - training loss: 0.1319\n",
      "2024-10-20 22:45:57 [INFO]: Epoch 048 - training loss: 0.0889\n",
      "2024-10-20 22:45:57 [INFO]: Epoch 049 - training loss: 0.0847\n",
      "2024-10-20 22:45:57 [INFO]: Epoch 050 - training loss: 0.2081\n",
      "2024-10-20 22:45:57 [INFO]: Epoch 051 - training loss: 0.2061\n",
      "2024-10-20 22:45:57 [INFO]: Epoch 052 - training loss: 0.1696\n",
      "2024-10-20 22:45:58 [INFO]: Epoch 053 - training loss: 0.1112\n",
      "2024-10-20 22:45:58 [INFO]: Epoch 054 - training loss: 0.1008\n",
      "2024-10-20 22:45:58 [INFO]: Epoch 055 - training loss: 0.1068\n",
      "2024-10-20 22:45:58 [INFO]: Epoch 056 - training loss: 0.1361\n",
      "2024-10-20 22:45:58 [INFO]: Epoch 057 - training loss: 0.1422\n",
      "2024-10-20 22:45:58 [INFO]: Epoch 058 - training loss: 0.1572\n",
      "2024-10-20 22:45:59 [INFO]: Epoch 059 - training loss: 0.1267\n",
      "2024-10-20 22:45:59 [INFO]: Epoch 060 - training loss: 0.1506\n",
      "2024-10-20 22:45:59 [INFO]: Epoch 061 - training loss: 0.1065\n",
      "2024-10-20 22:45:59 [INFO]: Epoch 062 - training loss: 0.1158\n",
      "2024-10-20 22:45:59 [INFO]: Epoch 063 - training loss: 0.1369\n",
      "2024-10-20 22:45:59 [INFO]: Epoch 064 - training loss: 0.1092\n",
      "2024-10-20 22:45:59 [INFO]: Epoch 065 - training loss: 0.1651\n",
      "2024-10-20 22:46:00 [INFO]: Epoch 066 - training loss: 0.2326\n",
      "2024-10-20 22:46:00 [INFO]: Epoch 067 - training loss: 0.1719\n",
      "2024-10-20 22:46:00 [INFO]: Epoch 068 - training loss: 0.1368\n",
      "2024-10-20 22:46:00 [INFO]: Epoch 069 - training loss: 0.1349\n",
      "2024-10-20 22:46:00 [INFO]: Epoch 070 - training loss: 0.1605\n",
      "2024-10-20 22:46:00 [INFO]: Epoch 071 - training loss: 0.1370\n",
      "2024-10-20 22:46:00 [INFO]: Epoch 072 - training loss: 0.1178\n",
      "2024-10-20 22:46:01 [INFO]: Epoch 073 - training loss: 0.1442\n",
      "2024-10-20 22:46:01 [INFO]: Epoch 074 - training loss: 0.2080\n",
      "2024-10-20 22:46:01 [INFO]: Epoch 075 - training loss: 0.1449\n",
      "2024-10-20 22:46:01 [INFO]: Epoch 076 - training loss: 0.1399\n",
      "2024-10-20 22:46:01 [INFO]: Epoch 077 - training loss: 0.1210\n",
      "2024-10-20 22:46:01 [INFO]: Epoch 078 - training loss: 0.1486\n",
      "2024-10-20 22:46:02 [INFO]: Epoch 079 - training loss: 0.1394\n",
      "2024-10-20 22:46:02 [INFO]: Epoch 080 - training loss: 0.2029\n",
      "2024-10-20 22:46:02 [INFO]: Epoch 081 - training loss: 0.1163\n",
      "2024-10-20 22:46:02 [INFO]: Epoch 082 - training loss: 0.1306\n",
      "2024-10-20 22:46:02 [INFO]: Epoch 083 - training loss: 0.1330\n",
      "2024-10-20 22:46:02 [INFO]: Epoch 084 - training loss: 0.1145\n",
      "2024-10-20 22:46:02 [INFO]: Epoch 085 - training loss: 0.1606\n",
      "2024-10-20 22:46:03 [INFO]: Epoch 086 - training loss: 0.1180\n",
      "2024-10-20 22:46:03 [INFO]: Epoch 087 - training loss: 0.1500\n",
      "2024-10-20 22:46:03 [INFO]: Epoch 088 - training loss: 0.1240\n",
      "2024-10-20 22:46:03 [INFO]: Epoch 089 - training loss: 0.0809\n",
      "2024-10-20 22:46:03 [INFO]: Epoch 090 - training loss: 0.1108\n",
      "2024-10-20 22:46:03 [INFO]: Epoch 091 - training loss: 0.1331\n",
      "2024-10-20 22:46:03 [INFO]: Epoch 092 - training loss: 0.1457\n",
      "2024-10-20 22:46:04 [INFO]: Epoch 093 - training loss: 0.1246\n",
      "2024-10-20 22:46:04 [INFO]: Epoch 094 - training loss: 0.2008\n",
      "2024-10-20 22:46:04 [INFO]: Epoch 095 - training loss: 0.1419\n",
      "2024-10-20 22:46:04 [INFO]: Epoch 096 - training loss: 0.1583\n",
      "2024-10-20 22:46:04 [INFO]: Epoch 097 - training loss: 0.1145\n",
      "2024-10-20 22:46:04 [INFO]: Epoch 098 - training loss: 0.1801\n",
      "2024-10-20 22:46:05 [INFO]: Epoch 099 - training loss: 0.1222\n",
      "2024-10-20 22:46:05 [INFO]: Epoch 100 - training loss: 0.1394\n",
      "2024-10-20 22:46:05 [INFO]: Finished training. The best model is from epoch#13.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:49:53 [INFO]: Epoch 001 - training loss: 0.1583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 27/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:49:53 [INFO]: Epoch 002 - training loss: 0.1662\n",
      "2024-10-20 22:49:54 [INFO]: Epoch 003 - training loss: 0.1477\n",
      "2024-10-20 22:49:54 [INFO]: Epoch 004 - training loss: 0.1922\n",
      "2024-10-20 22:49:54 [INFO]: Epoch 005 - training loss: 0.1680\n",
      "2024-10-20 22:49:54 [INFO]: Epoch 006 - training loss: 0.1660\n",
      "2024-10-20 22:49:54 [INFO]: Epoch 007 - training loss: 0.1657\n",
      "2024-10-20 22:49:54 [INFO]: Epoch 008 - training loss: 0.1414\n",
      "2024-10-20 22:49:55 [INFO]: Epoch 009 - training loss: 0.1643\n",
      "2024-10-20 22:49:55 [INFO]: Epoch 010 - training loss: 0.1435\n",
      "2024-10-20 22:49:55 [INFO]: Epoch 011 - training loss: 0.1501\n",
      "2024-10-20 22:49:55 [INFO]: Epoch 012 - training loss: 0.2169\n",
      "2024-10-20 22:49:55 [INFO]: Epoch 013 - training loss: 0.1963\n",
      "2024-10-20 22:49:55 [INFO]: Epoch 014 - training loss: 0.1169\n",
      "2024-10-20 22:49:55 [INFO]: Epoch 015 - training loss: 0.3377\n",
      "2024-10-20 22:49:56 [INFO]: Epoch 016 - training loss: 0.1295\n",
      "2024-10-20 22:49:56 [INFO]: Epoch 017 - training loss: 0.1250\n",
      "2024-10-20 22:49:56 [INFO]: Epoch 018 - training loss: 0.1472\n",
      "2024-10-20 22:49:56 [INFO]: Epoch 019 - training loss: 0.1233\n",
      "2024-10-20 22:49:56 [INFO]: Epoch 020 - training loss: 0.1391\n",
      "2024-10-20 22:49:56 [INFO]: Epoch 021 - training loss: 0.1147\n",
      "2024-10-20 22:49:56 [INFO]: Epoch 022 - training loss: 0.1145\n",
      "2024-10-20 22:49:57 [INFO]: Epoch 023 - training loss: 0.1331\n",
      "2024-10-20 22:49:57 [INFO]: Epoch 024 - training loss: 0.1524\n",
      "2024-10-20 22:49:57 [INFO]: Epoch 025 - training loss: 0.1063\n",
      "2024-10-20 22:49:57 [INFO]: Epoch 026 - training loss: 0.2286\n",
      "2024-10-20 22:49:57 [INFO]: Epoch 027 - training loss: 0.2207\n",
      "2024-10-20 22:49:57 [INFO]: Epoch 028 - training loss: 0.1209\n",
      "2024-10-20 22:49:58 [INFO]: Epoch 029 - training loss: 0.1352\n",
      "2024-10-20 22:49:58 [INFO]: Epoch 030 - training loss: 0.1472\n",
      "2024-10-20 22:49:58 [INFO]: Epoch 031 - training loss: 0.1175\n",
      "2024-10-20 22:49:58 [INFO]: Epoch 032 - training loss: 0.1385\n",
      "2024-10-20 22:49:58 [INFO]: Epoch 033 - training loss: 0.1207\n",
      "2024-10-20 22:49:58 [INFO]: Epoch 034 - training loss: 0.1187\n",
      "2024-10-20 22:49:58 [INFO]: Epoch 035 - training loss: 0.1118\n",
      "2024-10-20 22:49:59 [INFO]: Epoch 036 - training loss: 0.2555\n",
      "2024-10-20 22:49:59 [INFO]: Epoch 037 - training loss: 0.1988\n",
      "2024-10-20 22:49:59 [INFO]: Epoch 038 - training loss: 0.1518\n",
      "2024-10-20 22:49:59 [INFO]: Epoch 039 - training loss: 0.2155\n",
      "2024-10-20 22:49:59 [INFO]: Epoch 040 - training loss: 0.1054\n",
      "2024-10-20 22:49:59 [INFO]: Epoch 041 - training loss: 0.1281\n",
      "2024-10-20 22:50:00 [INFO]: Epoch 042 - training loss: 0.1373\n",
      "2024-10-20 22:50:00 [INFO]: Epoch 043 - training loss: 0.1802\n",
      "2024-10-20 22:50:00 [INFO]: Epoch 044 - training loss: 0.1345\n",
      "2024-10-20 22:50:00 [INFO]: Epoch 045 - training loss: 0.2326\n",
      "2024-10-20 22:50:00 [INFO]: Epoch 046 - training loss: 0.2396\n",
      "2024-10-20 22:50:00 [INFO]: Epoch 047 - training loss: 0.1152\n",
      "2024-10-20 22:50:00 [INFO]: Epoch 048 - training loss: 0.0945\n",
      "2024-10-20 22:50:01 [INFO]: Epoch 049 - training loss: 0.1656\n",
      "2024-10-20 22:50:01 [INFO]: Epoch 050 - training loss: 0.1840\n",
      "2024-10-20 22:50:01 [INFO]: Epoch 051 - training loss: 0.1635\n",
      "2024-10-20 22:50:01 [INFO]: Epoch 052 - training loss: 0.1214\n",
      "2024-10-20 22:50:01 [INFO]: Epoch 053 - training loss: 0.0976\n",
      "2024-10-20 22:50:01 [INFO]: Epoch 054 - training loss: 0.1465\n",
      "2024-10-20 22:50:01 [INFO]: Epoch 055 - training loss: 0.1196\n",
      "2024-10-20 22:50:02 [INFO]: Epoch 056 - training loss: 0.1811\n",
      "2024-10-20 22:50:02 [INFO]: Epoch 057 - training loss: 0.1093\n",
      "2024-10-20 22:50:02 [INFO]: Epoch 058 - training loss: 0.1257\n",
      "2024-10-20 22:50:02 [INFO]: Epoch 059 - training loss: 0.1832\n",
      "2024-10-20 22:50:02 [INFO]: Epoch 060 - training loss: 0.1099\n",
      "2024-10-20 22:50:02 [INFO]: Epoch 061 - training loss: 0.2151\n",
      "2024-10-20 22:50:03 [INFO]: Epoch 062 - training loss: 0.2097\n",
      "2024-10-20 22:50:03 [INFO]: Epoch 063 - training loss: 0.1426\n",
      "2024-10-20 22:50:03 [INFO]: Epoch 064 - training loss: 0.2082\n",
      "2024-10-20 22:50:03 [INFO]: Epoch 065 - training loss: 0.1766\n",
      "2024-10-20 22:50:03 [INFO]: Epoch 066 - training loss: 0.1759\n",
      "2024-10-20 22:50:03 [INFO]: Epoch 067 - training loss: 0.1062\n",
      "2024-10-20 22:50:03 [INFO]: Epoch 068 - training loss: 0.1301\n",
      "2024-10-20 22:50:04 [INFO]: Epoch 069 - training loss: 0.1457\n",
      "2024-10-20 22:50:04 [INFO]: Epoch 070 - training loss: 0.1280\n",
      "2024-10-20 22:50:04 [INFO]: Epoch 071 - training loss: 0.1474\n",
      "2024-10-20 22:50:04 [INFO]: Epoch 072 - training loss: 0.1219\n",
      "2024-10-20 22:50:04 [INFO]: Epoch 073 - training loss: 0.2729\n",
      "2024-10-20 22:50:04 [INFO]: Epoch 074 - training loss: 0.1633\n",
      "2024-10-20 22:50:04 [INFO]: Epoch 075 - training loss: 0.1685\n",
      "2024-10-20 22:50:05 [INFO]: Epoch 076 - training loss: 0.1044\n",
      "2024-10-20 22:50:05 [INFO]: Epoch 077 - training loss: 0.1564\n",
      "2024-10-20 22:50:05 [INFO]: Epoch 078 - training loss: 0.1432\n",
      "2024-10-20 22:50:05 [INFO]: Epoch 079 - training loss: 0.1610\n",
      "2024-10-20 22:50:05 [INFO]: Epoch 080 - training loss: 0.1641\n",
      "2024-10-20 22:50:05 [INFO]: Epoch 081 - training loss: 0.1277\n",
      "2024-10-20 22:50:06 [INFO]: Epoch 082 - training loss: 0.2131\n",
      "2024-10-20 22:50:06 [INFO]: Epoch 083 - training loss: 0.1354\n",
      "2024-10-20 22:50:06 [INFO]: Epoch 084 - training loss: 0.1795\n",
      "2024-10-20 22:50:06 [INFO]: Epoch 085 - training loss: 0.2255\n",
      "2024-10-20 22:50:06 [INFO]: Epoch 086 - training loss: 0.1260\n",
      "2024-10-20 22:50:06 [INFO]: Epoch 087 - training loss: 0.1366\n",
      "2024-10-20 22:50:06 [INFO]: Epoch 088 - training loss: 0.1546\n",
      "2024-10-20 22:50:07 [INFO]: Epoch 089 - training loss: 0.1379\n",
      "2024-10-20 22:50:07 [INFO]: Epoch 090 - training loss: 0.1614\n",
      "2024-10-20 22:50:07 [INFO]: Epoch 091 - training loss: 0.1249\n",
      "2024-10-20 22:50:07 [INFO]: Epoch 092 - training loss: 0.1509\n",
      "2024-10-20 22:50:07 [INFO]: Epoch 093 - training loss: 0.3322\n",
      "2024-10-20 22:50:07 [INFO]: Epoch 094 - training loss: 0.0879\n",
      "2024-10-20 22:50:07 [INFO]: Epoch 095 - training loss: 0.1992\n",
      "2024-10-20 22:50:08 [INFO]: Epoch 096 - training loss: 0.1341\n",
      "2024-10-20 22:50:08 [INFO]: Epoch 097 - training loss: 0.1694\n",
      "2024-10-20 22:50:08 [INFO]: Epoch 098 - training loss: 0.2095\n",
      "2024-10-20 22:50:08 [INFO]: Epoch 099 - training loss: 0.1288\n",
      "2024-10-20 22:50:08 [INFO]: Epoch 100 - training loss: 0.1430\n",
      "2024-10-20 22:50:08 [INFO]: Finished training. The best model is from epoch#94.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:53:58 [INFO]: Epoch 001 - training loss: 0.1592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 28/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training CSDI on fold 28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:53:58 [INFO]: Epoch 002 - training loss: 0.1391\n",
      "2024-10-20 22:53:58 [INFO]: Epoch 003 - training loss: 0.1382\n",
      "2024-10-20 22:53:58 [INFO]: Epoch 004 - training loss: 0.1141\n",
      "2024-10-20 22:53:59 [INFO]: Epoch 005 - training loss: 0.1679\n",
      "2024-10-20 22:53:59 [INFO]: Epoch 006 - training loss: 0.3235\n",
      "2024-10-20 22:53:59 [INFO]: Epoch 007 - training loss: 0.1858\n",
      "2024-10-20 22:53:59 [INFO]: Epoch 008 - training loss: 0.1304\n",
      "2024-10-20 22:53:59 [INFO]: Epoch 009 - training loss: 0.1757\n",
      "2024-10-20 22:53:59 [INFO]: Epoch 010 - training loss: 0.2476\n",
      "2024-10-20 22:54:00 [INFO]: Epoch 011 - training loss: 0.1258\n",
      "2024-10-20 22:54:00 [INFO]: Epoch 012 - training loss: 0.1229\n",
      "2024-10-20 22:54:00 [INFO]: Epoch 013 - training loss: 0.1580\n",
      "2024-10-20 22:54:00 [INFO]: Epoch 014 - training loss: 0.1616\n",
      "2024-10-20 22:54:00 [INFO]: Epoch 015 - training loss: 0.1235\n",
      "2024-10-20 22:54:00 [INFO]: Epoch 016 - training loss: 0.2093\n",
      "2024-10-20 22:54:00 [INFO]: Epoch 017 - training loss: 0.1554\n",
      "2024-10-20 22:54:01 [INFO]: Epoch 018 - training loss: 0.1795\n",
      "2024-10-20 22:54:01 [INFO]: Epoch 019 - training loss: 0.1389\n",
      "2024-10-20 22:54:01 [INFO]: Epoch 020 - training loss: 0.1267\n",
      "2024-10-20 22:54:01 [INFO]: Epoch 021 - training loss: 0.1229\n",
      "2024-10-20 22:54:01 [INFO]: Epoch 022 - training loss: 0.1575\n",
      "2024-10-20 22:54:01 [INFO]: Epoch 023 - training loss: 0.1692\n",
      "2024-10-20 22:54:02 [INFO]: Epoch 024 - training loss: 0.0682\n",
      "2024-10-20 22:54:02 [INFO]: Epoch 025 - training loss: 0.1517\n",
      "2024-10-20 22:54:02 [INFO]: Epoch 026 - training loss: 0.1598\n",
      "2024-10-20 22:54:02 [INFO]: Epoch 027 - training loss: 0.1856\n",
      "2024-10-20 22:54:02 [INFO]: Epoch 028 - training loss: 0.1656\n",
      "2024-10-20 22:54:02 [INFO]: Epoch 029 - training loss: 0.1482\n",
      "2024-10-20 22:54:02 [INFO]: Epoch 030 - training loss: 0.1535\n",
      "2024-10-20 22:54:03 [INFO]: Epoch 031 - training loss: 0.1461\n",
      "2024-10-20 22:54:03 [INFO]: Epoch 032 - training loss: 0.1405\n",
      "2024-10-20 22:54:03 [INFO]: Epoch 033 - training loss: 0.1250\n",
      "2024-10-20 22:54:03 [INFO]: Epoch 034 - training loss: 0.1316\n",
      "2024-10-20 22:54:03 [INFO]: Epoch 035 - training loss: 0.1032\n",
      "2024-10-20 22:54:03 [INFO]: Epoch 036 - training loss: 0.1565\n",
      "2024-10-20 22:54:03 [INFO]: Epoch 037 - training loss: 0.1640\n",
      "2024-10-20 22:54:04 [INFO]: Epoch 038 - training loss: 0.1367\n",
      "2024-10-20 22:54:04 [INFO]: Epoch 039 - training loss: 0.1567\n",
      "2024-10-20 22:54:04 [INFO]: Epoch 040 - training loss: 0.1856\n",
      "2024-10-20 22:54:04 [INFO]: Epoch 041 - training loss: 0.2115\n",
      "2024-10-20 22:54:04 [INFO]: Epoch 042 - training loss: 0.1581\n",
      "2024-10-20 22:54:04 [INFO]: Epoch 043 - training loss: 0.1513\n",
      "2024-10-20 22:54:05 [INFO]: Epoch 044 - training loss: 0.2203\n",
      "2024-10-20 22:54:05 [INFO]: Epoch 045 - training loss: 0.1802\n",
      "2024-10-20 22:54:05 [INFO]: Epoch 046 - training loss: 0.1150\n",
      "2024-10-20 22:54:05 [INFO]: Epoch 047 - training loss: 0.2069\n",
      "2024-10-20 22:54:05 [INFO]: Epoch 048 - training loss: 0.1099\n",
      "2024-10-20 22:54:05 [INFO]: Epoch 049 - training loss: 0.1515\n",
      "2024-10-20 22:54:05 [INFO]: Epoch 050 - training loss: 0.1606\n",
      "2024-10-20 22:54:06 [INFO]: Epoch 051 - training loss: 0.2595\n",
      "2024-10-20 22:54:06 [INFO]: Epoch 052 - training loss: 0.1788\n",
      "2024-10-20 22:54:06 [INFO]: Epoch 053 - training loss: 0.1878\n",
      "2024-10-20 22:54:06 [INFO]: Epoch 054 - training loss: 0.1693\n",
      "2024-10-20 22:54:06 [INFO]: Epoch 055 - training loss: 0.2287\n",
      "2024-10-20 22:54:06 [INFO]: Epoch 056 - training loss: 0.1264\n",
      "2024-10-20 22:54:07 [INFO]: Epoch 057 - training loss: 0.1557\n",
      "2024-10-20 22:54:07 [INFO]: Epoch 058 - training loss: 0.1754\n",
      "2024-10-20 22:54:07 [INFO]: Epoch 059 - training loss: 0.1922\n",
      "2024-10-20 22:54:07 [INFO]: Epoch 060 - training loss: 0.1589\n",
      "2024-10-20 22:54:07 [INFO]: Epoch 061 - training loss: 0.1604\n",
      "2024-10-20 22:54:07 [INFO]: Epoch 062 - training loss: 0.1146\n",
      "2024-10-20 22:54:07 [INFO]: Epoch 063 - training loss: 0.1629\n",
      "2024-10-20 22:54:08 [INFO]: Epoch 064 - training loss: 0.1999\n",
      "2024-10-20 22:54:08 [INFO]: Epoch 065 - training loss: 0.1218\n",
      "2024-10-20 22:54:08 [INFO]: Epoch 066 - training loss: 0.1568\n",
      "2024-10-20 22:54:08 [INFO]: Epoch 067 - training loss: 0.1826\n",
      "2024-10-20 22:54:08 [INFO]: Epoch 068 - training loss: 0.2092\n",
      "2024-10-20 22:54:08 [INFO]: Epoch 069 - training loss: 0.2976\n",
      "2024-10-20 22:54:08 [INFO]: Epoch 070 - training loss: 0.1536\n",
      "2024-10-20 22:54:09 [INFO]: Epoch 071 - training loss: 0.1317\n",
      "2024-10-20 22:54:09 [INFO]: Epoch 072 - training loss: 0.2038\n",
      "2024-10-20 22:54:09 [INFO]: Epoch 073 - training loss: 0.3110\n",
      "2024-10-20 22:54:09 [INFO]: Epoch 074 - training loss: 0.1197\n",
      "2024-10-20 22:54:09 [INFO]: Epoch 075 - training loss: 0.1463\n",
      "2024-10-20 22:54:09 [INFO]: Epoch 076 - training loss: 0.0925\n",
      "2024-10-20 22:54:10 [INFO]: Epoch 077 - training loss: 0.1258\n",
      "2024-10-20 22:54:10 [INFO]: Epoch 078 - training loss: 0.3235\n",
      "2024-10-20 22:54:10 [INFO]: Epoch 079 - training loss: 0.2270\n",
      "2024-10-20 22:54:10 [INFO]: Epoch 080 - training loss: 0.1367\n",
      "2024-10-20 22:54:10 [INFO]: Epoch 081 - training loss: 0.0980\n",
      "2024-10-20 22:54:10 [INFO]: Epoch 082 - training loss: 0.1203\n",
      "2024-10-20 22:54:10 [INFO]: Epoch 083 - training loss: 0.1504\n",
      "2024-10-20 22:54:11 [INFO]: Epoch 084 - training loss: 0.1622\n",
      "2024-10-20 22:54:11 [INFO]: Epoch 085 - training loss: 0.1029\n",
      "2024-10-20 22:54:11 [INFO]: Epoch 086 - training loss: 0.1659\n",
      "2024-10-20 22:54:11 [INFO]: Epoch 087 - training loss: 0.1435\n",
      "2024-10-20 22:54:11 [INFO]: Epoch 088 - training loss: 0.1518\n",
      "2024-10-20 22:54:11 [INFO]: Epoch 089 - training loss: 0.0985\n",
      "2024-10-20 22:54:11 [INFO]: Epoch 090 - training loss: 0.1834\n",
      "2024-10-20 22:54:12 [INFO]: Epoch 091 - training loss: 0.1297\n",
      "2024-10-20 22:54:12 [INFO]: Epoch 092 - training loss: 0.1881\n",
      "2024-10-20 22:54:12 [INFO]: Epoch 093 - training loss: 0.1554\n",
      "2024-10-20 22:54:12 [INFO]: Epoch 094 - training loss: 0.0843\n",
      "2024-10-20 22:54:12 [INFO]: Epoch 095 - training loss: 0.2258\n",
      "2024-10-20 22:54:12 [INFO]: Epoch 096 - training loss: 0.1295\n",
      "2024-10-20 22:54:13 [INFO]: Epoch 097 - training loss: 0.1184\n",
      "2024-10-20 22:54:13 [INFO]: Epoch 098 - training loss: 0.2187\n",
      "2024-10-20 22:54:13 [INFO]: Epoch 099 - training loss: 0.2447\n",
      "2024-10-20 22:54:13 [INFO]: Epoch 100 - training loss: 0.1045\n",
      "2024-10-20 22:54:13 [INFO]: Finished training. The best model is from epoch#24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:58:02 [INFO]: Epoch 001 - training loss: 0.1787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 29/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training CSDI on fold 29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:58:02 [INFO]: Epoch 002 - training loss: 0.2150\n",
      "2024-10-20 22:58:02 [INFO]: Epoch 003 - training loss: 0.1832\n",
      "2024-10-20 22:58:02 [INFO]: Epoch 004 - training loss: 0.1769\n",
      "2024-10-20 22:58:02 [INFO]: Epoch 005 - training loss: 0.2131\n",
      "2024-10-20 22:58:03 [INFO]: Epoch 006 - training loss: 0.1496\n",
      "2024-10-20 22:58:03 [INFO]: Epoch 007 - training loss: 0.1724\n",
      "2024-10-20 22:58:03 [INFO]: Epoch 008 - training loss: 0.1904\n",
      "2024-10-20 22:58:03 [INFO]: Epoch 009 - training loss: 0.1722\n",
      "2024-10-20 22:58:03 [INFO]: Epoch 010 - training loss: 0.1204\n",
      "2024-10-20 22:58:03 [INFO]: Epoch 011 - training loss: 0.1019\n",
      "2024-10-20 22:58:04 [INFO]: Epoch 012 - training loss: 0.1651\n",
      "2024-10-20 22:58:04 [INFO]: Epoch 013 - training loss: 0.1415\n",
      "2024-10-20 22:58:04 [INFO]: Epoch 014 - training loss: 0.1120\n",
      "2024-10-20 22:58:04 [INFO]: Epoch 015 - training loss: 0.0957\n",
      "2024-10-20 22:58:04 [INFO]: Epoch 016 - training loss: 0.1264\n",
      "2024-10-20 22:58:04 [INFO]: Epoch 017 - training loss: 0.2011\n",
      "2024-10-20 22:58:04 [INFO]: Epoch 018 - training loss: 0.2153\n",
      "2024-10-20 22:58:05 [INFO]: Epoch 019 - training loss: 0.1506\n",
      "2024-10-20 22:58:05 [INFO]: Epoch 020 - training loss: 0.1369\n",
      "2024-10-20 22:58:05 [INFO]: Epoch 021 - training loss: 0.1729\n",
      "2024-10-20 22:58:05 [INFO]: Epoch 022 - training loss: 0.2364\n",
      "2024-10-20 22:58:05 [INFO]: Epoch 023 - training loss: 0.3177\n",
      "2024-10-20 22:58:05 [INFO]: Epoch 024 - training loss: 0.1871\n",
      "2024-10-20 22:58:06 [INFO]: Epoch 025 - training loss: 0.1420\n",
      "2024-10-20 22:58:06 [INFO]: Epoch 026 - training loss: 0.2180\n",
      "2024-10-20 22:58:06 [INFO]: Epoch 027 - training loss: 0.0972\n",
      "2024-10-20 22:58:06 [INFO]: Epoch 028 - training loss: 0.1635\n",
      "2024-10-20 22:58:06 [INFO]: Epoch 029 - training loss: 0.2319\n",
      "2024-10-20 22:58:06 [INFO]: Epoch 030 - training loss: 0.1281\n",
      "2024-10-20 22:58:06 [INFO]: Epoch 031 - training loss: 0.1689\n",
      "2024-10-20 22:58:07 [INFO]: Epoch 032 - training loss: 0.2269\n",
      "2024-10-20 22:58:07 [INFO]: Epoch 033 - training loss: 0.2149\n",
      "2024-10-20 22:58:07 [INFO]: Epoch 034 - training loss: 0.2145\n",
      "2024-10-20 22:58:07 [INFO]: Epoch 035 - training loss: 0.1448\n",
      "2024-10-20 22:58:07 [INFO]: Epoch 036 - training loss: 0.1462\n",
      "2024-10-20 22:58:07 [INFO]: Epoch 037 - training loss: 0.2663\n",
      "2024-10-20 22:58:07 [INFO]: Epoch 038 - training loss: 0.0971\n",
      "2024-10-20 22:58:08 [INFO]: Epoch 039 - training loss: 0.0860\n",
      "2024-10-20 22:58:08 [INFO]: Epoch 040 - training loss: 0.1569\n",
      "2024-10-20 22:58:08 [INFO]: Epoch 041 - training loss: 0.1508\n",
      "2024-10-20 22:58:08 [INFO]: Epoch 042 - training loss: 0.1138\n",
      "2024-10-20 22:58:08 [INFO]: Epoch 043 - training loss: 0.1537\n",
      "2024-10-20 22:58:08 [INFO]: Epoch 044 - training loss: 0.1525\n",
      "2024-10-20 22:58:08 [INFO]: Epoch 045 - training loss: 0.2625\n",
      "2024-10-20 22:58:09 [INFO]: Epoch 046 - training loss: 0.1135\n",
      "2024-10-20 22:58:09 [INFO]: Epoch 047 - training loss: 0.1963\n",
      "2024-10-20 22:58:09 [INFO]: Epoch 048 - training loss: 0.1554\n",
      "2024-10-20 22:58:09 [INFO]: Epoch 049 - training loss: 0.1963\n",
      "2024-10-20 22:58:09 [INFO]: Epoch 050 - training loss: 0.0955\n",
      "2024-10-20 22:58:09 [INFO]: Epoch 051 - training loss: 0.1314\n",
      "2024-10-20 22:58:10 [INFO]: Epoch 052 - training loss: 0.0962\n",
      "2024-10-20 22:58:10 [INFO]: Epoch 053 - training loss: 0.3156\n",
      "2024-10-20 22:58:10 [INFO]: Epoch 054 - training loss: 0.2264\n",
      "2024-10-20 22:58:10 [INFO]: Epoch 055 - training loss: 0.2570\n",
      "2024-10-20 22:58:10 [INFO]: Epoch 056 - training loss: 0.1267\n",
      "2024-10-20 22:58:10 [INFO]: Epoch 057 - training loss: 0.1102\n",
      "2024-10-20 22:58:10 [INFO]: Epoch 058 - training loss: 0.2456\n",
      "2024-10-20 22:58:11 [INFO]: Epoch 059 - training loss: 0.2071\n",
      "2024-10-20 22:58:11 [INFO]: Epoch 060 - training loss: 0.0907\n",
      "2024-10-20 22:58:11 [INFO]: Epoch 061 - training loss: 0.1074\n",
      "2024-10-20 22:58:11 [INFO]: Epoch 062 - training loss: 0.1093\n",
      "2024-10-20 22:58:11 [INFO]: Epoch 063 - training loss: 0.1170\n",
      "2024-10-20 22:58:11 [INFO]: Epoch 064 - training loss: 0.1905\n",
      "2024-10-20 22:58:11 [INFO]: Epoch 065 - training loss: 0.1591\n",
      "2024-10-20 22:58:12 [INFO]: Epoch 066 - training loss: 0.0967\n",
      "2024-10-20 22:58:12 [INFO]: Epoch 067 - training loss: 0.0842\n",
      "2024-10-20 22:58:12 [INFO]: Epoch 068 - training loss: 0.1101\n",
      "2024-10-20 22:58:12 [INFO]: Epoch 069 - training loss: 0.2273\n",
      "2024-10-20 22:58:12 [INFO]: Epoch 070 - training loss: 0.1045\n",
      "2024-10-20 22:58:12 [INFO]: Epoch 071 - training loss: 0.1573\n",
      "2024-10-20 22:58:13 [INFO]: Epoch 072 - training loss: 0.1570\n",
      "2024-10-20 22:58:13 [INFO]: Epoch 073 - training loss: 0.1549\n",
      "2024-10-20 22:58:13 [INFO]: Epoch 074 - training loss: 0.1016\n",
      "2024-10-20 22:58:13 [INFO]: Epoch 075 - training loss: 0.1741\n",
      "2024-10-20 22:58:13 [INFO]: Epoch 076 - training loss: 0.1413\n",
      "2024-10-20 22:58:13 [INFO]: Epoch 077 - training loss: 0.1632\n",
      "2024-10-20 22:58:13 [INFO]: Epoch 078 - training loss: 0.1179\n",
      "2024-10-20 22:58:14 [INFO]: Epoch 079 - training loss: 0.1004\n",
      "2024-10-20 22:58:14 [INFO]: Epoch 080 - training loss: 0.1215\n",
      "2024-10-20 22:58:14 [INFO]: Epoch 081 - training loss: 0.1579\n",
      "2024-10-20 22:58:14 [INFO]: Epoch 082 - training loss: 0.1671\n",
      "2024-10-20 22:58:14 [INFO]: Epoch 083 - training loss: 0.1407\n",
      "2024-10-20 22:58:14 [INFO]: Epoch 084 - training loss: 0.1005\n",
      "2024-10-20 22:58:15 [INFO]: Epoch 085 - training loss: 0.1540\n",
      "2024-10-20 22:58:15 [INFO]: Epoch 086 - training loss: 0.1374\n",
      "2024-10-20 22:58:15 [INFO]: Epoch 087 - training loss: 0.2149\n",
      "2024-10-20 22:58:15 [INFO]: Epoch 088 - training loss: 0.1080\n",
      "2024-10-20 22:58:15 [INFO]: Epoch 089 - training loss: 0.1177\n",
      "2024-10-20 22:58:15 [INFO]: Epoch 090 - training loss: 0.1162\n",
      "2024-10-20 22:58:15 [INFO]: Epoch 091 - training loss: 0.0940\n",
      "2024-10-20 22:58:16 [INFO]: Epoch 092 - training loss: 0.1556\n",
      "2024-10-20 22:58:16 [INFO]: Epoch 093 - training loss: 0.0972\n",
      "2024-10-20 22:58:16 [INFO]: Epoch 094 - training loss: 0.1605\n",
      "2024-10-20 22:58:16 [INFO]: Epoch 095 - training loss: 0.1185\n",
      "2024-10-20 22:58:16 [INFO]: Epoch 096 - training loss: 0.1055\n",
      "2024-10-20 22:58:16 [INFO]: Epoch 097 - training loss: 0.0985\n",
      "2024-10-20 22:58:16 [INFO]: Epoch 098 - training loss: 0.1520\n",
      "2024-10-20 22:58:17 [INFO]: Epoch 099 - training loss: 0.1760\n",
      "2024-10-20 22:58:17 [INFO]: Epoch 100 - training loss: 0.1211\n",
      "2024-10-20 22:58:17 [INFO]: Finished training. The best model is from epoch#67.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    }
   ],
   "source": [
    "fold_scores_csdi = evaluate_folds_csdi(Xs, ys, resample_fold_idxs, window_idxs,csdi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ECG200_csdi_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(fold_scores_csdi, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean for 5% missing on fold 0: 0.2804880262267892\n",
      "Mean for 15% missing on fold 0: 0.3253034596447586\n",
      "Mean for 25% missing on fold 0: 0.41105413790522444\n",
      "Mean for 35% missing on fold 0: 0.5038128821280228\n",
      "Mean for 45% missing on fold 0: 0.5059458514683813\n",
      "Mean for 55% missing on fold 0: 0.5273815033110955\n",
      "Mean for 65% missing on fold 0: 0.5501303269912098\n",
      "Mean for 75% missing on fold 0: 0.5450053950417332\n",
      "Mean for 85% missing on fold 0: 0.5626965846180365\n",
      "Mean for 95% missing on fold 0: 0.6061931814252622\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 1: 0.3082787010277268\n",
      "Mean for 15% missing on fold 1: 0.33864635926529024\n",
      "Mean for 25% missing on fold 1: 0.4001187082048819\n",
      "Mean for 35% missing on fold 1: 0.465150877155686\n",
      "Mean for 45% missing on fold 1: 0.47083169801946456\n",
      "Mean for 55% missing on fold 1: 0.49155675967182094\n",
      "Mean for 65% missing on fold 1: 0.5021587270524326\n",
      "Mean for 75% missing on fold 1: 0.5035974939272478\n",
      "Mean for 85% missing on fold 1: 0.530582513247287\n",
      "Mean for 95% missing on fold 1: 0.5633642352943177\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 2: 0.30429671533038566\n",
      "Mean for 15% missing on fold 2: 0.3297510461124665\n",
      "Mean for 25% missing on fold 2: 0.36881649441802183\n",
      "Mean for 35% missing on fold 2: 0.45921810677385155\n",
      "Mean for 45% missing on fold 2: 0.4901541376783161\n",
      "Mean for 55% missing on fold 2: 0.5024889193280312\n",
      "Mean for 65% missing on fold 2: 0.527413106965941\n",
      "Mean for 75% missing on fold 2: 0.5278773932416461\n",
      "Mean for 85% missing on fold 2: 0.5408296291387952\n",
      "Mean for 95% missing on fold 2: 0.6241356942750578\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 3: 0.27140297010584585\n",
      "Mean for 15% missing on fold 3: 0.31367276581652725\n",
      "Mean for 25% missing on fold 3: 0.33235251409008537\n",
      "Mean for 35% missing on fold 3: 0.3604748729895883\n",
      "Mean for 45% missing on fold 3: 0.3952154580400836\n",
      "Mean for 55% missing on fold 3: 0.4216527996582343\n",
      "Mean for 65% missing on fold 3: 0.4396985585369967\n",
      "Mean for 75% missing on fold 3: 0.4467845091687234\n",
      "Mean for 85% missing on fold 3: 0.49616204726544944\n",
      "Mean for 95% missing on fold 3: 0.5626199720691247\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 4: 0.23529972165212978\n",
      "Mean for 15% missing on fold 4: 0.2957074002458408\n",
      "Mean for 25% missing on fold 4: 0.34444865805287656\n",
      "Mean for 35% missing on fold 4: 0.39463322436782866\n",
      "Mean for 45% missing on fold 4: 0.4137508007925399\n",
      "Mean for 55% missing on fold 4: 0.43852535416669236\n",
      "Mean for 65% missing on fold 4: 0.44028268720428126\n",
      "Mean for 75% missing on fold 4: 0.4501979130287159\n",
      "Mean for 85% missing on fold 4: 0.4893975109585604\n",
      "Mean for 95% missing on fold 4: 0.5341417442540284\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 5: 0.2370753928467861\n",
      "Mean for 15% missing on fold 5: 0.29696603452006254\n",
      "Mean for 25% missing on fold 5: 0.3183824975300702\n",
      "Mean for 35% missing on fold 5: 0.36199753671688106\n",
      "Mean for 45% missing on fold 5: 0.3912229311061363\n",
      "Mean for 55% missing on fold 5: 0.4214439392078873\n",
      "Mean for 65% missing on fold 5: 0.4325732857090686\n",
      "Mean for 75% missing on fold 5: 0.44288116726981486\n",
      "Mean for 85% missing on fold 5: 0.48386920692216656\n",
      "Mean for 95% missing on fold 5: 0.527622293937004\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 6: 0.2264536113695899\n",
      "Mean for 15% missing on fold 6: 0.27151972831325255\n",
      "Mean for 25% missing on fold 6: 0.3018956365458136\n",
      "Mean for 35% missing on fold 6: 0.3433272199210024\n",
      "Mean for 45% missing on fold 6: 0.359950485646306\n",
      "Mean for 55% missing on fold 6: 0.3880054968121508\n",
      "Mean for 65% missing on fold 6: 0.3979634122002158\n",
      "Mean for 75% missing on fold 6: 0.4022670160939326\n",
      "Mean for 85% missing on fold 6: 0.44464188473375976\n",
      "Mean for 95% missing on fold 6: 0.5070286632274247\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 7: 0.2526392528838765\n",
      "Mean for 15% missing on fold 7: 0.29794415510328887\n",
      "Mean for 25% missing on fold 7: 0.33620644969410524\n",
      "Mean for 35% missing on fold 7: 0.39871783006445977\n",
      "Mean for 45% missing on fold 7: 0.4127551205609693\n",
      "Mean for 55% missing on fold 7: 0.43919463267381836\n",
      "Mean for 65% missing on fold 7: 0.4654811154123911\n",
      "Mean for 75% missing on fold 7: 0.46503460238535205\n",
      "Mean for 85% missing on fold 7: 0.5150611171565938\n",
      "Mean for 95% missing on fold 7: 0.5931116250623485\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 8: 0.23465311041510775\n",
      "Mean for 15% missing on fold 8: 0.25695860115307373\n",
      "Mean for 25% missing on fold 8: 0.2910419900055147\n",
      "Mean for 35% missing on fold 8: 0.33204559746327944\n",
      "Mean for 45% missing on fold 8: 0.3636927926292237\n",
      "Mean for 55% missing on fold 8: 0.38726659423244925\n",
      "Mean for 65% missing on fold 8: 0.3988413525954935\n",
      "Mean for 75% missing on fold 8: 0.3993938622849362\n",
      "Mean for 85% missing on fold 8: 0.4359368397651862\n",
      "Mean for 95% missing on fold 8: 0.49284974150573924\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 9: 0.21472282639203266\n",
      "Mean for 15% missing on fold 9: 0.2663346645660708\n",
      "Mean for 25% missing on fold 9: 0.27713071628701175\n",
      "Mean for 35% missing on fold 9: 0.31349095849666886\n",
      "Mean for 45% missing on fold 9: 0.35259210765152815\n",
      "Mean for 55% missing on fold 9: 0.38967154772355767\n",
      "Mean for 65% missing on fold 9: 0.39970270622526294\n",
      "Mean for 75% missing on fold 9: 0.40107731696741317\n",
      "Mean for 85% missing on fold 9: 0.44938110618725974\n",
      "Mean for 95% missing on fold 9: 0.5369340205865626\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 10: 0.22802376221836965\n",
      "Mean for 15% missing on fold 10: 0.2632925160772774\n",
      "Mean for 25% missing on fold 10: 0.290269488368931\n",
      "Mean for 35% missing on fold 10: 0.329500866542752\n",
      "Mean for 45% missing on fold 10: 0.35355955392296134\n",
      "Mean for 55% missing on fold 10: 0.38347239620710344\n",
      "Mean for 65% missing on fold 10: 0.40209005728835445\n",
      "Mean for 75% missing on fold 10: 0.41377492058184573\n",
      "Mean for 85% missing on fold 10: 0.4522534802581633\n",
      "Mean for 95% missing on fold 10: 0.5132316013053312\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 11: 0.22088188041458898\n",
      "Mean for 15% missing on fold 11: 0.26309403268040304\n",
      "Mean for 25% missing on fold 11: 0.2841288421661403\n",
      "Mean for 35% missing on fold 11: 0.32939391245302463\n",
      "Mean for 45% missing on fold 11: 0.3794416036979085\n",
      "Mean for 55% missing on fold 11: 0.4258450522276403\n",
      "Mean for 65% missing on fold 11: 0.43224816957514817\n",
      "Mean for 75% missing on fold 11: 0.4608706908368008\n",
      "Mean for 85% missing on fold 11: 0.5119300397175376\n",
      "Mean for 95% missing on fold 11: 0.586764739501675\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 12: 0.22213151521811192\n",
      "Mean for 15% missing on fold 12: 0.2749137493593373\n",
      "Mean for 25% missing on fold 12: 0.30361693949665025\n",
      "Mean for 35% missing on fold 12: 0.3446462153706381\n",
      "Mean for 45% missing on fold 12: 0.36127738187247294\n",
      "Mean for 55% missing on fold 12: 0.38018372207381645\n",
      "Mean for 65% missing on fold 12: 0.40215931094001117\n",
      "Mean for 75% missing on fold 12: 0.40372557411638427\n",
      "Mean for 85% missing on fold 12: 0.43490754213842525\n",
      "Mean for 95% missing on fold 12: 0.502488506654663\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 13: 0.20751531590583988\n",
      "Mean for 15% missing on fold 13: 0.2589863811666475\n",
      "Mean for 25% missing on fold 13: 0.2834310463715157\n",
      "Mean for 35% missing on fold 13: 0.35115906029575866\n",
      "Mean for 45% missing on fold 13: 0.40638182402916395\n",
      "Mean for 55% missing on fold 13: 0.48280055166867797\n",
      "Mean for 65% missing on fold 13: 0.49374445427914654\n",
      "Mean for 75% missing on fold 13: 0.535827987863803\n",
      "Mean for 85% missing on fold 13: 0.6312607933941452\n",
      "Mean for 95% missing on fold 13: 0.75513035191088\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 14: 0.22880814734885216\n",
      "Mean for 15% missing on fold 14: 0.2692710252294556\n",
      "Mean for 25% missing on fold 14: 0.2939901244443728\n",
      "Mean for 35% missing on fold 14: 0.33629229615622197\n",
      "Mean for 45% missing on fold 14: 0.35680829177525103\n",
      "Mean for 55% missing on fold 14: 0.3804327299799601\n",
      "Mean for 65% missing on fold 14: 0.39731559231588526\n",
      "Mean for 75% missing on fold 14: 0.3940205866603993\n",
      "Mean for 85% missing on fold 14: 0.4412134310277077\n",
      "Mean for 95% missing on fold 14: 0.514327170685506\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 15: 0.19935315043253982\n",
      "Mean for 15% missing on fold 15: 0.242098299479783\n",
      "Mean for 25% missing on fold 15: 0.26429308178879357\n",
      "Mean for 35% missing on fold 15: 0.3039577604462007\n",
      "Mean for 45% missing on fold 15: 0.33660376988468615\n",
      "Mean for 55% missing on fold 15: 0.37189324685346536\n",
      "Mean for 65% missing on fold 15: 0.37878602706625025\n",
      "Mean for 75% missing on fold 15: 0.39262269637799985\n",
      "Mean for 85% missing on fold 15: 0.4491620291525243\n",
      "Mean for 95% missing on fold 15: 0.5274933365108498\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 16: 0.20876707090829383\n",
      "Mean for 15% missing on fold 16: 0.2316303164868503\n",
      "Mean for 25% missing on fold 16: 0.2570646336727912\n",
      "Mean for 35% missing on fold 16: 0.2971633598611824\n",
      "Mean for 45% missing on fold 16: 0.31549873178613413\n",
      "Mean for 55% missing on fold 16: 0.3498470889080681\n",
      "Mean for 65% missing on fold 16: 0.3575690090659469\n",
      "Mean for 75% missing on fold 16: 0.36439646262928105\n",
      "Mean for 85% missing on fold 16: 0.413966894748778\n",
      "Mean for 95% missing on fold 16: 0.47073772354797166\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 17: 0.20943054128951957\n",
      "Mean for 15% missing on fold 17: 0.23716883541087952\n",
      "Mean for 25% missing on fold 17: 0.284564679139239\n",
      "Mean for 35% missing on fold 17: 0.34032361384244253\n",
      "Mean for 45% missing on fold 17: 0.35986410068618707\n",
      "Mean for 55% missing on fold 17: 0.37878435879882344\n",
      "Mean for 65% missing on fold 17: 0.3989499718941797\n",
      "Mean for 75% missing on fold 17: 0.40562170600824754\n",
      "Mean for 85% missing on fold 17: 0.4456472008431988\n",
      "Mean for 95% missing on fold 17: 0.5154164836114802\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 18: 0.22382994571467557\n",
      "Mean for 15% missing on fold 18: 0.26169681149241264\n",
      "Mean for 25% missing on fold 18: 0.28493781438137405\n",
      "Mean for 35% missing on fold 18: 0.3086855249062579\n",
      "Mean for 45% missing on fold 18: 0.3251182873445534\n",
      "Mean for 55% missing on fold 18: 0.3565708097089952\n",
      "Mean for 65% missing on fold 18: 0.3686957858769615\n",
      "Mean for 75% missing on fold 18: 0.3805913342843173\n",
      "Mean for 85% missing on fold 18: 0.4273951744247594\n",
      "Mean for 95% missing on fold 18: 0.4934153183058282\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 19: 0.2036870961656657\n",
      "Mean for 15% missing on fold 19: 0.2502965263971301\n",
      "Mean for 25% missing on fold 19: 0.2567235594448985\n",
      "Mean for 35% missing on fold 19: 0.28083802703799876\n",
      "Mean for 45% missing on fold 19: 0.3032283389142718\n",
      "Mean for 55% missing on fold 19: 0.3300048470523293\n",
      "Mean for 65% missing on fold 19: 0.34418227682531455\n",
      "Mean for 75% missing on fold 19: 0.35148752432467006\n",
      "Mean for 85% missing on fold 19: 0.395816525974185\n",
      "Mean for 95% missing on fold 19: 0.4629150842985118\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 20: 0.20144861182528295\n",
      "Mean for 15% missing on fold 20: 0.24685076111508455\n",
      "Mean for 25% missing on fold 20: 0.259375576495184\n",
      "Mean for 35% missing on fold 20: 0.28012476120605295\n",
      "Mean for 45% missing on fold 20: 0.306171009538145\n",
      "Mean for 55% missing on fold 20: 0.33815560663999306\n",
      "Mean for 65% missing on fold 20: 0.35317549229542666\n",
      "Mean for 75% missing on fold 20: 0.35618194510786977\n",
      "Mean for 85% missing on fold 20: 0.40810243007695274\n",
      "Mean for 95% missing on fold 20: 0.5008204264341312\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 21: 0.2103780298644321\n",
      "Mean for 15% missing on fold 21: 0.2526160946381815\n",
      "Mean for 25% missing on fold 21: 0.27080469908946053\n",
      "Mean for 35% missing on fold 21: 0.29594052941004356\n",
      "Mean for 45% missing on fold 21: 0.32324128144311776\n",
      "Mean for 55% missing on fold 21: 0.35674040306097543\n",
      "Mean for 65% missing on fold 21: 0.37856044492483354\n",
      "Mean for 75% missing on fold 21: 0.3851101319277213\n",
      "Mean for 85% missing on fold 21: 0.43679775271316235\n",
      "Mean for 95% missing on fold 21: 0.5099637505287056\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 22: 0.18903624218114143\n",
      "Mean for 15% missing on fold 22: 0.2310913295522251\n",
      "Mean for 25% missing on fold 22: 0.2536911977934983\n",
      "Mean for 35% missing on fold 22: 0.2956061759647645\n",
      "Mean for 45% missing on fold 22: 0.32948216454466944\n",
      "Mean for 55% missing on fold 22: 0.3691742485293958\n",
      "Mean for 65% missing on fold 22: 0.3853812726487252\n",
      "Mean for 75% missing on fold 22: 0.3944084249365257\n",
      "Mean for 85% missing on fold 22: 0.44799977947906733\n",
      "Mean for 95% missing on fold 22: 0.5100610737409643\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 23: 0.197374033402879\n",
      "Mean for 15% missing on fold 23: 0.23942058000003896\n",
      "Mean for 25% missing on fold 23: 0.2610050852533736\n",
      "Mean for 35% missing on fold 23: 0.2968719810119592\n",
      "Mean for 45% missing on fold 23: 0.32699476436875236\n",
      "Mean for 55% missing on fold 23: 0.35679966109148964\n",
      "Mean for 65% missing on fold 23: 0.35705895375383306\n",
      "Mean for 75% missing on fold 23: 0.3705943329523082\n",
      "Mean for 85% missing on fold 23: 0.4028391200234772\n",
      "Mean for 95% missing on fold 23: 0.4874238556333432\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 24: 0.19024476486581599\n",
      "Mean for 15% missing on fold 24: 0.2282010981413003\n",
      "Mean for 25% missing on fold 24: 0.23987261758972372\n",
      "Mean for 35% missing on fold 24: 0.2640532153303394\n",
      "Mean for 45% missing on fold 24: 0.2818424011401267\n",
      "Mean for 55% missing on fold 24: 0.31411268751289606\n",
      "Mean for 65% missing on fold 24: 0.32342908163459394\n",
      "Mean for 75% missing on fold 24: 0.33797587802124057\n",
      "Mean for 85% missing on fold 24: 0.3966556838033736\n",
      "Mean for 95% missing on fold 24: 0.48745311682512965\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 25: 0.2049379973583395\n",
      "Mean for 15% missing on fold 25: 0.25022042334155686\n",
      "Mean for 25% missing on fold 25: 0.2729372406574001\n",
      "Mean for 35% missing on fold 25: 0.3105565159721394\n",
      "Mean for 45% missing on fold 25: 0.3356488061960458\n",
      "Mean for 55% missing on fold 25: 0.3662199409655529\n",
      "Mean for 65% missing on fold 25: 0.3806066169771865\n",
      "Mean for 75% missing on fold 25: 0.39443744003512216\n",
      "Mean for 85% missing on fold 25: 0.4487722645235473\n",
      "Mean for 95% missing on fold 25: 0.5268123103858061\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 26: 0.18768953891701864\n",
      "Mean for 15% missing on fold 26: 0.23205342322337824\n",
      "Mean for 25% missing on fold 26: 0.24710733662471066\n",
      "Mean for 35% missing on fold 26: 0.27213311202726126\n",
      "Mean for 45% missing on fold 26: 0.29846649885924376\n",
      "Mean for 55% missing on fold 26: 0.32192438434505716\n",
      "Mean for 65% missing on fold 26: 0.33261472784157037\n",
      "Mean for 75% missing on fold 26: 0.34423900953111175\n",
      "Mean for 85% missing on fold 26: 0.3800143518773397\n",
      "Mean for 95% missing on fold 26: 0.45104840678637403\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 27: 0.2176434710449522\n",
      "Mean for 15% missing on fold 27: 0.28077047161323726\n",
      "Mean for 25% missing on fold 27: 0.31600164724671204\n",
      "Mean for 35% missing on fold 27: 0.36776023073855607\n",
      "Mean for 45% missing on fold 27: 0.37908888737970714\n",
      "Mean for 55% missing on fold 27: 0.3966074327628969\n",
      "Mean for 65% missing on fold 27: 0.4126621263603392\n",
      "Mean for 75% missing on fold 27: 0.4292354802560513\n",
      "Mean for 85% missing on fold 27: 0.4687357487577022\n",
      "Mean for 95% missing on fold 27: 0.5350544189771755\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 28: 0.18961317557818508\n",
      "Mean for 15% missing on fold 28: 0.23706213021618003\n",
      "Mean for 25% missing on fold 28: 0.26485571773219657\n",
      "Mean for 35% missing on fold 28: 0.3239079475063649\n",
      "Mean for 45% missing on fold 28: 0.3711014506640867\n",
      "Mean for 55% missing on fold 28: 0.41023866038397566\n",
      "Mean for 65% missing on fold 28: 0.42920321919535187\n",
      "Mean for 75% missing on fold 28: 0.45152278288428616\n",
      "Mean for 85% missing on fold 28: 0.523304574589944\n",
      "Mean for 95% missing on fold 28: 0.6204260118934888\n",
      "--------------------------------------------------------------------------------\n",
      "Mean for 5% missing on fold 29: 0.20789365401429413\n",
      "Mean for 15% missing on fold 29: 0.24724313845239188\n",
      "Mean for 25% missing on fold 29: 0.2672553696678348\n",
      "Mean for 35% missing on fold 29: 0.30254467950605923\n",
      "Mean for 45% missing on fold 29: 0.3222449619099497\n",
      "Mean for 55% missing on fold 29: 0.3435514935721667\n",
      "Mean for 65% missing on fold 29: 0.3665703936110887\n",
      "Mean for 75% missing on fold 29: 0.37170796722171917\n",
      "Mean for 85% missing on fold 29: 0.4157417793765979\n",
      "Mean for 95% missing on fold 29: 0.5095817904277343\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for fidx in fold_scores_csdi:\n",
    "    for pm in fold_scores_csdi[fidx]:\n",
    "        window_scores = [np.mean(fold_scores_csdi[fidx][pm][i]) for i in range(0, len(fold_scores_csdi[fidx][pm]))] # average over all test instances for given window location\n",
    "        all_windows_mean = np.mean(window_scores) # mean across all windows with the same length, but different locations\n",
    "        print(f\"Mean for {pm}% missing on fold {fidx}: {all_windows_mean}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_folds_brits(Xs, ys, fold_idxs, window_idxs, model):\n",
    "    fold_scores = dict()\n",
    "    for fold in range(0, len(fold_idxs)):\n",
    "        print(f\"Evaluating fold {fold}/{len(fold_idxs)-1}...\")\n",
    "        # make the splits\n",
    "        X_train_fold = Xs[fold_idxs[fold][\"train\"]]\n",
    "        y_train_fold = ys[fold_idxs[fold][\"train\"]]\n",
    "        X_test_fold = Xs[fold_idxs[fold][\"test\"]]\n",
    "        y_test_fold = ys[fold_idxs[fold][\"test\"]]\n",
    "        # check class distributions\n",
    "        counts_tr = np.unique(y_train_fold, return_counts=True)[1]\n",
    "        print(f\"Training class distribution: {counts_tr/np.sum(counts_tr)}\")\n",
    "        counts_te = np.unique(y_test_fold, return_counts=True)[1]\n",
    "        print(f\"Testing class distribution: {counts_te/np.sum(counts_te)}\")\n",
    "        print(f\"Training BRITS on fold {fold}...\")\n",
    "        model.fit(train_set={'X':X_train_fold})\n",
    "        print(\"Finished training!\")\n",
    "        # loop over % missing\n",
    "        percent_missing_score = dict()\n",
    "        for pm in window_idxs:\n",
    "            print(f\"Imputing {pm}% missing data over {len(window_idxs[pm])} windows...\")\n",
    "            per_window_scores = dict()\n",
    "            for (idx, widx) in enumerate(window_idxs[pm]):\n",
    "                X_test_corrupted = X_test_fold.copy()\n",
    "                X_test_corrupted[:, widx] = np.nan\n",
    "                mask = np.isnan(X_test_corrupted) # mask ensures only misisng values are imputed\n",
    "                brits_imputed = model.impute(test_set={'X': X_test_corrupted})\n",
    "                errs = [calc_mae(brits_imputed[i], X_test_fold[i], mask[i]) for i in range(0, X_test_fold.shape[0])] # get individual errors for uncertainty quantification\n",
    "                per_window_scores[idx] = errs\n",
    "            percent_missing_score[pm] = per_window_scores\n",
    "        fold_scores[fold] = percent_missing_score\n",
    "    return fold_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 23:02:05 [INFO]: No given device, using default device: cuda\n",
      "2024-10-20 23:02:05 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-10-20 23:02:05 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 135,952\n"
     ]
    }
   ],
   "source": [
    "brits = BRITS(\n",
    "    n_steps=len(X_test_original[0]),\n",
    "    n_features=1,\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    num_workers=0,\n",
    "    device=None, # infer the best device to use\n",
    "    model_saving_strategy=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 0/29...\n",
      "Training class distribution: [0.31 0.69]\n",
      "Testing class distribution: [0.36 0.64]\n",
      "Training BRITS on fold 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 23:02:09 [INFO]: Epoch 001 - training loss: 1.6030\n",
      "2024-10-20 23:02:11 [INFO]: Epoch 002 - training loss: 1.5346\n",
      "2024-10-20 23:02:14 [INFO]: Epoch 003 - training loss: 1.4898\n",
      "2024-10-20 23:02:17 [INFO]: Epoch 004 - training loss: 1.4407\n",
      "2024-10-20 23:02:20 [INFO]: Epoch 005 - training loss: 1.3606\n",
      "2024-10-20 23:02:22 [INFO]: Epoch 006 - training loss: 1.3076\n",
      "2024-10-20 23:02:25 [INFO]: Epoch 007 - training loss: 1.2290\n",
      "2024-10-20 23:02:28 [INFO]: Epoch 008 - training loss: 1.1774\n",
      "2024-10-20 23:02:31 [INFO]: Epoch 009 - training loss: 1.1428\n",
      "2024-10-20 23:02:34 [INFO]: Epoch 010 - training loss: 1.1194\n",
      "2024-10-20 23:02:37 [INFO]: Epoch 011 - training loss: 1.1012\n",
      "2024-10-20 23:02:39 [INFO]: Epoch 012 - training loss: 1.0915\n",
      "2024-10-20 23:02:42 [INFO]: Epoch 013 - training loss: 1.0752\n",
      "2024-10-20 23:02:45 [INFO]: Epoch 014 - training loss: 1.0733\n",
      "2024-10-20 23:02:48 [INFO]: Epoch 015 - training loss: 1.0494\n",
      "2024-10-20 23:02:51 [INFO]: Epoch 016 - training loss: 1.0425\n",
      "2024-10-20 23:02:54 [INFO]: Epoch 017 - training loss: 1.0348\n",
      "2024-10-20 23:02:57 [INFO]: Epoch 018 - training loss: 1.0330\n",
      "2024-10-20 23:02:59 [INFO]: Epoch 019 - training loss: 1.0242\n",
      "2024-10-20 23:03:02 [INFO]: Epoch 020 - training loss: 1.0274\n",
      "2024-10-20 23:03:05 [INFO]: Epoch 021 - training loss: 1.0057\n",
      "2024-10-20 23:03:08 [INFO]: Epoch 022 - training loss: 1.0123\n",
      "2024-10-20 23:03:11 [INFO]: Epoch 023 - training loss: 1.0050\n",
      "2024-10-20 23:03:14 [INFO]: Epoch 024 - training loss: 0.9952\n",
      "2024-10-20 23:03:17 [INFO]: Epoch 025 - training loss: 0.9934\n",
      "2024-10-20 23:03:19 [INFO]: Epoch 026 - training loss: 0.9877\n",
      "2024-10-20 23:03:22 [INFO]: Epoch 027 - training loss: 0.9853\n",
      "2024-10-20 23:03:25 [INFO]: Epoch 028 - training loss: 0.9731\n",
      "2024-10-20 23:03:28 [INFO]: Epoch 029 - training loss: 0.9678\n",
      "2024-10-20 23:03:31 [INFO]: Epoch 030 - training loss: 0.9706\n",
      "2024-10-20 23:03:34 [INFO]: Epoch 031 - training loss: 0.9613\n",
      "2024-10-20 23:03:36 [INFO]: Epoch 032 - training loss: 0.9658\n",
      "2024-10-20 23:03:39 [INFO]: Epoch 033 - training loss: 0.9639\n",
      "2024-10-20 23:03:42 [INFO]: Epoch 034 - training loss: 0.9631\n",
      "2024-10-20 23:03:45 [INFO]: Epoch 035 - training loss: 0.9551\n",
      "2024-10-20 23:03:47 [INFO]: Epoch 036 - training loss: 0.9439\n",
      "2024-10-20 23:03:50 [INFO]: Epoch 037 - training loss: 0.9438\n",
      "2024-10-20 23:03:53 [INFO]: Epoch 038 - training loss: 0.9547\n",
      "2024-10-20 23:03:56 [INFO]: Epoch 039 - training loss: 0.9386\n",
      "2024-10-20 23:03:59 [INFO]: Epoch 040 - training loss: 0.9394\n",
      "2024-10-20 23:04:01 [INFO]: Epoch 041 - training loss: 0.9259\n",
      "2024-10-20 23:04:04 [INFO]: Epoch 042 - training loss: 0.9211\n",
      "2024-10-20 23:04:07 [INFO]: Epoch 043 - training loss: 0.9286\n",
      "2024-10-20 23:04:10 [INFO]: Epoch 044 - training loss: 0.9264\n",
      "2024-10-20 23:04:13 [INFO]: Epoch 045 - training loss: 0.9151\n",
      "2024-10-20 23:04:15 [INFO]: Epoch 046 - training loss: 0.9107\n",
      "2024-10-20 23:04:18 [INFO]: Epoch 047 - training loss: 0.9240\n",
      "2024-10-20 23:04:21 [INFO]: Epoch 048 - training loss: 0.9120\n",
      "2024-10-20 23:04:24 [INFO]: Epoch 049 - training loss: 0.9131\n",
      "2024-10-20 23:04:26 [INFO]: Epoch 050 - training loss: 0.9107\n",
      "2024-10-20 23:04:29 [INFO]: Epoch 051 - training loss: 0.9045\n",
      "2024-10-20 23:04:32 [INFO]: Epoch 052 - training loss: 0.9141\n",
      "2024-10-20 23:04:35 [INFO]: Epoch 053 - training loss: 0.9023\n",
      "2024-10-20 23:04:38 [INFO]: Epoch 054 - training loss: 0.8950\n",
      "2024-10-20 23:04:41 [INFO]: Epoch 055 - training loss: 0.8909\n",
      "2024-10-20 23:04:43 [INFO]: Epoch 056 - training loss: 0.8943\n",
      "2024-10-20 23:04:46 [INFO]: Epoch 057 - training loss: 0.8846\n",
      "2024-10-20 23:04:49 [INFO]: Epoch 058 - training loss: 0.8900\n",
      "2024-10-20 23:04:52 [INFO]: Epoch 059 - training loss: 0.8843\n",
      "2024-10-20 23:04:55 [INFO]: Epoch 060 - training loss: 0.8795\n",
      "2024-10-20 23:04:57 [INFO]: Epoch 061 - training loss: 0.8653\n",
      "2024-10-20 23:05:00 [INFO]: Epoch 062 - training loss: 0.8755\n",
      "2024-10-20 23:05:03 [INFO]: Epoch 063 - training loss: 0.8654\n",
      "2024-10-20 23:05:06 [INFO]: Epoch 064 - training loss: 0.8664\n",
      "2024-10-20 23:05:09 [INFO]: Epoch 065 - training loss: 0.8684\n",
      "2024-10-20 23:05:11 [INFO]: Epoch 066 - training loss: 0.8556\n",
      "2024-10-20 23:05:14 [INFO]: Epoch 067 - training loss: 0.8532\n",
      "2024-10-20 23:05:17 [INFO]: Epoch 068 - training loss: 0.8555\n",
      "2024-10-20 23:05:20 [INFO]: Epoch 069 - training loss: 0.8617\n",
      "2024-10-20 23:05:23 [INFO]: Epoch 070 - training loss: 0.8549\n",
      "2024-10-20 23:05:26 [INFO]: Epoch 071 - training loss: 0.8413\n",
      "2024-10-20 23:05:28 [INFO]: Epoch 072 - training loss: 0.8588\n",
      "2024-10-20 23:05:31 [INFO]: Epoch 073 - training loss: 0.8424\n",
      "2024-10-20 23:05:34 [INFO]: Epoch 074 - training loss: 0.8391\n",
      "2024-10-20 23:05:37 [INFO]: Epoch 075 - training loss: 0.8425\n",
      "2024-10-20 23:05:40 [INFO]: Epoch 076 - training loss: 0.8484\n",
      "2024-10-20 23:05:42 [INFO]: Epoch 077 - training loss: 0.8682\n",
      "2024-10-20 23:05:45 [INFO]: Epoch 078 - training loss: 0.8289\n",
      "2024-10-20 23:05:48 [INFO]: Epoch 079 - training loss: 0.8316\n",
      "2024-10-20 23:05:51 [INFO]: Epoch 080 - training loss: 0.8278\n",
      "2024-10-20 23:05:54 [INFO]: Epoch 081 - training loss: 0.8251\n",
      "2024-10-20 23:05:57 [INFO]: Epoch 082 - training loss: 0.8274\n",
      "2024-10-20 23:05:59 [INFO]: Epoch 083 - training loss: 0.8258\n",
      "2024-10-20 23:06:02 [INFO]: Epoch 084 - training loss: 0.8194\n",
      "2024-10-20 23:06:05 [INFO]: Epoch 085 - training loss: 0.8176\n",
      "2024-10-20 23:06:08 [INFO]: Epoch 086 - training loss: 0.8132\n",
      "2024-10-20 23:06:11 [INFO]: Epoch 087 - training loss: 0.8286\n",
      "2024-10-20 23:06:14 [INFO]: Epoch 088 - training loss: 0.8205\n",
      "2024-10-20 23:06:16 [INFO]: Epoch 089 - training loss: 0.8123\n",
      "2024-10-20 23:06:19 [INFO]: Epoch 090 - training loss: 0.8153\n",
      "2024-10-20 23:06:22 [INFO]: Epoch 091 - training loss: 0.8120\n",
      "2024-10-20 23:06:25 [INFO]: Epoch 092 - training loss: 0.7998\n",
      "2024-10-20 23:06:28 [INFO]: Epoch 093 - training loss: 0.8084\n",
      "2024-10-20 23:06:31 [INFO]: Epoch 094 - training loss: 0.8024\n",
      "2024-10-20 23:06:33 [INFO]: Epoch 095 - training loss: 0.8092\n",
      "2024-10-20 23:06:36 [INFO]: Epoch 096 - training loss: 0.8073\n",
      "2024-10-20 23:06:39 [INFO]: Epoch 097 - training loss: 0.7986\n",
      "2024-10-20 23:06:42 [INFO]: Epoch 098 - training loss: 0.8076\n",
      "2024-10-20 23:06:45 [INFO]: Epoch 099 - training loss: 0.8104\n",
      "2024-10-20 23:06:47 [INFO]: Epoch 100 - training loss: 0.8001\n",
      "2024-10-20 23:06:47 [INFO]: Finished training. The best model is from epoch#97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 1/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 23:12:05 [INFO]: Epoch 001 - training loss: 0.7818\n",
      "2024-10-20 23:12:08 [INFO]: Epoch 002 - training loss: 0.7858\n",
      "2024-10-20 23:12:11 [INFO]: Epoch 003 - training loss: 0.7884\n",
      "2024-10-20 23:12:14 [INFO]: Epoch 004 - training loss: 0.7936\n",
      "2024-10-20 23:12:16 [INFO]: Epoch 005 - training loss: 0.7828\n",
      "2024-10-20 23:12:19 [INFO]: Epoch 006 - training loss: 0.7763\n",
      "2024-10-20 23:12:22 [INFO]: Epoch 007 - training loss: 0.7837\n",
      "2024-10-20 23:12:25 [INFO]: Epoch 008 - training loss: 0.7857\n",
      "2024-10-20 23:12:28 [INFO]: Epoch 009 - training loss: 0.7852\n",
      "2024-10-20 23:12:31 [INFO]: Epoch 010 - training loss: 0.7940\n",
      "2024-10-20 23:12:33 [INFO]: Epoch 011 - training loss: 0.7727\n",
      "2024-10-20 23:12:36 [INFO]: Epoch 012 - training loss: 0.7852\n",
      "2024-10-20 23:12:39 [INFO]: Epoch 013 - training loss: 0.7757\n",
      "2024-10-20 23:12:42 [INFO]: Epoch 014 - training loss: 0.7782\n",
      "2024-10-20 23:12:45 [INFO]: Epoch 015 - training loss: 0.7641\n",
      "2024-10-20 23:12:48 [INFO]: Epoch 016 - training loss: 0.7689\n",
      "2024-10-20 23:12:50 [INFO]: Epoch 017 - training loss: 0.7636\n",
      "2024-10-20 23:12:53 [INFO]: Epoch 018 - training loss: 0.7723\n",
      "2024-10-20 23:12:56 [INFO]: Epoch 019 - training loss: 0.7785\n",
      "2024-10-20 23:12:59 [INFO]: Epoch 020 - training loss: 0.7622\n",
      "2024-10-20 23:13:02 [INFO]: Epoch 021 - training loss: 0.7570\n",
      "2024-10-20 23:13:04 [INFO]: Epoch 022 - training loss: 0.7736\n",
      "2024-10-20 23:13:07 [INFO]: Epoch 023 - training loss: 0.7505\n",
      "2024-10-20 23:13:10 [INFO]: Epoch 024 - training loss: 0.7658\n",
      "2024-10-20 23:13:13 [INFO]: Epoch 025 - training loss: 0.7577\n",
      "2024-10-20 23:13:16 [INFO]: Epoch 026 - training loss: 0.7513\n",
      "2024-10-20 23:13:19 [INFO]: Epoch 027 - training loss: 0.7621\n",
      "2024-10-20 23:13:21 [INFO]: Epoch 028 - training loss: 0.7603\n",
      "2024-10-20 23:13:24 [INFO]: Epoch 029 - training loss: 0.7579\n",
      "2024-10-20 23:13:27 [INFO]: Epoch 030 - training loss: 0.7552\n",
      "2024-10-20 23:13:30 [INFO]: Epoch 031 - training loss: 0.7533\n",
      "2024-10-20 23:13:33 [INFO]: Epoch 032 - training loss: 0.7548\n",
      "2024-10-20 23:13:36 [INFO]: Epoch 033 - training loss: 0.7453\n",
      "2024-10-20 23:13:38 [INFO]: Epoch 034 - training loss: 0.7444\n",
      "2024-10-20 23:13:41 [INFO]: Epoch 035 - training loss: 0.7467\n",
      "2024-10-20 23:13:44 [INFO]: Epoch 036 - training loss: 0.7444\n",
      "2024-10-20 23:13:47 [INFO]: Epoch 037 - training loss: 0.7503\n",
      "2024-10-20 23:13:50 [INFO]: Epoch 038 - training loss: 0.7391\n",
      "2024-10-20 23:13:53 [INFO]: Epoch 039 - training loss: 0.7507\n",
      "2024-10-20 23:13:55 [INFO]: Epoch 040 - training loss: 0.7400\n",
      "2024-10-20 23:13:58 [INFO]: Epoch 041 - training loss: 0.7456\n",
      "2024-10-20 23:14:01 [INFO]: Epoch 042 - training loss: 0.7348\n",
      "2024-10-20 23:14:04 [INFO]: Epoch 043 - training loss: 0.7505\n",
      "2024-10-20 23:14:07 [INFO]: Epoch 044 - training loss: 0.7339\n",
      "2024-10-20 23:14:09 [INFO]: Epoch 045 - training loss: 0.7490\n",
      "2024-10-20 23:14:12 [INFO]: Epoch 046 - training loss: 0.7493\n",
      "2024-10-20 23:14:15 [INFO]: Epoch 047 - training loss: 0.7377\n",
      "2024-10-20 23:14:18 [INFO]: Epoch 048 - training loss: 0.7480\n",
      "2024-10-20 23:14:21 [INFO]: Epoch 049 - training loss: 0.7312\n",
      "2024-10-20 23:14:23 [INFO]: Epoch 050 - training loss: 0.7440\n",
      "2024-10-20 23:14:26 [INFO]: Epoch 051 - training loss: 0.7381\n",
      "2024-10-20 23:14:29 [INFO]: Epoch 052 - training loss: 0.7530\n",
      "2024-10-20 23:14:32 [INFO]: Epoch 053 - training loss: 0.7456\n",
      "2024-10-20 23:14:35 [INFO]: Epoch 054 - training loss: 0.7360\n",
      "2024-10-20 23:14:37 [INFO]: Epoch 055 - training loss: 0.7353\n",
      "2024-10-20 23:14:40 [INFO]: Epoch 056 - training loss: 0.7377\n",
      "2024-10-20 23:14:43 [INFO]: Epoch 057 - training loss: 0.7373\n",
      "2024-10-20 23:14:46 [INFO]: Epoch 058 - training loss: 0.7355\n",
      "2024-10-20 23:14:49 [INFO]: Epoch 059 - training loss: 0.7293\n",
      "2024-10-20 23:14:52 [INFO]: Epoch 060 - training loss: 0.7293\n",
      "2024-10-20 23:14:54 [INFO]: Epoch 061 - training loss: 0.7183\n",
      "2024-10-20 23:14:57 [INFO]: Epoch 062 - training loss: 0.7289\n",
      "2024-10-20 23:15:00 [INFO]: Epoch 063 - training loss: 0.7215\n",
      "2024-10-20 23:15:03 [INFO]: Epoch 064 - training loss: 0.7289\n",
      "2024-10-20 23:15:06 [INFO]: Epoch 065 - training loss: 0.7310\n",
      "2024-10-20 23:15:08 [INFO]: Epoch 066 - training loss: 0.7282\n",
      "2024-10-20 23:15:11 [INFO]: Epoch 067 - training loss: 0.7273\n",
      "2024-10-20 23:15:14 [INFO]: Epoch 068 - training loss: 0.7272\n",
      "2024-10-20 23:15:17 [INFO]: Epoch 069 - training loss: 0.7278\n",
      "2024-10-20 23:15:20 [INFO]: Epoch 070 - training loss: 0.7265\n",
      "2024-10-20 23:15:23 [INFO]: Epoch 071 - training loss: 0.7212\n",
      "2024-10-20 23:15:25 [INFO]: Epoch 072 - training loss: 0.7263\n",
      "2024-10-20 23:15:28 [INFO]: Epoch 073 - training loss: 0.7235\n",
      "2024-10-20 23:15:31 [INFO]: Epoch 074 - training loss: 0.7261\n",
      "2024-10-20 23:15:34 [INFO]: Epoch 075 - training loss: 0.7251\n",
      "2024-10-20 23:15:37 [INFO]: Epoch 076 - training loss: 0.7326\n",
      "2024-10-20 23:15:40 [INFO]: Epoch 077 - training loss: 0.7210\n",
      "2024-10-20 23:15:42 [INFO]: Epoch 078 - training loss: 0.7199\n",
      "2024-10-20 23:15:45 [INFO]: Epoch 079 - training loss: 0.7277\n",
      "2024-10-20 23:15:48 [INFO]: Epoch 080 - training loss: 0.7286\n",
      "2024-10-20 23:15:51 [INFO]: Epoch 081 - training loss: 0.7182\n",
      "2024-10-20 23:15:54 [INFO]: Epoch 082 - training loss: 0.7222\n",
      "2024-10-20 23:15:56 [INFO]: Epoch 083 - training loss: 0.7259\n",
      "2024-10-20 23:15:59 [INFO]: Epoch 084 - training loss: 0.7151\n",
      "2024-10-20 23:16:02 [INFO]: Epoch 085 - training loss: 0.7386\n",
      "2024-10-20 23:16:05 [INFO]: Epoch 086 - training loss: 0.7200\n",
      "2024-10-20 23:16:08 [INFO]: Epoch 087 - training loss: 0.7166\n",
      "2024-10-20 23:16:11 [INFO]: Epoch 088 - training loss: 0.7194\n",
      "2024-10-20 23:16:13 [INFO]: Epoch 089 - training loss: 0.7191\n",
      "2024-10-20 23:16:16 [INFO]: Epoch 090 - training loss: 0.7100\n",
      "2024-10-20 23:16:19 [INFO]: Epoch 091 - training loss: 0.7124\n",
      "2024-10-20 23:16:22 [INFO]: Epoch 092 - training loss: 0.7252\n",
      "2024-10-20 23:16:25 [INFO]: Epoch 093 - training loss: 0.7172\n",
      "2024-10-20 23:16:27 [INFO]: Epoch 094 - training loss: 0.7146\n",
      "2024-10-20 23:16:30 [INFO]: Epoch 095 - training loss: 0.7096\n",
      "2024-10-20 23:16:33 [INFO]: Epoch 096 - training loss: 0.7230\n",
      "2024-10-20 23:16:36 [INFO]: Epoch 097 - training loss: 0.7093\n",
      "2024-10-20 23:16:39 [INFO]: Epoch 098 - training loss: 0.7222\n",
      "2024-10-20 23:16:41 [INFO]: Epoch 099 - training loss: 0.7073\n",
      "2024-10-20 23:16:44 [INFO]: Epoch 100 - training loss: 0.7186\n",
      "2024-10-20 23:16:44 [INFO]: Finished training. The best model is from epoch#99.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 2/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 23:22:03 [INFO]: Epoch 001 - training loss: 0.7269\n",
      "2024-10-20 23:22:05 [INFO]: Epoch 002 - training loss: 0.7238\n",
      "2024-10-20 23:22:08 [INFO]: Epoch 003 - training loss: 0.7310\n",
      "2024-10-20 23:22:11 [INFO]: Epoch 004 - training loss: 0.7475\n",
      "2024-10-20 23:22:14 [INFO]: Epoch 005 - training loss: 0.7246\n",
      "2024-10-20 23:22:17 [INFO]: Epoch 006 - training loss: 0.7344\n",
      "2024-10-20 23:22:20 [INFO]: Epoch 007 - training loss: 0.7281\n",
      "2024-10-20 23:22:22 [INFO]: Epoch 008 - training loss: 0.7186\n",
      "2024-10-20 23:22:25 [INFO]: Epoch 009 - training loss: 0.7202\n",
      "2024-10-20 23:22:28 [INFO]: Epoch 010 - training loss: 0.7255\n",
      "2024-10-20 23:22:31 [INFO]: Epoch 011 - training loss: 0.7254\n",
      "2024-10-20 23:22:34 [INFO]: Epoch 012 - training loss: 0.7132\n",
      "2024-10-20 23:22:36 [INFO]: Epoch 013 - training loss: 0.7162\n",
      "2024-10-20 23:22:39 [INFO]: Epoch 014 - training loss: 0.7192\n",
      "2024-10-20 23:22:42 [INFO]: Epoch 015 - training loss: 0.7223\n",
      "2024-10-20 23:22:45 [INFO]: Epoch 016 - training loss: 0.7274\n",
      "2024-10-20 23:22:48 [INFO]: Epoch 017 - training loss: 0.7176\n",
      "2024-10-20 23:22:51 [INFO]: Epoch 018 - training loss: 0.7243\n",
      "2024-10-20 23:22:53 [INFO]: Epoch 019 - training loss: 0.7214\n",
      "2024-10-20 23:22:56 [INFO]: Epoch 020 - training loss: 0.7242\n",
      "2024-10-20 23:22:59 [INFO]: Epoch 021 - training loss: 0.7112\n",
      "2024-10-20 23:23:02 [INFO]: Epoch 022 - training loss: 0.7166\n",
      "2024-10-20 23:23:05 [INFO]: Epoch 023 - training loss: 0.7228\n",
      "2024-10-20 23:23:07 [INFO]: Epoch 024 - training loss: 0.7081\n",
      "2024-10-20 23:23:10 [INFO]: Epoch 025 - training loss: 0.7170\n",
      "2024-10-20 23:23:13 [INFO]: Epoch 026 - training loss: 0.7175\n",
      "2024-10-20 23:23:16 [INFO]: Epoch 027 - training loss: 0.7287\n",
      "2024-10-20 23:23:19 [INFO]: Epoch 028 - training loss: 0.7117\n",
      "2024-10-20 23:23:22 [INFO]: Epoch 029 - training loss: 0.7229\n",
      "2024-10-20 23:23:24 [INFO]: Epoch 030 - training loss: 0.7241\n",
      "2024-10-20 23:23:27 [INFO]: Epoch 031 - training loss: 0.7255\n",
      "2024-10-20 23:23:30 [INFO]: Epoch 032 - training loss: 0.7258\n",
      "2024-10-20 23:23:33 [INFO]: Epoch 033 - training loss: 0.7185\n",
      "2024-10-20 23:23:36 [INFO]: Epoch 034 - training loss: 0.7217\n",
      "2024-10-20 23:23:38 [INFO]: Epoch 035 - training loss: 0.7310\n",
      "2024-10-20 23:23:41 [INFO]: Epoch 036 - training loss: 0.7285\n",
      "2024-10-20 23:23:44 [INFO]: Epoch 037 - training loss: 0.7192\n",
      "2024-10-20 23:23:47 [INFO]: Epoch 038 - training loss: 0.7084\n",
      "2024-10-20 23:23:50 [INFO]: Epoch 039 - training loss: 0.7168\n",
      "2024-10-20 23:23:53 [INFO]: Epoch 040 - training loss: 0.7135\n",
      "2024-10-20 23:23:55 [INFO]: Epoch 041 - training loss: 0.7113\n",
      "2024-10-20 23:23:58 [INFO]: Epoch 042 - training loss: 0.7116\n",
      "2024-10-20 23:24:01 [INFO]: Epoch 043 - training loss: 0.7148\n",
      "2024-10-20 23:24:04 [INFO]: Epoch 044 - training loss: 0.7078\n",
      "2024-10-20 23:24:07 [INFO]: Epoch 045 - training loss: 0.7136\n",
      "2024-10-20 23:24:10 [INFO]: Epoch 046 - training loss: 0.7066\n",
      "2024-10-20 23:24:12 [INFO]: Epoch 047 - training loss: 0.7155\n",
      "2024-10-20 23:24:15 [INFO]: Epoch 048 - training loss: 0.7004\n",
      "2024-10-20 23:24:18 [INFO]: Epoch 049 - training loss: 0.7195\n",
      "2024-10-20 23:24:21 [INFO]: Epoch 050 - training loss: 0.7140\n",
      "2024-10-20 23:24:24 [INFO]: Epoch 051 - training loss: 0.7305\n",
      "2024-10-20 23:24:26 [INFO]: Epoch 052 - training loss: 0.7070\n",
      "2024-10-20 23:24:29 [INFO]: Epoch 053 - training loss: 0.7254\n",
      "2024-10-20 23:24:32 [INFO]: Epoch 054 - training loss: 0.7032\n",
      "2024-10-20 23:24:35 [INFO]: Epoch 055 - training loss: 0.7100\n",
      "2024-10-20 23:24:38 [INFO]: Epoch 056 - training loss: 0.7069\n",
      "2024-10-20 23:24:41 [INFO]: Epoch 057 - training loss: 0.7206\n",
      "2024-10-20 23:24:43 [INFO]: Epoch 058 - training loss: 0.7157\n",
      "2024-10-20 23:24:46 [INFO]: Epoch 059 - training loss: 0.7007\n",
      "2024-10-20 23:24:49 [INFO]: Epoch 060 - training loss: 0.7148\n",
      "2024-10-20 23:24:52 [INFO]: Epoch 061 - training loss: 0.7022\n",
      "2024-10-20 23:24:55 [INFO]: Epoch 062 - training loss: 0.7012\n",
      "2024-10-20 23:24:57 [INFO]: Epoch 063 - training loss: 0.7084\n",
      "2024-10-20 23:25:00 [INFO]: Epoch 064 - training loss: 0.7153\n",
      "2024-10-20 23:25:03 [INFO]: Epoch 065 - training loss: 0.7084\n",
      "2024-10-20 23:25:06 [INFO]: Epoch 066 - training loss: 0.7099\n",
      "2024-10-20 23:25:09 [INFO]: Epoch 067 - training loss: 0.7070\n",
      "2024-10-20 23:25:11 [INFO]: Epoch 068 - training loss: 0.7082\n",
      "2024-10-20 23:25:14 [INFO]: Epoch 069 - training loss: 0.6995\n",
      "2024-10-20 23:25:17 [INFO]: Epoch 070 - training loss: 0.7075\n",
      "2024-10-20 23:25:20 [INFO]: Epoch 071 - training loss: 0.7084\n",
      "2024-10-20 23:25:23 [INFO]: Epoch 072 - training loss: 0.7027\n",
      "2024-10-20 23:25:26 [INFO]: Epoch 073 - training loss: 0.6968\n",
      "2024-10-20 23:25:28 [INFO]: Epoch 074 - training loss: 0.7107\n",
      "2024-10-20 23:25:31 [INFO]: Epoch 075 - training loss: 0.7039\n",
      "2024-10-20 23:25:34 [INFO]: Epoch 076 - training loss: 0.7089\n",
      "2024-10-20 23:25:37 [INFO]: Epoch 077 - training loss: 0.7021\n",
      "2024-10-20 23:25:40 [INFO]: Epoch 078 - training loss: 0.7072\n",
      "2024-10-20 23:25:42 [INFO]: Epoch 079 - training loss: 0.6989\n",
      "2024-10-20 23:25:45 [INFO]: Epoch 080 - training loss: 0.7193\n",
      "2024-10-20 23:25:48 [INFO]: Epoch 081 - training loss: 0.7039\n",
      "2024-10-20 23:25:51 [INFO]: Epoch 082 - training loss: 0.6906\n",
      "2024-10-20 23:25:54 [INFO]: Epoch 083 - training loss: 0.6936\n",
      "2024-10-20 23:25:56 [INFO]: Epoch 084 - training loss: 0.6967\n",
      "2024-10-20 23:25:59 [INFO]: Epoch 085 - training loss: 0.7058\n",
      "2024-10-20 23:26:02 [INFO]: Epoch 086 - training loss: 0.7094\n",
      "2024-10-20 23:26:05 [INFO]: Epoch 087 - training loss: 0.7000\n",
      "2024-10-20 23:26:08 [INFO]: Epoch 088 - training loss: 0.6979\n",
      "2024-10-20 23:26:10 [INFO]: Epoch 089 - training loss: 0.6883\n",
      "2024-10-20 23:26:13 [INFO]: Epoch 090 - training loss: 0.7093\n",
      "2024-10-20 23:26:16 [INFO]: Epoch 091 - training loss: 0.7036\n",
      "2024-10-20 23:26:19 [INFO]: Epoch 092 - training loss: 0.6991\n",
      "2024-10-20 23:26:22 [INFO]: Epoch 093 - training loss: 0.7127\n",
      "2024-10-20 23:26:24 [INFO]: Epoch 094 - training loss: 0.7005\n",
      "2024-10-20 23:26:27 [INFO]: Epoch 095 - training loss: 0.6954\n",
      "2024-10-20 23:26:30 [INFO]: Epoch 096 - training loss: 0.6991\n",
      "2024-10-20 23:26:33 [INFO]: Epoch 097 - training loss: 0.7000\n",
      "2024-10-20 23:26:36 [INFO]: Epoch 098 - training loss: 0.7102\n",
      "2024-10-20 23:26:39 [INFO]: Epoch 099 - training loss: 0.7069\n",
      "2024-10-20 23:26:41 [INFO]: Epoch 100 - training loss: 0.6939\n",
      "2024-10-20 23:26:41 [INFO]: Finished training. The best model is from epoch#89.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 3/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 23:32:01 [INFO]: Epoch 001 - training loss: 0.7156\n",
      "2024-10-20 23:32:03 [INFO]: Epoch 002 - training loss: 0.6971\n",
      "2024-10-20 23:32:06 [INFO]: Epoch 003 - training loss: 0.6959\n",
      "2024-10-20 23:32:09 [INFO]: Epoch 004 - training loss: 0.7014\n",
      "2024-10-20 23:32:12 [INFO]: Epoch 005 - training loss: 0.6967\n",
      "2024-10-20 23:32:15 [INFO]: Epoch 006 - training loss: 0.7022\n",
      "2024-10-20 23:32:18 [INFO]: Epoch 007 - training loss: 0.6962\n",
      "2024-10-20 23:32:20 [INFO]: Epoch 008 - training loss: 0.7045\n",
      "2024-10-20 23:32:23 [INFO]: Epoch 009 - training loss: 0.6993\n",
      "2024-10-20 23:32:26 [INFO]: Epoch 010 - training loss: 0.7001\n",
      "2024-10-20 23:32:29 [INFO]: Epoch 011 - training loss: 0.6903\n",
      "2024-10-20 23:32:32 [INFO]: Epoch 012 - training loss: 0.6882\n",
      "2024-10-20 23:32:35 [INFO]: Epoch 013 - training loss: 0.7015\n",
      "2024-10-20 23:32:37 [INFO]: Epoch 014 - training loss: 0.6926\n",
      "2024-10-20 23:32:40 [INFO]: Epoch 015 - training loss: 0.6944\n",
      "2024-10-20 23:32:43 [INFO]: Epoch 016 - training loss: 0.6972\n",
      "2024-10-20 23:32:46 [INFO]: Epoch 017 - training loss: 0.6942\n",
      "2024-10-20 23:32:49 [INFO]: Epoch 018 - training loss: 0.6958\n",
      "2024-10-20 23:32:51 [INFO]: Epoch 019 - training loss: 0.6958\n",
      "2024-10-20 23:32:54 [INFO]: Epoch 020 - training loss: 0.7277\n",
      "2024-10-20 23:32:57 [INFO]: Epoch 021 - training loss: 0.7073\n",
      "2024-10-20 23:33:00 [INFO]: Epoch 022 - training loss: 0.7041\n",
      "2024-10-20 23:33:03 [INFO]: Epoch 023 - training loss: 0.6896\n",
      "2024-10-20 23:33:05 [INFO]: Epoch 024 - training loss: 0.6988\n",
      "2024-10-20 23:33:08 [INFO]: Epoch 025 - training loss: 0.6891\n",
      "2024-10-20 23:33:11 [INFO]: Epoch 026 - training loss: 0.6948\n",
      "2024-10-20 23:33:14 [INFO]: Epoch 027 - training loss: 0.6927\n",
      "2024-10-20 23:33:17 [INFO]: Epoch 028 - training loss: 0.6911\n",
      "2024-10-20 23:33:19 [INFO]: Epoch 029 - training loss: 0.6882\n",
      "2024-10-20 23:33:22 [INFO]: Epoch 030 - training loss: 0.6935\n",
      "2024-10-20 23:33:25 [INFO]: Epoch 031 - training loss: 0.6904\n",
      "2024-10-20 23:33:28 [INFO]: Epoch 032 - training loss: 0.7021\n",
      "2024-10-20 23:33:31 [INFO]: Epoch 033 - training loss: 0.6939\n",
      "2024-10-20 23:33:34 [INFO]: Epoch 034 - training loss: 0.6906\n",
      "2024-10-20 23:33:36 [INFO]: Epoch 035 - training loss: 0.6860\n",
      "2024-10-20 23:33:39 [INFO]: Epoch 036 - training loss: 0.6917\n",
      "2024-10-20 23:33:42 [INFO]: Epoch 037 - training loss: 0.6876\n",
      "2024-10-20 23:33:45 [INFO]: Epoch 038 - training loss: 0.6919\n",
      "2024-10-20 23:33:48 [INFO]: Epoch 039 - training loss: 0.6972\n",
      "2024-10-20 23:33:50 [INFO]: Epoch 040 - training loss: 0.6983\n",
      "2024-10-20 23:33:53 [INFO]: Epoch 041 - training loss: 0.6886\n",
      "2024-10-20 23:33:56 [INFO]: Epoch 042 - training loss: 0.6885\n",
      "2024-10-20 23:33:59 [INFO]: Epoch 043 - training loss: 0.7029\n",
      "2024-10-20 23:34:02 [INFO]: Epoch 044 - training loss: 0.6963\n",
      "2024-10-20 23:34:05 [INFO]: Epoch 045 - training loss: 0.6970\n",
      "2024-10-20 23:34:07 [INFO]: Epoch 046 - training loss: 0.7355\n",
      "2024-10-20 23:34:10 [INFO]: Epoch 047 - training loss: 0.6991\n",
      "2024-10-20 23:34:13 [INFO]: Epoch 048 - training loss: 0.7015\n",
      "2024-10-20 23:34:16 [INFO]: Epoch 049 - training loss: 0.6945\n",
      "2024-10-20 23:34:19 [INFO]: Epoch 050 - training loss: 0.6824\n",
      "2024-10-20 23:34:21 [INFO]: Epoch 051 - training loss: 0.6936\n",
      "2024-10-20 23:34:24 [INFO]: Epoch 052 - training loss: 0.6884\n",
      "2024-10-20 23:34:27 [INFO]: Epoch 053 - training loss: 0.6922\n",
      "2024-10-20 23:34:30 [INFO]: Epoch 054 - training loss: 0.6809\n",
      "2024-10-20 23:34:33 [INFO]: Epoch 055 - training loss: 0.6819\n",
      "2024-10-20 23:34:35 [INFO]: Epoch 056 - training loss: 0.6880\n",
      "2024-10-20 23:34:38 [INFO]: Epoch 057 - training loss: 0.6783\n",
      "2024-10-20 23:34:41 [INFO]: Epoch 058 - training loss: 0.6910\n",
      "2024-10-20 23:34:44 [INFO]: Epoch 059 - training loss: 0.6782\n",
      "2024-10-20 23:34:47 [INFO]: Epoch 060 - training loss: 0.6775\n",
      "2024-10-20 23:34:50 [INFO]: Epoch 061 - training loss: 0.6864\n",
      "2024-10-20 23:34:52 [INFO]: Epoch 062 - training loss: 0.6875\n",
      "2024-10-20 23:34:55 [INFO]: Epoch 063 - training loss: 0.6906\n",
      "2024-10-20 23:34:58 [INFO]: Epoch 064 - training loss: 0.6812\n",
      "2024-10-20 23:35:01 [INFO]: Epoch 065 - training loss: 0.6857\n",
      "2024-10-20 23:35:04 [INFO]: Epoch 066 - training loss: 0.6891\n",
      "2024-10-20 23:35:07 [INFO]: Epoch 067 - training loss: 0.6829\n",
      "2024-10-20 23:35:09 [INFO]: Epoch 068 - training loss: 0.6782\n",
      "2024-10-20 23:35:12 [INFO]: Epoch 069 - training loss: 0.6906\n",
      "2024-10-20 23:35:15 [INFO]: Epoch 070 - training loss: 0.6903\n",
      "2024-10-20 23:35:18 [INFO]: Epoch 071 - training loss: 0.6914\n",
      "2024-10-20 23:35:21 [INFO]: Epoch 072 - training loss: 0.7039\n",
      "2024-10-20 23:35:24 [INFO]: Epoch 073 - training loss: 0.6866\n",
      "2024-10-20 23:35:26 [INFO]: Epoch 074 - training loss: 0.6937\n",
      "2024-10-20 23:35:29 [INFO]: Epoch 075 - training loss: 0.6819\n",
      "2024-10-20 23:35:32 [INFO]: Epoch 076 - training loss: 0.6821\n",
      "2024-10-20 23:35:35 [INFO]: Epoch 077 - training loss: 0.6898\n",
      "2024-10-20 23:35:38 [INFO]: Epoch 078 - training loss: 0.6873\n",
      "2024-10-20 23:35:40 [INFO]: Epoch 079 - training loss: 0.6908\n",
      "2024-10-20 23:35:43 [INFO]: Epoch 080 - training loss: 0.6776\n",
      "2024-10-20 23:35:46 [INFO]: Epoch 081 - training loss: 0.6765\n",
      "2024-10-20 23:35:49 [INFO]: Epoch 082 - training loss: 0.6810\n",
      "2024-10-20 23:35:52 [INFO]: Epoch 083 - training loss: 0.6898\n",
      "2024-10-20 23:35:55 [INFO]: Epoch 084 - training loss: 0.7123\n",
      "2024-10-20 23:35:57 [INFO]: Epoch 085 - training loss: 0.6961\n",
      "2024-10-20 23:36:00 [INFO]: Epoch 086 - training loss: 0.6821\n",
      "2024-10-20 23:36:03 [INFO]: Epoch 087 - training loss: 0.6826\n",
      "2024-10-20 23:36:06 [INFO]: Epoch 088 - training loss: 0.6802\n",
      "2024-10-20 23:36:09 [INFO]: Epoch 089 - training loss: 0.6951\n",
      "2024-10-20 23:36:12 [INFO]: Epoch 090 - training loss: 0.6870\n",
      "2024-10-20 23:36:14 [INFO]: Epoch 091 - training loss: 0.6872\n",
      "2024-10-20 23:36:17 [INFO]: Epoch 092 - training loss: 0.6903\n",
      "2024-10-20 23:36:20 [INFO]: Epoch 093 - training loss: 0.7091\n",
      "2024-10-20 23:36:23 [INFO]: Epoch 094 - training loss: 0.7075\n",
      "2024-10-20 23:36:26 [INFO]: Epoch 095 - training loss: 0.6760\n",
      "2024-10-20 23:36:28 [INFO]: Epoch 096 - training loss: 0.6750\n",
      "2024-10-20 23:36:31 [INFO]: Epoch 097 - training loss: 0.6834\n",
      "2024-10-20 23:36:34 [INFO]: Epoch 098 - training loss: 0.6743\n",
      "2024-10-20 23:36:37 [INFO]: Epoch 099 - training loss: 0.6831\n",
      "2024-10-20 23:36:40 [INFO]: Epoch 100 - training loss: 0.6729\n",
      "2024-10-20 23:36:40 [INFO]: Finished training. The best model is from epoch#100.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 4/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 23:41:59 [INFO]: Epoch 001 - training loss: 0.6924\n",
      "2024-10-20 23:42:02 [INFO]: Epoch 002 - training loss: 0.6899\n",
      "2024-10-20 23:42:04 [INFO]: Epoch 003 - training loss: 0.6885\n",
      "2024-10-20 23:42:07 [INFO]: Epoch 004 - training loss: 0.6853\n",
      "2024-10-20 23:42:10 [INFO]: Epoch 005 - training loss: 0.6859\n",
      "2024-10-20 23:42:13 [INFO]: Epoch 006 - training loss: 0.6880\n",
      "2024-10-20 23:42:16 [INFO]: Epoch 007 - training loss: 0.6834\n",
      "2024-10-20 23:42:18 [INFO]: Epoch 008 - training loss: 0.6800\n",
      "2024-10-20 23:42:21 [INFO]: Epoch 009 - training loss: 0.6818\n",
      "2024-10-20 23:42:24 [INFO]: Epoch 010 - training loss: 0.6822\n",
      "2024-10-20 23:42:27 [INFO]: Epoch 011 - training loss: 0.6815\n",
      "2024-10-20 23:42:30 [INFO]: Epoch 012 - training loss: 0.6796\n",
      "2024-10-20 23:42:33 [INFO]: Epoch 013 - training loss: 0.6891\n",
      "2024-10-20 23:42:35 [INFO]: Epoch 014 - training loss: 0.6837\n",
      "2024-10-20 23:42:38 [INFO]: Epoch 015 - training loss: 0.6778\n",
      "2024-10-20 23:42:41 [INFO]: Epoch 016 - training loss: 0.6882\n",
      "2024-10-20 23:42:44 [INFO]: Epoch 017 - training loss: 0.6818\n",
      "2024-10-20 23:42:47 [INFO]: Epoch 018 - training loss: 0.6834\n",
      "2024-10-20 23:42:50 [INFO]: Epoch 019 - training loss: 0.6870\n",
      "2024-10-20 23:42:52 [INFO]: Epoch 020 - training loss: 0.6756\n",
      "2024-10-20 23:42:55 [INFO]: Epoch 021 - training loss: 0.6906\n",
      "2024-10-20 23:42:58 [INFO]: Epoch 022 - training loss: 0.6769\n",
      "2024-10-20 23:43:01 [INFO]: Epoch 023 - training loss: 0.6956\n",
      "2024-10-20 23:43:04 [INFO]: Epoch 024 - training loss: 0.6910\n",
      "2024-10-20 23:43:07 [INFO]: Epoch 025 - training loss: 0.6812\n",
      "2024-10-20 23:43:09 [INFO]: Epoch 026 - training loss: 0.6861\n",
      "2024-10-20 23:43:12 [INFO]: Epoch 027 - training loss: 0.6862\n",
      "2024-10-20 23:43:15 [INFO]: Epoch 028 - training loss: 0.6927\n",
      "2024-10-20 23:43:18 [INFO]: Epoch 029 - training loss: 0.6804\n",
      "2024-10-20 23:43:21 [INFO]: Epoch 030 - training loss: 0.6804\n",
      "2024-10-20 23:43:23 [INFO]: Epoch 031 - training loss: 0.6807\n",
      "2024-10-20 23:43:26 [INFO]: Epoch 032 - training loss: 0.6732\n",
      "2024-10-20 23:43:29 [INFO]: Epoch 033 - training loss: 0.6861\n",
      "2024-10-20 23:43:32 [INFO]: Epoch 034 - training loss: 0.6848\n",
      "2024-10-20 23:43:35 [INFO]: Epoch 035 - training loss: 0.6957\n",
      "2024-10-20 23:43:38 [INFO]: Epoch 036 - training loss: 0.6821\n",
      "2024-10-20 23:43:40 [INFO]: Epoch 037 - training loss: 0.6724\n",
      "2024-10-20 23:43:43 [INFO]: Epoch 038 - training loss: 0.6843\n",
      "2024-10-20 23:43:46 [INFO]: Epoch 039 - training loss: 0.6870\n",
      "2024-10-20 23:43:49 [INFO]: Epoch 040 - training loss: 0.6922\n",
      "2024-10-20 23:43:52 [INFO]: Epoch 041 - training loss: 0.6794\n",
      "2024-10-20 23:43:54 [INFO]: Epoch 042 - training loss: 0.6819\n",
      "2024-10-20 23:43:57 [INFO]: Epoch 043 - training loss: 0.6942\n",
      "2024-10-20 23:44:00 [INFO]: Epoch 044 - training loss: 0.6922\n",
      "2024-10-20 23:44:03 [INFO]: Epoch 045 - training loss: 0.6878\n",
      "2024-10-20 23:44:06 [INFO]: Epoch 046 - training loss: 0.6927\n",
      "2024-10-20 23:44:09 [INFO]: Epoch 047 - training loss: 0.6805\n",
      "2024-10-20 23:44:11 [INFO]: Epoch 048 - training loss: 0.6855\n",
      "2024-10-20 23:44:14 [INFO]: Epoch 049 - training loss: 0.6889\n",
      "2024-10-20 23:44:17 [INFO]: Epoch 050 - training loss: 0.6791\n",
      "2024-10-20 23:44:20 [INFO]: Epoch 051 - training loss: 0.6940\n",
      "2024-10-20 23:44:23 [INFO]: Epoch 052 - training loss: 0.6781\n",
      "2024-10-20 23:44:26 [INFO]: Epoch 053 - training loss: 0.6851\n",
      "2024-10-20 23:44:28 [INFO]: Epoch 054 - training loss: 0.6983\n",
      "2024-10-20 23:44:31 [INFO]: Epoch 055 - training loss: 0.6731\n",
      "2024-10-20 23:44:34 [INFO]: Epoch 056 - training loss: 0.6858\n",
      "2024-10-20 23:44:37 [INFO]: Epoch 057 - training loss: 0.6835\n",
      "2024-10-20 23:44:40 [INFO]: Epoch 058 - training loss: 0.6840\n",
      "2024-10-20 23:44:42 [INFO]: Epoch 059 - training loss: 0.6848\n",
      "2024-10-20 23:44:45 [INFO]: Epoch 060 - training loss: 0.6936\n",
      "2024-10-20 23:44:48 [INFO]: Epoch 061 - training loss: 0.6909\n",
      "2024-10-20 23:44:51 [INFO]: Epoch 062 - training loss: 0.6746\n",
      "2024-10-20 23:44:54 [INFO]: Epoch 063 - training loss: 0.6972\n",
      "2024-10-20 23:44:57 [INFO]: Epoch 064 - training loss: 0.6761\n",
      "2024-10-20 23:44:59 [INFO]: Epoch 065 - training loss: 0.6787\n",
      "2024-10-20 23:45:02 [INFO]: Epoch 066 - training loss: 0.6953\n",
      "2024-10-20 23:45:05 [INFO]: Epoch 067 - training loss: 0.6770\n",
      "2024-10-20 23:45:08 [INFO]: Epoch 068 - training loss: 0.6763\n",
      "2024-10-20 23:45:11 [INFO]: Epoch 069 - training loss: 0.6868\n",
      "2024-10-20 23:45:13 [INFO]: Epoch 070 - training loss: 0.6728\n",
      "2024-10-20 23:45:16 [INFO]: Epoch 071 - training loss: 0.6876\n",
      "2024-10-20 23:45:19 [INFO]: Epoch 072 - training loss: 0.6842\n",
      "2024-10-20 23:45:22 [INFO]: Epoch 073 - training loss: 0.6749\n",
      "2024-10-20 23:45:25 [INFO]: Epoch 074 - training loss: 0.6766\n",
      "2024-10-20 23:45:27 [INFO]: Epoch 075 - training loss: 0.6706\n",
      "2024-10-20 23:45:30 [INFO]: Epoch 076 - training loss: 0.6827\n",
      "2024-10-20 23:45:33 [INFO]: Epoch 077 - training loss: 0.6764\n",
      "2024-10-20 23:45:36 [INFO]: Epoch 078 - training loss: 0.6708\n",
      "2024-10-20 23:45:39 [INFO]: Epoch 079 - training loss: 0.6742\n",
      "2024-10-20 23:45:41 [INFO]: Epoch 080 - training loss: 0.6786\n",
      "2024-10-20 23:45:44 [INFO]: Epoch 081 - training loss: 0.6739\n",
      "2024-10-20 23:45:47 [INFO]: Epoch 082 - training loss: 0.6742\n",
      "2024-10-20 23:45:50 [INFO]: Epoch 083 - training loss: 0.6724\n",
      "2024-10-20 23:45:53 [INFO]: Epoch 084 - training loss: 0.6700\n",
      "2024-10-20 23:45:56 [INFO]: Epoch 085 - training loss: 0.6765\n",
      "2024-10-20 23:45:58 [INFO]: Epoch 086 - training loss: 0.6738\n",
      "2024-10-20 23:46:01 [INFO]: Epoch 087 - training loss: 0.6724\n",
      "2024-10-20 23:46:04 [INFO]: Epoch 088 - training loss: 0.6716\n",
      "2024-10-20 23:46:07 [INFO]: Epoch 089 - training loss: 0.6844\n",
      "2024-10-20 23:46:10 [INFO]: Epoch 090 - training loss: 0.6807\n",
      "2024-10-20 23:46:13 [INFO]: Epoch 091 - training loss: 0.6756\n",
      "2024-10-20 23:46:16 [INFO]: Epoch 092 - training loss: 0.6703\n",
      "2024-10-20 23:46:18 [INFO]: Epoch 093 - training loss: 0.6801\n",
      "2024-10-20 23:46:21 [INFO]: Epoch 094 - training loss: 0.6632\n",
      "2024-10-20 23:46:24 [INFO]: Epoch 095 - training loss: 0.6815\n",
      "2024-10-20 23:46:27 [INFO]: Epoch 096 - training loss: 0.6775\n",
      "2024-10-20 23:46:30 [INFO]: Epoch 097 - training loss: 0.6708\n",
      "2024-10-20 23:46:32 [INFO]: Epoch 098 - training loss: 0.6721\n",
      "2024-10-20 23:46:35 [INFO]: Epoch 099 - training loss: 0.6740\n",
      "2024-10-20 23:46:38 [INFO]: Epoch 100 - training loss: 0.6652\n",
      "2024-10-20 23:46:38 [INFO]: Finished training. The best model is from epoch#94.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 5/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 23:51:55 [INFO]: Epoch 001 - training loss: 0.6687\n",
      "2024-10-20 23:51:58 [INFO]: Epoch 002 - training loss: 0.6638\n",
      "2024-10-20 23:52:01 [INFO]: Epoch 003 - training loss: 0.6712\n",
      "2024-10-20 23:52:04 [INFO]: Epoch 004 - training loss: 0.6698\n",
      "2024-10-20 23:52:06 [INFO]: Epoch 005 - training loss: 0.6739\n",
      "2024-10-20 23:52:09 [INFO]: Epoch 006 - training loss: 0.6710\n",
      "2024-10-20 23:52:12 [INFO]: Epoch 007 - training loss: 0.6717\n",
      "2024-10-20 23:52:15 [INFO]: Epoch 008 - training loss: 0.6821\n",
      "2024-10-20 23:52:18 [INFO]: Epoch 009 - training loss: 0.6891\n",
      "2024-10-20 23:52:21 [INFO]: Epoch 010 - training loss: 0.6922\n",
      "2024-10-20 23:52:23 [INFO]: Epoch 011 - training loss: 0.6742\n",
      "2024-10-20 23:52:26 [INFO]: Epoch 012 - training loss: 0.6694\n",
      "2024-10-20 23:52:29 [INFO]: Epoch 013 - training loss: 0.7056\n",
      "2024-10-20 23:52:32 [INFO]: Epoch 014 - training loss: 0.6773\n",
      "2024-10-20 23:52:35 [INFO]: Epoch 015 - training loss: 0.6787\n",
      "2024-10-20 23:52:37 [INFO]: Epoch 016 - training loss: 0.6693\n",
      "2024-10-20 23:52:40 [INFO]: Epoch 017 - training loss: 0.6682\n",
      "2024-10-20 23:52:43 [INFO]: Epoch 018 - training loss: 0.6650\n",
      "2024-10-20 23:52:46 [INFO]: Epoch 019 - training loss: 0.6731\n",
      "2024-10-20 23:52:49 [INFO]: Epoch 020 - training loss: 0.6678\n",
      "2024-10-20 23:52:51 [INFO]: Epoch 021 - training loss: 0.6677\n",
      "2024-10-20 23:52:54 [INFO]: Epoch 022 - training loss: 0.6572\n",
      "2024-10-20 23:52:57 [INFO]: Epoch 023 - training loss: 0.6703\n",
      "2024-10-20 23:53:00 [INFO]: Epoch 024 - training loss: 0.6743\n",
      "2024-10-20 23:53:03 [INFO]: Epoch 025 - training loss: 0.6667\n",
      "2024-10-20 23:53:05 [INFO]: Epoch 026 - training loss: 0.6606\n",
      "2024-10-20 23:53:08 [INFO]: Epoch 027 - training loss: 0.6618\n",
      "2024-10-20 23:53:11 [INFO]: Epoch 028 - training loss: 0.6618\n",
      "2024-10-20 23:53:14 [INFO]: Epoch 029 - training loss: 0.6707\n",
      "2024-10-20 23:53:17 [INFO]: Epoch 030 - training loss: 0.6613\n",
      "2024-10-20 23:53:19 [INFO]: Epoch 031 - training loss: 0.6664\n",
      "2024-10-20 23:53:22 [INFO]: Epoch 032 - training loss: 0.6694\n",
      "2024-10-20 23:53:25 [INFO]: Epoch 033 - training loss: 0.6935\n",
      "2024-10-20 23:53:28 [INFO]: Epoch 034 - training loss: 0.6659\n",
      "2024-10-20 23:53:31 [INFO]: Epoch 035 - training loss: 0.6690\n",
      "2024-10-20 23:53:33 [INFO]: Epoch 036 - training loss: 0.6611\n",
      "2024-10-20 23:53:36 [INFO]: Epoch 037 - training loss: 0.6703\n",
      "2024-10-20 23:53:39 [INFO]: Epoch 038 - training loss: 0.6674\n",
      "2024-10-20 23:53:42 [INFO]: Epoch 039 - training loss: 0.6531\n",
      "2024-10-20 23:53:45 [INFO]: Epoch 040 - training loss: 0.6693\n",
      "2024-10-20 23:53:47 [INFO]: Epoch 041 - training loss: 0.6652\n",
      "2024-10-20 23:53:50 [INFO]: Epoch 042 - training loss: 0.6586\n",
      "2024-10-20 23:53:53 [INFO]: Epoch 043 - training loss: 0.6651\n",
      "2024-10-20 23:53:56 [INFO]: Epoch 044 - training loss: 0.6870\n",
      "2024-10-20 23:53:59 [INFO]: Epoch 045 - training loss: 0.6681\n",
      "2024-10-20 23:54:01 [INFO]: Epoch 046 - training loss: 0.6777\n",
      "2024-10-20 23:54:04 [INFO]: Epoch 047 - training loss: 0.6737\n",
      "2024-10-20 23:54:07 [INFO]: Epoch 048 - training loss: 0.6711\n",
      "2024-10-20 23:54:10 [INFO]: Epoch 049 - training loss: 0.6542\n",
      "2024-10-20 23:54:13 [INFO]: Epoch 050 - training loss: 0.6631\n",
      "2024-10-20 23:54:15 [INFO]: Epoch 051 - training loss: 0.6546\n",
      "2024-10-20 23:54:18 [INFO]: Epoch 052 - training loss: 0.6699\n",
      "2024-10-20 23:54:21 [INFO]: Epoch 053 - training loss: 0.6628\n",
      "2024-10-20 23:54:24 [INFO]: Epoch 054 - training loss: 0.6622\n",
      "2024-10-20 23:54:27 [INFO]: Epoch 055 - training loss: 0.6625\n",
      "2024-10-20 23:54:29 [INFO]: Epoch 056 - training loss: 0.6850\n",
      "2024-10-20 23:54:32 [INFO]: Epoch 057 - training loss: 0.6717\n",
      "2024-10-20 23:54:35 [INFO]: Epoch 058 - training loss: 0.6673\n",
      "2024-10-20 23:54:38 [INFO]: Epoch 059 - training loss: 0.6612\n",
      "2024-10-20 23:54:41 [INFO]: Epoch 060 - training loss: 0.6652\n",
      "2024-10-20 23:54:43 [INFO]: Epoch 061 - training loss: 0.6652\n",
      "2024-10-20 23:54:46 [INFO]: Epoch 062 - training loss: 0.6716\n",
      "2024-10-20 23:54:49 [INFO]: Epoch 063 - training loss: 0.6829\n",
      "2024-10-20 23:54:52 [INFO]: Epoch 064 - training loss: 0.6641\n",
      "2024-10-20 23:54:55 [INFO]: Epoch 065 - training loss: 0.6771\n",
      "2024-10-20 23:54:58 [INFO]: Epoch 066 - training loss: 0.6612\n",
      "2024-10-20 23:55:00 [INFO]: Epoch 067 - training loss: 0.6827\n",
      "2024-10-20 23:55:03 [INFO]: Epoch 068 - training loss: 0.6631\n",
      "2024-10-20 23:55:06 [INFO]: Epoch 069 - training loss: 0.6662\n",
      "2024-10-20 23:55:09 [INFO]: Epoch 070 - training loss: 0.6606\n",
      "2024-10-20 23:55:12 [INFO]: Epoch 071 - training loss: 0.6606\n",
      "2024-10-20 23:55:15 [INFO]: Epoch 072 - training loss: 0.6642\n",
      "2024-10-20 23:55:17 [INFO]: Epoch 073 - training loss: 0.6725\n",
      "2024-10-20 23:55:20 [INFO]: Epoch 074 - training loss: 0.6874\n",
      "2024-10-20 23:55:23 [INFO]: Epoch 075 - training loss: 0.6878\n",
      "2024-10-20 23:55:26 [INFO]: Epoch 076 - training loss: 0.6592\n",
      "2024-10-20 23:55:29 [INFO]: Epoch 077 - training loss: 0.6688\n",
      "2024-10-20 23:55:31 [INFO]: Epoch 078 - training loss: 0.6647\n",
      "2024-10-20 23:55:34 [INFO]: Epoch 079 - training loss: 0.6706\n",
      "2024-10-20 23:55:37 [INFO]: Epoch 080 - training loss: 0.6761\n",
      "2024-10-20 23:55:40 [INFO]: Epoch 081 - training loss: 0.6644\n",
      "2024-10-20 23:55:43 [INFO]: Epoch 082 - training loss: 0.6919\n",
      "2024-10-20 23:55:46 [INFO]: Epoch 083 - training loss: 0.6773\n",
      "2024-10-20 23:55:48 [INFO]: Epoch 084 - training loss: 0.6595\n",
      "2024-10-20 23:55:51 [INFO]: Epoch 085 - training loss: 0.6602\n",
      "2024-10-20 23:55:54 [INFO]: Epoch 086 - training loss: 0.6494\n",
      "2024-10-20 23:55:57 [INFO]: Epoch 087 - training loss: 0.6589\n",
      "2024-10-20 23:56:00 [INFO]: Epoch 088 - training loss: 0.6669\n",
      "2024-10-20 23:56:02 [INFO]: Epoch 089 - training loss: 0.6701\n",
      "2024-10-20 23:56:05 [INFO]: Epoch 090 - training loss: 0.6610\n",
      "2024-10-20 23:56:08 [INFO]: Epoch 091 - training loss: 0.6627\n",
      "2024-10-20 23:56:11 [INFO]: Epoch 092 - training loss: 0.6606\n",
      "2024-10-20 23:56:13 [INFO]: Epoch 093 - training loss: 0.6778\n",
      "2024-10-20 23:56:16 [INFO]: Epoch 094 - training loss: 0.6594\n",
      "2024-10-20 23:56:19 [INFO]: Epoch 095 - training loss: 0.6557\n",
      "2024-10-20 23:56:22 [INFO]: Epoch 096 - training loss: 0.6532\n",
      "2024-10-20 23:56:25 [INFO]: Epoch 097 - training loss: 0.6629\n",
      "2024-10-20 23:56:28 [INFO]: Epoch 098 - training loss: 0.6617\n",
      "2024-10-20 23:56:30 [INFO]: Epoch 099 - training loss: 0.6558\n",
      "2024-10-20 23:56:33 [INFO]: Epoch 100 - training loss: 0.6613\n",
      "2024-10-20 23:56:33 [INFO]: Finished training. The best model is from epoch#86.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 6/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 00:01:52 [INFO]: Epoch 001 - training loss: 0.6858\n",
      "2024-10-21 00:01:55 [INFO]: Epoch 002 - training loss: 0.6699\n",
      "2024-10-21 00:01:58 [INFO]: Epoch 003 - training loss: 0.6644\n",
      "2024-10-21 00:02:01 [INFO]: Epoch 004 - training loss: 0.6657\n",
      "2024-10-21 00:02:04 [INFO]: Epoch 005 - training loss: 0.6747\n",
      "2024-10-21 00:02:06 [INFO]: Epoch 006 - training loss: 0.6775\n",
      "2024-10-21 00:02:09 [INFO]: Epoch 007 - training loss: 0.6739\n",
      "2024-10-21 00:02:12 [INFO]: Epoch 008 - training loss: 0.6699\n",
      "2024-10-21 00:02:15 [INFO]: Epoch 009 - training loss: 0.6741\n",
      "2024-10-21 00:02:18 [INFO]: Epoch 010 - training loss: 0.6646\n",
      "2024-10-21 00:02:21 [INFO]: Epoch 011 - training loss: 0.6528\n",
      "2024-10-21 00:02:23 [INFO]: Epoch 012 - training loss: 0.6637\n",
      "2024-10-21 00:02:26 [INFO]: Epoch 013 - training loss: 0.6509\n",
      "2024-10-21 00:02:29 [INFO]: Epoch 014 - training loss: 0.6611\n",
      "2024-10-21 00:02:32 [INFO]: Epoch 015 - training loss: 0.6641\n",
      "2024-10-21 00:02:35 [INFO]: Epoch 016 - training loss: 0.6703\n",
      "2024-10-21 00:02:37 [INFO]: Epoch 017 - training loss: 0.6711\n",
      "2024-10-21 00:02:40 [INFO]: Epoch 018 - training loss: 0.6649\n",
      "2024-10-21 00:02:43 [INFO]: Epoch 019 - training loss: 0.6543\n",
      "2024-10-21 00:02:46 [INFO]: Epoch 020 - training loss: 0.6666\n",
      "2024-10-21 00:02:49 [INFO]: Epoch 021 - training loss: 0.6651\n",
      "2024-10-21 00:02:51 [INFO]: Epoch 022 - training loss: 0.6554\n",
      "2024-10-21 00:02:54 [INFO]: Epoch 023 - training loss: 0.6598\n",
      "2024-10-21 00:02:57 [INFO]: Epoch 024 - training loss: 0.6548\n",
      "2024-10-21 00:03:00 [INFO]: Epoch 025 - training loss: 0.6458\n",
      "2024-10-21 00:03:03 [INFO]: Epoch 026 - training loss: 0.6562\n",
      "2024-10-21 00:03:06 [INFO]: Epoch 027 - training loss: 0.6555\n",
      "2024-10-21 00:03:08 [INFO]: Epoch 028 - training loss: 0.6474\n",
      "2024-10-21 00:03:11 [INFO]: Epoch 029 - training loss: 0.6614\n",
      "2024-10-21 00:03:14 [INFO]: Epoch 030 - training loss: 0.6563\n",
      "2024-10-21 00:03:17 [INFO]: Epoch 031 - training loss: 0.6638\n",
      "2024-10-21 00:03:20 [INFO]: Epoch 032 - training loss: 0.6520\n",
      "2024-10-21 00:03:22 [INFO]: Epoch 033 - training loss: 0.6559\n",
      "2024-10-21 00:03:25 [INFO]: Epoch 034 - training loss: 0.6602\n",
      "2024-10-21 00:03:28 [INFO]: Epoch 035 - training loss: 0.6588\n",
      "2024-10-21 00:03:31 [INFO]: Epoch 036 - training loss: 0.6602\n",
      "2024-10-21 00:03:34 [INFO]: Epoch 037 - training loss: 0.6590\n",
      "2024-10-21 00:03:36 [INFO]: Epoch 038 - training loss: 0.6628\n",
      "2024-10-21 00:03:39 [INFO]: Epoch 039 - training loss: 0.6570\n",
      "2024-10-21 00:03:42 [INFO]: Epoch 040 - training loss: 0.6517\n",
      "2024-10-21 00:03:45 [INFO]: Epoch 041 - training loss: 0.6665\n",
      "2024-10-21 00:03:48 [INFO]: Epoch 042 - training loss: 0.6531\n",
      "2024-10-21 00:03:51 [INFO]: Epoch 043 - training loss: 0.6507\n",
      "2024-10-21 00:03:53 [INFO]: Epoch 044 - training loss: 0.6543\n",
      "2024-10-21 00:03:56 [INFO]: Epoch 045 - training loss: 0.6466\n",
      "2024-10-21 00:03:59 [INFO]: Epoch 046 - training loss: 0.6595\n",
      "2024-10-21 00:04:02 [INFO]: Epoch 047 - training loss: 0.6522\n",
      "2024-10-21 00:04:05 [INFO]: Epoch 048 - training loss: 0.6499\n",
      "2024-10-21 00:04:07 [INFO]: Epoch 049 - training loss: 0.6542\n",
      "2024-10-21 00:04:10 [INFO]: Epoch 050 - training loss: 0.6516\n",
      "2024-10-21 00:04:13 [INFO]: Epoch 051 - training loss: 0.6556\n",
      "2024-10-21 00:04:16 [INFO]: Epoch 052 - training loss: 0.6551\n",
      "2024-10-21 00:04:19 [INFO]: Epoch 053 - training loss: 0.6579\n",
      "2024-10-21 00:04:22 [INFO]: Epoch 054 - training loss: 0.6605\n",
      "2024-10-21 00:04:24 [INFO]: Epoch 055 - training loss: 0.6558\n",
      "2024-10-21 00:04:27 [INFO]: Epoch 056 - training loss: 0.6542\n",
      "2024-10-21 00:04:30 [INFO]: Epoch 057 - training loss: 0.6537\n",
      "2024-10-21 00:04:33 [INFO]: Epoch 058 - training loss: 0.6605\n",
      "2024-10-21 00:04:36 [INFO]: Epoch 059 - training loss: 0.6591\n",
      "2024-10-21 00:04:39 [INFO]: Epoch 060 - training loss: 0.6631\n",
      "2024-10-21 00:04:41 [INFO]: Epoch 061 - training loss: 0.6516\n",
      "2024-10-21 00:04:44 [INFO]: Epoch 062 - training loss: 0.6481\n",
      "2024-10-21 00:04:47 [INFO]: Epoch 063 - training loss: 0.6568\n",
      "2024-10-21 00:04:49 [INFO]: Epoch 064 - training loss: 0.6471\n",
      "2024-10-21 00:04:52 [INFO]: Epoch 065 - training loss: 0.6559\n",
      "2024-10-21 00:04:55 [INFO]: Epoch 066 - training loss: 0.6531\n",
      "2024-10-21 00:04:57 [INFO]: Epoch 067 - training loss: 0.6525\n",
      "2024-10-21 00:05:00 [INFO]: Epoch 068 - training loss: 0.6547\n",
      "2024-10-21 00:05:03 [INFO]: Epoch 069 - training loss: 0.6485\n",
      "2024-10-21 00:05:06 [INFO]: Epoch 070 - training loss: 0.6499\n",
      "2024-10-21 00:05:09 [INFO]: Epoch 071 - training loss: 0.6559\n",
      "2024-10-21 00:05:12 [INFO]: Epoch 072 - training loss: 0.6455\n",
      "2024-10-21 00:05:14 [INFO]: Epoch 073 - training loss: 0.6473\n",
      "2024-10-21 00:05:17 [INFO]: Epoch 074 - training loss: 0.6580\n",
      "2024-10-21 00:05:20 [INFO]: Epoch 075 - training loss: 0.6564\n",
      "2024-10-21 00:05:23 [INFO]: Epoch 076 - training loss: 0.6458\n",
      "2024-10-21 00:05:26 [INFO]: Epoch 077 - training loss: 0.6487\n",
      "2024-10-21 00:05:29 [INFO]: Epoch 078 - training loss: 0.6518\n",
      "2024-10-21 00:05:31 [INFO]: Epoch 079 - training loss: 0.6528\n",
      "2024-10-21 00:05:34 [INFO]: Epoch 080 - training loss: 0.6558\n",
      "2024-10-21 00:05:37 [INFO]: Epoch 081 - training loss: 0.6518\n",
      "2024-10-21 00:05:40 [INFO]: Epoch 082 - training loss: 0.6511\n",
      "2024-10-21 00:05:43 [INFO]: Epoch 083 - training loss: 0.6689\n",
      "2024-10-21 00:05:45 [INFO]: Epoch 084 - training loss: 0.6566\n",
      "2024-10-21 00:05:48 [INFO]: Epoch 085 - training loss: 0.6502\n",
      "2024-10-21 00:05:51 [INFO]: Epoch 086 - training loss: 0.6626\n",
      "2024-10-21 00:05:54 [INFO]: Epoch 087 - training loss: 0.6401\n",
      "2024-10-21 00:05:57 [INFO]: Epoch 088 - training loss: 0.6457\n",
      "2024-10-21 00:05:59 [INFO]: Epoch 089 - training loss: 0.6483\n",
      "2024-10-21 00:06:02 [INFO]: Epoch 090 - training loss: 0.6438\n",
      "2024-10-21 00:06:05 [INFO]: Epoch 091 - training loss: 0.6505\n",
      "2024-10-21 00:06:08 [INFO]: Epoch 092 - training loss: 0.6527\n",
      "2024-10-21 00:06:11 [INFO]: Epoch 093 - training loss: 0.6442\n",
      "2024-10-21 00:06:14 [INFO]: Epoch 094 - training loss: 0.6521\n",
      "2024-10-21 00:06:16 [INFO]: Epoch 095 - training loss: 0.6538\n",
      "2024-10-21 00:06:19 [INFO]: Epoch 096 - training loss: 0.6755\n",
      "2024-10-21 00:06:22 [INFO]: Epoch 097 - training loss: 0.6393\n",
      "2024-10-21 00:06:25 [INFO]: Epoch 098 - training loss: 0.6487\n",
      "2024-10-21 00:06:28 [INFO]: Epoch 099 - training loss: 0.6621\n",
      "2024-10-21 00:06:30 [INFO]: Epoch 100 - training loss: 0.6433\n",
      "2024-10-21 00:06:30 [INFO]: Finished training. The best model is from epoch#97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 7/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 00:11:49 [INFO]: Epoch 001 - training loss: 0.6592\n",
      "2024-10-21 00:11:52 [INFO]: Epoch 002 - training loss: 0.6600\n",
      "2024-10-21 00:11:55 [INFO]: Epoch 003 - training loss: 0.6726\n",
      "2024-10-21 00:11:57 [INFO]: Epoch 004 - training loss: 0.6645\n",
      "2024-10-21 00:12:00 [INFO]: Epoch 005 - training loss: 0.6533\n",
      "2024-10-21 00:12:03 [INFO]: Epoch 006 - training loss: 0.6568\n",
      "2024-10-21 00:12:06 [INFO]: Epoch 007 - training loss: 0.6584\n",
      "2024-10-21 00:12:09 [INFO]: Epoch 008 - training loss: 0.6708\n",
      "2024-10-21 00:12:11 [INFO]: Epoch 009 - training loss: 0.6569\n",
      "2024-10-21 00:12:14 [INFO]: Epoch 010 - training loss: 0.6589\n",
      "2024-10-21 00:12:17 [INFO]: Epoch 011 - training loss: 0.6549\n",
      "2024-10-21 00:12:20 [INFO]: Epoch 012 - training loss: 0.6588\n",
      "2024-10-21 00:12:23 [INFO]: Epoch 013 - training loss: 0.6660\n",
      "2024-10-21 00:12:25 [INFO]: Epoch 014 - training loss: 0.6581\n",
      "2024-10-21 00:12:28 [INFO]: Epoch 015 - training loss: 0.6625\n",
      "2024-10-21 00:12:31 [INFO]: Epoch 016 - training loss: 0.6584\n",
      "2024-10-21 00:12:34 [INFO]: Epoch 017 - training loss: 0.6675\n",
      "2024-10-21 00:12:37 [INFO]: Epoch 018 - training loss: 0.6562\n",
      "2024-10-21 00:12:39 [INFO]: Epoch 019 - training loss: 0.6615\n",
      "2024-10-21 00:12:42 [INFO]: Epoch 020 - training loss: 0.6493\n",
      "2024-10-21 00:12:45 [INFO]: Epoch 021 - training loss: 0.6475\n",
      "2024-10-21 00:12:48 [INFO]: Epoch 022 - training loss: 0.6472\n",
      "2024-10-21 00:12:51 [INFO]: Epoch 023 - training loss: 0.6511\n",
      "2024-10-21 00:12:53 [INFO]: Epoch 024 - training loss: 0.6535\n",
      "2024-10-21 00:12:56 [INFO]: Epoch 025 - training loss: 0.6488\n",
      "2024-10-21 00:12:59 [INFO]: Epoch 026 - training loss: 0.6508\n",
      "2024-10-21 00:13:02 [INFO]: Epoch 027 - training loss: 0.6487\n",
      "2024-10-21 00:13:05 [INFO]: Epoch 028 - training loss: 0.6559\n",
      "2024-10-21 00:13:08 [INFO]: Epoch 029 - training loss: 0.6494\n",
      "2024-10-21 00:13:10 [INFO]: Epoch 030 - training loss: 0.6438\n",
      "2024-10-21 00:13:13 [INFO]: Epoch 031 - training loss: 0.6482\n",
      "2024-10-21 00:13:16 [INFO]: Epoch 032 - training loss: 0.6647\n",
      "2024-10-21 00:13:19 [INFO]: Epoch 033 - training loss: 0.6596\n",
      "2024-10-21 00:13:22 [INFO]: Epoch 034 - training loss: 0.6584\n",
      "2024-10-21 00:13:24 [INFO]: Epoch 035 - training loss: 0.6475\n",
      "2024-10-21 00:13:27 [INFO]: Epoch 036 - training loss: 0.6544\n",
      "2024-10-21 00:13:30 [INFO]: Epoch 037 - training loss: 0.6505\n",
      "2024-10-21 00:13:33 [INFO]: Epoch 038 - training loss: 0.6502\n",
      "2024-10-21 00:13:36 [INFO]: Epoch 039 - training loss: 0.6436\n",
      "2024-10-21 00:13:39 [INFO]: Epoch 040 - training loss: 0.6576\n",
      "2024-10-21 00:13:41 [INFO]: Epoch 041 - training loss: 0.6492\n",
      "2024-10-21 00:13:44 [INFO]: Epoch 042 - training loss: 0.6499\n",
      "2024-10-21 00:13:47 [INFO]: Epoch 043 - training loss: 0.6465\n",
      "2024-10-21 00:13:50 [INFO]: Epoch 044 - training loss: 0.6530\n",
      "2024-10-21 00:13:53 [INFO]: Epoch 045 - training loss: 0.6452\n",
      "2024-10-21 00:13:56 [INFO]: Epoch 046 - training loss: 0.6533\n",
      "2024-10-21 00:13:59 [INFO]: Epoch 047 - training loss: 0.6555\n",
      "2024-10-21 00:14:01 [INFO]: Epoch 048 - training loss: 0.6456\n",
      "2024-10-21 00:14:04 [INFO]: Epoch 049 - training loss: 0.6491\n",
      "2024-10-21 00:14:07 [INFO]: Epoch 050 - training loss: 0.6460\n",
      "2024-10-21 00:14:10 [INFO]: Epoch 051 - training loss: 0.6419\n",
      "2024-10-21 00:14:13 [INFO]: Epoch 052 - training loss: 0.6496\n",
      "2024-10-21 00:14:15 [INFO]: Epoch 053 - training loss: 0.6479\n",
      "2024-10-21 00:14:18 [INFO]: Epoch 054 - training loss: 0.6500\n",
      "2024-10-21 00:14:21 [INFO]: Epoch 055 - training loss: 0.6492\n",
      "2024-10-21 00:14:24 [INFO]: Epoch 056 - training loss: 0.6505\n",
      "2024-10-21 00:14:27 [INFO]: Epoch 057 - training loss: 0.6542\n",
      "2024-10-21 00:14:30 [INFO]: Epoch 058 - training loss: 0.6469\n",
      "2024-10-21 00:14:32 [INFO]: Epoch 059 - training loss: 0.6498\n",
      "2024-10-21 00:14:35 [INFO]: Epoch 060 - training loss: 0.6428\n",
      "2024-10-21 00:14:38 [INFO]: Epoch 061 - training loss: 0.6495\n",
      "2024-10-21 00:14:41 [INFO]: Epoch 062 - training loss: 0.6525\n",
      "2024-10-21 00:14:44 [INFO]: Epoch 063 - training loss: 0.6519\n",
      "2024-10-21 00:14:47 [INFO]: Epoch 064 - training loss: 0.6485\n",
      "2024-10-21 00:14:50 [INFO]: Epoch 065 - training loss: 0.6536\n",
      "2024-10-21 00:14:52 [INFO]: Epoch 066 - training loss: 0.6502\n",
      "2024-10-21 00:14:55 [INFO]: Epoch 067 - training loss: 0.6444\n",
      "2024-10-21 00:14:58 [INFO]: Epoch 068 - training loss: 0.6490\n",
      "2024-10-21 00:15:01 [INFO]: Epoch 069 - training loss: 0.6571\n",
      "2024-10-21 00:15:04 [INFO]: Epoch 070 - training loss: 0.6572\n",
      "2024-10-21 00:15:06 [INFO]: Epoch 071 - training loss: 0.6526\n",
      "2024-10-21 00:15:09 [INFO]: Epoch 072 - training loss: 0.6421\n",
      "2024-10-21 00:15:12 [INFO]: Epoch 073 - training loss: 0.6506\n",
      "2024-10-21 00:15:15 [INFO]: Epoch 074 - training loss: 0.6469\n",
      "2024-10-21 00:15:18 [INFO]: Epoch 075 - training loss: 0.6410\n",
      "2024-10-21 00:15:21 [INFO]: Epoch 076 - training loss: 0.6412\n",
      "2024-10-21 00:15:23 [INFO]: Epoch 077 - training loss: 0.6449\n",
      "2024-10-21 00:15:26 [INFO]: Epoch 078 - training loss: 0.6474\n",
      "2024-10-21 00:15:29 [INFO]: Epoch 079 - training loss: 0.6440\n",
      "2024-10-21 00:15:32 [INFO]: Epoch 080 - training loss: 0.6526\n",
      "2024-10-21 00:15:35 [INFO]: Epoch 081 - training loss: 0.6563\n",
      "2024-10-21 00:15:37 [INFO]: Epoch 082 - training loss: 0.6458\n",
      "2024-10-21 00:15:40 [INFO]: Epoch 083 - training loss: 0.6516\n",
      "2024-10-21 00:15:43 [INFO]: Epoch 084 - training loss: 0.6442\n",
      "2024-10-21 00:15:46 [INFO]: Epoch 085 - training loss: 0.6467\n",
      "2024-10-21 00:15:49 [INFO]: Epoch 086 - training loss: 0.6488\n",
      "2024-10-21 00:15:51 [INFO]: Epoch 087 - training loss: 0.6482\n",
      "2024-10-21 00:15:54 [INFO]: Epoch 088 - training loss: 0.6477\n",
      "2024-10-21 00:15:57 [INFO]: Epoch 089 - training loss: 0.6510\n",
      "2024-10-21 00:16:00 [INFO]: Epoch 090 - training loss: 0.6446\n",
      "2024-10-21 00:16:03 [INFO]: Epoch 091 - training loss: 0.6485\n",
      "2024-10-21 00:16:06 [INFO]: Epoch 092 - training loss: 0.6481\n",
      "2024-10-21 00:16:08 [INFO]: Epoch 093 - training loss: 0.6478\n",
      "2024-10-21 00:16:11 [INFO]: Epoch 094 - training loss: 0.6428\n",
      "2024-10-21 00:16:14 [INFO]: Epoch 095 - training loss: 0.6535\n",
      "2024-10-21 00:16:17 [INFO]: Epoch 096 - training loss: 0.6484\n",
      "2024-10-21 00:16:19 [INFO]: Epoch 097 - training loss: 0.6392\n",
      "2024-10-21 00:16:22 [INFO]: Epoch 098 - training loss: 0.6465\n",
      "2024-10-21 00:16:25 [INFO]: Epoch 099 - training loss: 0.6492\n",
      "2024-10-21 00:16:28 [INFO]: Epoch 100 - training loss: 0.6436\n",
      "2024-10-21 00:16:28 [INFO]: Finished training. The best model is from epoch#97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 8/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 00:21:47 [INFO]: Epoch 001 - training loss: 0.6535\n",
      "2024-10-21 00:21:50 [INFO]: Epoch 002 - training loss: 0.6601\n",
      "2024-10-21 00:21:52 [INFO]: Epoch 003 - training loss: 0.6541\n",
      "2024-10-21 00:21:55 [INFO]: Epoch 004 - training loss: 0.6579\n",
      "2024-10-21 00:21:58 [INFO]: Epoch 005 - training loss: 0.6618\n",
      "2024-10-21 00:22:01 [INFO]: Epoch 006 - training loss: 0.6612\n",
      "2024-10-21 00:22:04 [INFO]: Epoch 007 - training loss: 0.6514\n",
      "2024-10-21 00:22:06 [INFO]: Epoch 008 - training loss: 0.6446\n",
      "2024-10-21 00:22:09 [INFO]: Epoch 009 - training loss: 0.6585\n",
      "2024-10-21 00:22:12 [INFO]: Epoch 010 - training loss: 0.6483\n",
      "2024-10-21 00:22:15 [INFO]: Epoch 011 - training loss: 0.6485\n",
      "2024-10-21 00:22:18 [INFO]: Epoch 012 - training loss: 0.6480\n",
      "2024-10-21 00:22:21 [INFO]: Epoch 013 - training loss: 0.6437\n",
      "2024-10-21 00:22:23 [INFO]: Epoch 014 - training loss: 0.6475\n",
      "2024-10-21 00:22:26 [INFO]: Epoch 015 - training loss: 0.6514\n",
      "2024-10-21 00:22:29 [INFO]: Epoch 016 - training loss: 0.6516\n",
      "2024-10-21 00:22:32 [INFO]: Epoch 017 - training loss: 0.6481\n",
      "2024-10-21 00:22:35 [INFO]: Epoch 018 - training loss: 0.6511\n",
      "2024-10-21 00:22:37 [INFO]: Epoch 019 - training loss: 0.6525\n",
      "2024-10-21 00:22:40 [INFO]: Epoch 020 - training loss: 0.6491\n",
      "2024-10-21 00:22:43 [INFO]: Epoch 021 - training loss: 0.6376\n",
      "2024-10-21 00:22:46 [INFO]: Epoch 022 - training loss: 0.6494\n",
      "2024-10-21 00:22:49 [INFO]: Epoch 023 - training loss: 0.6442\n",
      "2024-10-21 00:22:51 [INFO]: Epoch 024 - training loss: 0.6434\n",
      "2024-10-21 00:22:54 [INFO]: Epoch 025 - training loss: 0.6496\n",
      "2024-10-21 00:22:57 [INFO]: Epoch 026 - training loss: 0.6492\n",
      "2024-10-21 00:23:00 [INFO]: Epoch 027 - training loss: 0.6484\n",
      "2024-10-21 00:23:03 [INFO]: Epoch 028 - training loss: 0.6499\n",
      "2024-10-21 00:23:05 [INFO]: Epoch 029 - training loss: 0.6553\n",
      "2024-10-21 00:23:08 [INFO]: Epoch 030 - training loss: 0.6409\n",
      "2024-10-21 00:23:11 [INFO]: Epoch 031 - training loss: 0.6402\n",
      "2024-10-21 00:23:14 [INFO]: Epoch 032 - training loss: 0.6489\n",
      "2024-10-21 00:23:17 [INFO]: Epoch 033 - training loss: 0.6434\n",
      "2024-10-21 00:23:20 [INFO]: Epoch 034 - training loss: 0.6418\n",
      "2024-10-21 00:23:22 [INFO]: Epoch 035 - training loss: 0.6562\n",
      "2024-10-21 00:23:25 [INFO]: Epoch 036 - training loss: 0.6470\n",
      "2024-10-21 00:23:28 [INFO]: Epoch 037 - training loss: 0.6440\n",
      "2024-10-21 00:23:31 [INFO]: Epoch 038 - training loss: 0.6556\n",
      "2024-10-21 00:23:34 [INFO]: Epoch 039 - training loss: 0.6438\n",
      "2024-10-21 00:23:36 [INFO]: Epoch 040 - training loss: 0.6453\n",
      "2024-10-21 00:23:39 [INFO]: Epoch 041 - training loss: 0.6475\n",
      "2024-10-21 00:23:42 [INFO]: Epoch 042 - training loss: 0.6368\n",
      "2024-10-21 00:23:45 [INFO]: Epoch 043 - training loss: 0.6426\n",
      "2024-10-21 00:23:48 [INFO]: Epoch 044 - training loss: 0.6354\n",
      "2024-10-21 00:23:51 [INFO]: Epoch 045 - training loss: 0.6340\n",
      "2024-10-21 00:23:53 [INFO]: Epoch 046 - training loss: 0.6419\n",
      "2024-10-21 00:23:56 [INFO]: Epoch 047 - training loss: 0.6517\n",
      "2024-10-21 00:23:59 [INFO]: Epoch 048 - training loss: 0.6421\n",
      "2024-10-21 00:24:02 [INFO]: Epoch 049 - training loss: 0.6393\n",
      "2024-10-21 00:24:05 [INFO]: Epoch 050 - training loss: 0.6461\n",
      "2024-10-21 00:24:08 [INFO]: Epoch 051 - training loss: 0.6456\n",
      "2024-10-21 00:24:10 [INFO]: Epoch 052 - training loss: 0.6522\n",
      "2024-10-21 00:24:13 [INFO]: Epoch 053 - training loss: 0.6476\n",
      "2024-10-21 00:24:16 [INFO]: Epoch 054 - training loss: 0.6517\n",
      "2024-10-21 00:24:19 [INFO]: Epoch 055 - training loss: 0.6381\n",
      "2024-10-21 00:24:22 [INFO]: Epoch 056 - training loss: 0.6464\n",
      "2024-10-21 00:24:25 [INFO]: Epoch 057 - training loss: 0.6430\n",
      "2024-10-21 00:24:27 [INFO]: Epoch 058 - training loss: 0.6542\n",
      "2024-10-21 00:24:30 [INFO]: Epoch 059 - training loss: 0.6442\n",
      "2024-10-21 00:24:33 [INFO]: Epoch 060 - training loss: 0.6402\n",
      "2024-10-21 00:24:36 [INFO]: Epoch 061 - training loss: 0.6457\n",
      "2024-10-21 00:24:39 [INFO]: Epoch 062 - training loss: 0.6458\n",
      "2024-10-21 00:24:41 [INFO]: Epoch 063 - training loss: 0.6553\n",
      "2024-10-21 00:24:44 [INFO]: Epoch 064 - training loss: 0.6400\n",
      "2024-10-21 00:24:47 [INFO]: Epoch 065 - training loss: 0.6426\n",
      "2024-10-21 00:24:50 [INFO]: Epoch 066 - training loss: 0.6483\n",
      "2024-10-21 00:24:53 [INFO]: Epoch 067 - training loss: 0.6562\n",
      "2024-10-21 00:24:55 [INFO]: Epoch 068 - training loss: 0.6473\n",
      "2024-10-21 00:24:58 [INFO]: Epoch 069 - training loss: 0.6449\n",
      "2024-10-21 00:25:01 [INFO]: Epoch 070 - training loss: 0.6453\n",
      "2024-10-21 00:25:04 [INFO]: Epoch 071 - training loss: 0.6432\n",
      "2024-10-21 00:25:07 [INFO]: Epoch 072 - training loss: 0.6445\n",
      "2024-10-21 00:25:10 [INFO]: Epoch 073 - training loss: 0.6366\n",
      "2024-10-21 00:25:13 [INFO]: Epoch 074 - training loss: 0.6479\n",
      "2024-10-21 00:25:15 [INFO]: Epoch 075 - training loss: 0.6424\n",
      "2024-10-21 00:25:18 [INFO]: Epoch 076 - training loss: 0.6360\n",
      "2024-10-21 00:25:21 [INFO]: Epoch 077 - training loss: 0.6473\n",
      "2024-10-21 00:25:24 [INFO]: Epoch 078 - training loss: 0.6411\n",
      "2024-10-21 00:25:27 [INFO]: Epoch 079 - training loss: 0.6483\n",
      "2024-10-21 00:25:30 [INFO]: Epoch 080 - training loss: 0.6297\n",
      "2024-10-21 00:25:32 [INFO]: Epoch 081 - training loss: 0.6418\n",
      "2024-10-21 00:25:35 [INFO]: Epoch 082 - training loss: 0.6488\n",
      "2024-10-21 00:25:38 [INFO]: Epoch 083 - training loss: 0.6410\n",
      "2024-10-21 00:25:41 [INFO]: Epoch 084 - training loss: 0.6438\n",
      "2024-10-21 00:25:44 [INFO]: Epoch 085 - training loss: 0.6334\n",
      "2024-10-21 00:25:46 [INFO]: Epoch 086 - training loss: 0.6374\n",
      "2024-10-21 00:25:49 [INFO]: Epoch 087 - training loss: 0.6523\n",
      "2024-10-21 00:25:52 [INFO]: Epoch 088 - training loss: 0.6398\n",
      "2024-10-21 00:25:55 [INFO]: Epoch 089 - training loss: 0.6421\n",
      "2024-10-21 00:25:58 [INFO]: Epoch 090 - training loss: 0.6376\n",
      "2024-10-21 00:26:00 [INFO]: Epoch 091 - training loss: 0.6357\n",
      "2024-10-21 00:26:03 [INFO]: Epoch 092 - training loss: 0.6392\n",
      "2024-10-21 00:26:06 [INFO]: Epoch 093 - training loss: 0.6500\n",
      "2024-10-21 00:26:09 [INFO]: Epoch 094 - training loss: 0.6511\n",
      "2024-10-21 00:26:12 [INFO]: Epoch 095 - training loss: 0.6512\n",
      "2024-10-21 00:26:14 [INFO]: Epoch 096 - training loss: 0.6443\n",
      "2024-10-21 00:26:17 [INFO]: Epoch 097 - training loss: 0.6538\n",
      "2024-10-21 00:26:20 [INFO]: Epoch 098 - training loss: 0.6421\n",
      "2024-10-21 00:26:23 [INFO]: Epoch 099 - training loss: 0.6447\n",
      "2024-10-21 00:26:26 [INFO]: Epoch 100 - training loss: 0.6375\n",
      "2024-10-21 00:26:26 [INFO]: Finished training. The best model is from epoch#80.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 9/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 00:31:42 [INFO]: Epoch 001 - training loss: 0.6516\n",
      "2024-10-21 00:31:45 [INFO]: Epoch 002 - training loss: 0.6468\n",
      "2024-10-21 00:31:48 [INFO]: Epoch 003 - training loss: 0.6431\n",
      "2024-10-21 00:31:50 [INFO]: Epoch 004 - training loss: 0.6436\n",
      "2024-10-21 00:31:53 [INFO]: Epoch 005 - training loss: 0.6446\n",
      "2024-10-21 00:31:56 [INFO]: Epoch 006 - training loss: 0.6465\n",
      "2024-10-21 00:31:59 [INFO]: Epoch 007 - training loss: 0.6433\n",
      "2024-10-21 00:32:02 [INFO]: Epoch 008 - training loss: 0.6450\n",
      "2024-10-21 00:32:05 [INFO]: Epoch 009 - training loss: 0.6453\n",
      "2024-10-21 00:32:07 [INFO]: Epoch 010 - training loss: 0.6380\n",
      "2024-10-21 00:32:10 [INFO]: Epoch 011 - training loss: 0.6421\n",
      "2024-10-21 00:32:13 [INFO]: Epoch 012 - training loss: 0.6432\n",
      "2024-10-21 00:32:16 [INFO]: Epoch 013 - training loss: 0.6385\n",
      "2024-10-21 00:32:19 [INFO]: Epoch 014 - training loss: 0.6447\n",
      "2024-10-21 00:32:21 [INFO]: Epoch 015 - training loss: 0.6507\n",
      "2024-10-21 00:32:24 [INFO]: Epoch 016 - training loss: 0.6475\n",
      "2024-10-21 00:32:27 [INFO]: Epoch 017 - training loss: 0.6472\n",
      "2024-10-21 00:32:30 [INFO]: Epoch 018 - training loss: 0.6392\n",
      "2024-10-21 00:32:33 [INFO]: Epoch 019 - training loss: 0.6371\n",
      "2024-10-21 00:32:36 [INFO]: Epoch 020 - training loss: 0.6468\n",
      "2024-10-21 00:32:38 [INFO]: Epoch 021 - training loss: 0.6448\n",
      "2024-10-21 00:32:41 [INFO]: Epoch 022 - training loss: 0.6444\n",
      "2024-10-21 00:32:44 [INFO]: Epoch 023 - training loss: 0.6442\n",
      "2024-10-21 00:32:47 [INFO]: Epoch 024 - training loss: 0.6509\n",
      "2024-10-21 00:32:49 [INFO]: Epoch 025 - training loss: 0.6469\n",
      "2024-10-21 00:32:52 [INFO]: Epoch 026 - training loss: 0.6491\n",
      "2024-10-21 00:32:55 [INFO]: Epoch 027 - training loss: 0.6612\n",
      "2024-10-21 00:32:58 [INFO]: Epoch 028 - training loss: 0.6381\n",
      "2024-10-21 00:33:01 [INFO]: Epoch 029 - training loss: 0.6447\n",
      "2024-10-21 00:33:03 [INFO]: Epoch 030 - training loss: 0.6428\n",
      "2024-10-21 00:33:06 [INFO]: Epoch 031 - training loss: 0.6374\n",
      "2024-10-21 00:33:09 [INFO]: Epoch 032 - training loss: 0.6392\n",
      "2024-10-21 00:33:12 [INFO]: Epoch 033 - training loss: 0.6409\n",
      "2024-10-21 00:33:15 [INFO]: Epoch 034 - training loss: 0.6398\n",
      "2024-10-21 00:33:17 [INFO]: Epoch 035 - training loss: 0.6400\n",
      "2024-10-21 00:33:20 [INFO]: Epoch 036 - training loss: 0.6438\n",
      "2024-10-21 00:33:23 [INFO]: Epoch 037 - training loss: 0.6392\n",
      "2024-10-21 00:33:26 [INFO]: Epoch 038 - training loss: 0.6454\n",
      "2024-10-21 00:33:29 [INFO]: Epoch 039 - training loss: 0.6549\n",
      "2024-10-21 00:33:31 [INFO]: Epoch 040 - training loss: 0.6440\n",
      "2024-10-21 00:33:34 [INFO]: Epoch 041 - training loss: 0.6363\n",
      "2024-10-21 00:33:37 [INFO]: Epoch 042 - training loss: 0.6353\n",
      "2024-10-21 00:33:40 [INFO]: Epoch 043 - training loss: 0.6422\n",
      "2024-10-21 00:33:43 [INFO]: Epoch 044 - training loss: 0.6340\n",
      "2024-10-21 00:33:45 [INFO]: Epoch 045 - training loss: 0.6387\n",
      "2024-10-21 00:33:48 [INFO]: Epoch 046 - training loss: 0.6326\n",
      "2024-10-21 00:33:51 [INFO]: Epoch 047 - training loss: 0.6403\n",
      "2024-10-21 00:33:54 [INFO]: Epoch 048 - training loss: 0.6325\n",
      "2024-10-21 00:33:57 [INFO]: Epoch 049 - training loss: 0.6355\n",
      "2024-10-21 00:33:59 [INFO]: Epoch 050 - training loss: 0.6330\n",
      "2024-10-21 00:34:02 [INFO]: Epoch 051 - training loss: 0.6362\n",
      "2024-10-21 00:34:05 [INFO]: Epoch 052 - training loss: 0.6302\n",
      "2024-10-21 00:34:08 [INFO]: Epoch 053 - training loss: 0.6346\n",
      "2024-10-21 00:34:11 [INFO]: Epoch 054 - training loss: 0.6446\n",
      "2024-10-21 00:34:13 [INFO]: Epoch 055 - training loss: 0.6390\n",
      "2024-10-21 00:34:16 [INFO]: Epoch 056 - training loss: 0.6284\n",
      "2024-10-21 00:34:19 [INFO]: Epoch 057 - training loss: 0.6363\n",
      "2024-10-21 00:34:22 [INFO]: Epoch 058 - training loss: 0.6386\n",
      "2024-10-21 00:34:25 [INFO]: Epoch 059 - training loss: 0.6323\n",
      "2024-10-21 00:34:27 [INFO]: Epoch 060 - training loss: 0.6331\n",
      "2024-10-21 00:34:30 [INFO]: Epoch 061 - training loss: 0.6394\n",
      "2024-10-21 00:34:33 [INFO]: Epoch 062 - training loss: 0.6402\n",
      "2024-10-21 00:34:36 [INFO]: Epoch 063 - training loss: 0.6358\n",
      "2024-10-21 00:34:39 [INFO]: Epoch 064 - training loss: 0.6309\n",
      "2024-10-21 00:34:42 [INFO]: Epoch 065 - training loss: 0.6492\n",
      "2024-10-21 00:34:44 [INFO]: Epoch 066 - training loss: 0.6376\n",
      "2024-10-21 00:34:47 [INFO]: Epoch 067 - training loss: 0.6447\n",
      "2024-10-21 00:34:50 [INFO]: Epoch 068 - training loss: 0.6418\n",
      "2024-10-21 00:34:53 [INFO]: Epoch 069 - training loss: 0.6366\n",
      "2024-10-21 00:34:56 [INFO]: Epoch 070 - training loss: 0.6475\n",
      "2024-10-21 00:34:58 [INFO]: Epoch 071 - training loss: 0.6410\n",
      "2024-10-21 00:35:01 [INFO]: Epoch 072 - training loss: 0.6483\n",
      "2024-10-21 00:35:04 [INFO]: Epoch 073 - training loss: 0.6477\n",
      "2024-10-21 00:35:07 [INFO]: Epoch 074 - training loss: 0.6381\n",
      "2024-10-21 00:35:10 [INFO]: Epoch 075 - training loss: 0.6334\n",
      "2024-10-21 00:35:12 [INFO]: Epoch 076 - training loss: 0.6329\n",
      "2024-10-21 00:35:15 [INFO]: Epoch 077 - training loss: 0.6474\n",
      "2024-10-21 00:35:18 [INFO]: Epoch 078 - training loss: 0.6401\n",
      "2024-10-21 00:35:21 [INFO]: Epoch 079 - training loss: 0.6422\n",
      "2024-10-21 00:35:24 [INFO]: Epoch 080 - training loss: 0.6333\n",
      "2024-10-21 00:35:26 [INFO]: Epoch 081 - training loss: 0.6339\n",
      "2024-10-21 00:35:29 [INFO]: Epoch 082 - training loss: 0.6395\n",
      "2024-10-21 00:35:32 [INFO]: Epoch 083 - training loss: 0.6338\n",
      "2024-10-21 00:35:35 [INFO]: Epoch 084 - training loss: 0.6389\n",
      "2024-10-21 00:35:38 [INFO]: Epoch 085 - training loss: 0.6300\n",
      "2024-10-21 00:35:41 [INFO]: Epoch 086 - training loss: 0.6402\n",
      "2024-10-21 00:35:43 [INFO]: Epoch 087 - training loss: 0.6336\n",
      "2024-10-21 00:35:46 [INFO]: Epoch 088 - training loss: 0.6334\n",
      "2024-10-21 00:35:49 [INFO]: Epoch 089 - training loss: 0.6325\n",
      "2024-10-21 00:35:52 [INFO]: Epoch 090 - training loss: 0.6282\n",
      "2024-10-21 00:35:55 [INFO]: Epoch 091 - training loss: 0.6287\n",
      "2024-10-21 00:35:57 [INFO]: Epoch 092 - training loss: 0.6306\n",
      "2024-10-21 00:36:00 [INFO]: Epoch 093 - training loss: 0.6350\n",
      "2024-10-21 00:36:03 [INFO]: Epoch 094 - training loss: 0.6343\n",
      "2024-10-21 00:36:06 [INFO]: Epoch 095 - training loss: 0.6419\n",
      "2024-10-21 00:36:09 [INFO]: Epoch 096 - training loss: 0.6505\n",
      "2024-10-21 00:36:12 [INFO]: Epoch 097 - training loss: 0.6391\n",
      "2024-10-21 00:36:14 [INFO]: Epoch 098 - training loss: 0.6311\n",
      "2024-10-21 00:36:17 [INFO]: Epoch 099 - training loss: 0.6368\n",
      "2024-10-21 00:36:20 [INFO]: Epoch 100 - training loss: 0.6400\n",
      "2024-10-21 00:36:20 [INFO]: Finished training. The best model is from epoch#90.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 10/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 00:41:37 [INFO]: Epoch 001 - training loss: 0.6510\n",
      "2024-10-21 00:41:40 [INFO]: Epoch 002 - training loss: 0.6422\n",
      "2024-10-21 00:41:43 [INFO]: Epoch 003 - training loss: 0.6492\n",
      "2024-10-21 00:41:46 [INFO]: Epoch 004 - training loss: 0.6528\n",
      "2024-10-21 00:41:49 [INFO]: Epoch 005 - training loss: 0.6518\n",
      "2024-10-21 00:41:52 [INFO]: Epoch 006 - training loss: 0.6406\n",
      "2024-10-21 00:41:54 [INFO]: Epoch 007 - training loss: 0.6556\n",
      "2024-10-21 00:41:57 [INFO]: Epoch 008 - training loss: 0.6450\n",
      "2024-10-21 00:42:00 [INFO]: Epoch 009 - training loss: 0.6436\n",
      "2024-10-21 00:42:03 [INFO]: Epoch 010 - training loss: 0.6484\n",
      "2024-10-21 00:42:06 [INFO]: Epoch 011 - training loss: 0.6368\n",
      "2024-10-21 00:42:08 [INFO]: Epoch 012 - training loss: 0.6424\n",
      "2024-10-21 00:42:11 [INFO]: Epoch 013 - training loss: 0.6380\n",
      "2024-10-21 00:42:14 [INFO]: Epoch 014 - training loss: 0.6504\n",
      "2024-10-21 00:42:17 [INFO]: Epoch 015 - training loss: 0.6584\n",
      "2024-10-21 00:42:20 [INFO]: Epoch 016 - training loss: 0.6382\n",
      "2024-10-21 00:42:23 [INFO]: Epoch 017 - training loss: 0.6496\n",
      "2024-10-21 00:42:25 [INFO]: Epoch 018 - training loss: 0.6427\n",
      "2024-10-21 00:42:28 [INFO]: Epoch 019 - training loss: 0.6438\n",
      "2024-10-21 00:42:31 [INFO]: Epoch 020 - training loss: 0.6463\n",
      "2024-10-21 00:42:34 [INFO]: Epoch 021 - training loss: 0.6453\n",
      "2024-10-21 00:42:37 [INFO]: Epoch 022 - training loss: 0.6470\n",
      "2024-10-21 00:42:39 [INFO]: Epoch 023 - training loss: 0.6375\n",
      "2024-10-21 00:42:42 [INFO]: Epoch 024 - training loss: 0.6400\n",
      "2024-10-21 00:42:45 [INFO]: Epoch 025 - training loss: 0.6393\n",
      "2024-10-21 00:42:48 [INFO]: Epoch 026 - training loss: 0.6355\n",
      "2024-10-21 00:42:51 [INFO]: Epoch 027 - training loss: 0.6293\n",
      "2024-10-21 00:42:53 [INFO]: Epoch 028 - training loss: 0.6387\n",
      "2024-10-21 00:42:56 [INFO]: Epoch 029 - training loss: 0.6416\n",
      "2024-10-21 00:42:59 [INFO]: Epoch 030 - training loss: 0.6413\n",
      "2024-10-21 00:43:02 [INFO]: Epoch 031 - training loss: 0.6397\n",
      "2024-10-21 00:43:04 [INFO]: Epoch 032 - training loss: 0.6414\n",
      "2024-10-21 00:43:07 [INFO]: Epoch 033 - training loss: 0.6399\n",
      "2024-10-21 00:43:10 [INFO]: Epoch 034 - training loss: 0.6365\n",
      "2024-10-21 00:43:13 [INFO]: Epoch 035 - training loss: 0.6331\n",
      "2024-10-21 00:43:16 [INFO]: Epoch 036 - training loss: 0.6348\n",
      "2024-10-21 00:43:18 [INFO]: Epoch 037 - training loss: 0.6327\n",
      "2024-10-21 00:43:21 [INFO]: Epoch 038 - training loss: 0.6404\n",
      "2024-10-21 00:43:24 [INFO]: Epoch 039 - training loss: 0.6405\n",
      "2024-10-21 00:43:27 [INFO]: Epoch 040 - training loss: 0.6349\n",
      "2024-10-21 00:43:30 [INFO]: Epoch 041 - training loss: 0.6413\n",
      "2024-10-21 00:43:33 [INFO]: Epoch 042 - training loss: 0.6422\n",
      "2024-10-21 00:43:35 [INFO]: Epoch 043 - training loss: 0.6468\n",
      "2024-10-21 00:43:38 [INFO]: Epoch 044 - training loss: 0.6454\n",
      "2024-10-21 00:43:41 [INFO]: Epoch 045 - training loss: 0.6312\n",
      "2024-10-21 00:43:44 [INFO]: Epoch 046 - training loss: 0.6425\n",
      "2024-10-21 00:43:47 [INFO]: Epoch 047 - training loss: 0.6424\n",
      "2024-10-21 00:43:49 [INFO]: Epoch 048 - training loss: 0.6368\n",
      "2024-10-21 00:43:52 [INFO]: Epoch 049 - training loss: 0.6399\n",
      "2024-10-21 00:43:55 [INFO]: Epoch 050 - training loss: 0.6456\n",
      "2024-10-21 00:43:58 [INFO]: Epoch 051 - training loss: 0.6298\n",
      "2024-10-21 00:44:01 [INFO]: Epoch 052 - training loss: 0.6346\n",
      "2024-10-21 00:44:03 [INFO]: Epoch 053 - training loss: 0.6441\n",
      "2024-10-21 00:44:06 [INFO]: Epoch 054 - training loss: 0.6355\n",
      "2024-10-21 00:44:09 [INFO]: Epoch 055 - training loss: 0.6408\n",
      "2024-10-21 00:44:12 [INFO]: Epoch 056 - training loss: 0.6439\n",
      "2024-10-21 00:44:15 [INFO]: Epoch 057 - training loss: 0.6341\n",
      "2024-10-21 00:44:18 [INFO]: Epoch 058 - training loss: 0.6350\n",
      "2024-10-21 00:44:20 [INFO]: Epoch 059 - training loss: 0.6355\n",
      "2024-10-21 00:44:23 [INFO]: Epoch 060 - training loss: 0.6321\n",
      "2024-10-21 00:44:26 [INFO]: Epoch 061 - training loss: 0.6343\n",
      "2024-10-21 00:44:29 [INFO]: Epoch 062 - training loss: 0.6251\n",
      "2024-10-21 00:44:32 [INFO]: Epoch 063 - training loss: 0.6303\n",
      "2024-10-21 00:44:34 [INFO]: Epoch 064 - training loss: 0.6323\n",
      "2024-10-21 00:44:37 [INFO]: Epoch 065 - training loss: 0.6350\n",
      "2024-10-21 00:44:40 [INFO]: Epoch 066 - training loss: 0.6400\n",
      "2024-10-21 00:44:43 [INFO]: Epoch 067 - training loss: 0.6275\n",
      "2024-10-21 00:44:46 [INFO]: Epoch 068 - training loss: 0.6351\n",
      "2024-10-21 00:44:48 [INFO]: Epoch 069 - training loss: 0.6277\n",
      "2024-10-21 00:44:51 [INFO]: Epoch 070 - training loss: 0.6353\n",
      "2024-10-21 00:44:54 [INFO]: Epoch 071 - training loss: 0.6354\n",
      "2024-10-21 00:44:57 [INFO]: Epoch 072 - training loss: 0.6262\n",
      "2024-10-21 00:45:00 [INFO]: Epoch 073 - training loss: 0.6275\n",
      "2024-10-21 00:45:02 [INFO]: Epoch 074 - training loss: 0.6384\n",
      "2024-10-21 00:45:05 [INFO]: Epoch 075 - training loss: 0.6358\n",
      "2024-10-21 00:45:08 [INFO]: Epoch 076 - training loss: 0.6263\n",
      "2024-10-21 00:45:11 [INFO]: Epoch 077 - training loss: 0.6382\n",
      "2024-10-21 00:45:14 [INFO]: Epoch 078 - training loss: 0.6254\n",
      "2024-10-21 00:45:16 [INFO]: Epoch 079 - training loss: 0.6391\n",
      "2024-10-21 00:45:19 [INFO]: Epoch 080 - training loss: 0.6371\n",
      "2024-10-21 00:45:22 [INFO]: Epoch 081 - training loss: 0.6285\n",
      "2024-10-21 00:45:25 [INFO]: Epoch 082 - training loss: 0.6327\n",
      "2024-10-21 00:45:28 [INFO]: Epoch 083 - training loss: 0.6250\n",
      "2024-10-21 00:45:30 [INFO]: Epoch 084 - training loss: 0.6339\n",
      "2024-10-21 00:45:33 [INFO]: Epoch 085 - training loss: 0.6336\n",
      "2024-10-21 00:45:36 [INFO]: Epoch 086 - training loss: 0.6364\n",
      "2024-10-21 00:45:39 [INFO]: Epoch 087 - training loss: 0.6439\n",
      "2024-10-21 00:45:42 [INFO]: Epoch 088 - training loss: 0.6384\n",
      "2024-10-21 00:45:45 [INFO]: Epoch 089 - training loss: 0.6318\n",
      "2024-10-21 00:45:47 [INFO]: Epoch 090 - training loss: 0.6349\n",
      "2024-10-21 00:45:50 [INFO]: Epoch 091 - training loss: 0.6340\n",
      "2024-10-21 00:45:53 [INFO]: Epoch 092 - training loss: 0.6441\n",
      "2024-10-21 00:45:56 [INFO]: Epoch 093 - training loss: 0.6283\n",
      "2024-10-21 00:45:58 [INFO]: Epoch 094 - training loss: 0.6292\n",
      "2024-10-21 00:46:01 [INFO]: Epoch 095 - training loss: 0.6210\n",
      "2024-10-21 00:46:04 [INFO]: Epoch 096 - training loss: 0.6297\n",
      "2024-10-21 00:46:07 [INFO]: Epoch 097 - training loss: 0.6343\n",
      "2024-10-21 00:46:10 [INFO]: Epoch 098 - training loss: 0.6277\n",
      "2024-10-21 00:46:12 [INFO]: Epoch 099 - training loss: 0.6372\n",
      "2024-10-21 00:46:15 [INFO]: Epoch 100 - training loss: 0.6330\n",
      "2024-10-21 00:46:15 [INFO]: Finished training. The best model is from epoch#95.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 11/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 00:51:32 [INFO]: Epoch 001 - training loss: 0.6544\n",
      "2024-10-21 00:51:35 [INFO]: Epoch 002 - training loss: 0.6496\n",
      "2024-10-21 00:51:38 [INFO]: Epoch 003 - training loss: 0.6420\n",
      "2024-10-21 00:51:41 [INFO]: Epoch 004 - training loss: 0.6478\n",
      "2024-10-21 00:51:44 [INFO]: Epoch 005 - training loss: 0.6396\n",
      "2024-10-21 00:51:46 [INFO]: Epoch 006 - training loss: 0.6425\n",
      "2024-10-21 00:51:49 [INFO]: Epoch 007 - training loss: 0.6372\n",
      "2024-10-21 00:51:52 [INFO]: Epoch 008 - training loss: 0.6456\n",
      "2024-10-21 00:51:55 [INFO]: Epoch 009 - training loss: 0.6367\n",
      "2024-10-21 00:51:58 [INFO]: Epoch 010 - training loss: 0.6709\n",
      "2024-10-21 00:52:01 [INFO]: Epoch 011 - training loss: 0.6442\n",
      "2024-10-21 00:52:03 [INFO]: Epoch 012 - training loss: 0.6437\n",
      "2024-10-21 00:52:06 [INFO]: Epoch 013 - training loss: 0.6475\n",
      "2024-10-21 00:52:09 [INFO]: Epoch 014 - training loss: 0.6365\n",
      "2024-10-21 00:52:12 [INFO]: Epoch 015 - training loss: 0.6425\n",
      "2024-10-21 00:52:15 [INFO]: Epoch 016 - training loss: 0.6298\n",
      "2024-10-21 00:52:17 [INFO]: Epoch 017 - training loss: 0.6483\n",
      "2024-10-21 00:52:20 [INFO]: Epoch 018 - training loss: 0.6501\n",
      "2024-10-21 00:52:23 [INFO]: Epoch 019 - training loss: 0.6377\n",
      "2024-10-21 00:52:26 [INFO]: Epoch 020 - training loss: 0.6336\n",
      "2024-10-21 00:52:29 [INFO]: Epoch 021 - training loss: 0.6461\n",
      "2024-10-21 00:52:31 [INFO]: Epoch 022 - training loss: 0.6409\n",
      "2024-10-21 00:52:34 [INFO]: Epoch 023 - training loss: 0.6280\n",
      "2024-10-21 00:52:37 [INFO]: Epoch 024 - training loss: 0.6381\n",
      "2024-10-21 00:52:40 [INFO]: Epoch 025 - training loss: 0.6371\n",
      "2024-10-21 00:52:43 [INFO]: Epoch 026 - training loss: 0.6319\n",
      "2024-10-21 00:52:45 [INFO]: Epoch 027 - training loss: 0.6324\n",
      "2024-10-21 00:52:48 [INFO]: Epoch 028 - training loss: 0.6360\n",
      "2024-10-21 00:52:51 [INFO]: Epoch 029 - training loss: 0.6421\n",
      "2024-10-21 00:52:54 [INFO]: Epoch 030 - training loss: 0.6344\n",
      "2024-10-21 00:52:57 [INFO]: Epoch 031 - training loss: 0.6482\n",
      "2024-10-21 00:52:59 [INFO]: Epoch 032 - training loss: 0.6395\n",
      "2024-10-21 00:53:02 [INFO]: Epoch 033 - training loss: 0.6333\n",
      "2024-10-21 00:53:05 [INFO]: Epoch 034 - training loss: 0.6267\n",
      "2024-10-21 00:53:08 [INFO]: Epoch 035 - training loss: 0.6382\n",
      "2024-10-21 00:53:11 [INFO]: Epoch 036 - training loss: 0.6257\n",
      "2024-10-21 00:53:13 [INFO]: Epoch 037 - training loss: 0.6266\n",
      "2024-10-21 00:53:16 [INFO]: Epoch 038 - training loss: 0.6285\n",
      "2024-10-21 00:53:19 [INFO]: Epoch 039 - training loss: 0.6276\n",
      "2024-10-21 00:53:22 [INFO]: Epoch 040 - training loss: 0.6212\n",
      "2024-10-21 00:53:25 [INFO]: Epoch 041 - training loss: 0.6338\n",
      "2024-10-21 00:53:27 [INFO]: Epoch 042 - training loss: 0.6290\n",
      "2024-10-21 00:53:30 [INFO]: Epoch 043 - training loss: 0.6419\n",
      "2024-10-21 00:53:33 [INFO]: Epoch 044 - training loss: 0.6256\n",
      "2024-10-21 00:53:36 [INFO]: Epoch 045 - training loss: 0.6410\n",
      "2024-10-21 00:53:39 [INFO]: Epoch 046 - training loss: 0.6238\n",
      "2024-10-21 00:53:42 [INFO]: Epoch 047 - training loss: 0.6297\n",
      "2024-10-21 00:53:44 [INFO]: Epoch 048 - training loss: 0.6281\n",
      "2024-10-21 00:53:47 [INFO]: Epoch 049 - training loss: 0.6286\n",
      "2024-10-21 00:53:50 [INFO]: Epoch 050 - training loss: 0.6370\n",
      "2024-10-21 00:53:53 [INFO]: Epoch 051 - training loss: 0.6277\n",
      "2024-10-21 00:53:56 [INFO]: Epoch 052 - training loss: 0.6404\n",
      "2024-10-21 00:53:58 [INFO]: Epoch 053 - training loss: 0.6261\n",
      "2024-10-21 00:54:01 [INFO]: Epoch 054 - training loss: 0.6291\n",
      "2024-10-21 00:54:04 [INFO]: Epoch 055 - training loss: 0.6286\n",
      "2024-10-21 00:54:07 [INFO]: Epoch 056 - training loss: 0.6316\n",
      "2024-10-21 00:54:10 [INFO]: Epoch 057 - training loss: 0.6553\n",
      "2024-10-21 00:54:13 [INFO]: Epoch 058 - training loss: 0.6344\n",
      "2024-10-21 00:54:15 [INFO]: Epoch 059 - training loss: 0.6298\n",
      "2024-10-21 00:54:18 [INFO]: Epoch 060 - training loss: 0.6374\n",
      "2024-10-21 00:54:21 [INFO]: Epoch 061 - training loss: 0.6392\n",
      "2024-10-21 00:54:24 [INFO]: Epoch 062 - training loss: 0.6278\n",
      "2024-10-21 00:54:26 [INFO]: Epoch 063 - training loss: 0.6276\n",
      "2024-10-21 00:54:29 [INFO]: Epoch 064 - training loss: 0.6330\n",
      "2024-10-21 00:54:32 [INFO]: Epoch 065 - training loss: 0.6229\n",
      "2024-10-21 00:54:35 [INFO]: Epoch 066 - training loss: 0.6265\n",
      "2024-10-21 00:54:38 [INFO]: Epoch 067 - training loss: 0.6276\n",
      "2024-10-21 00:54:40 [INFO]: Epoch 068 - training loss: 0.6673\n",
      "2024-10-21 00:54:43 [INFO]: Epoch 069 - training loss: 0.6318\n",
      "2024-10-21 00:54:46 [INFO]: Epoch 070 - training loss: 0.6288\n",
      "2024-10-21 00:54:49 [INFO]: Epoch 071 - training loss: 0.6295\n",
      "2024-10-21 00:54:52 [INFO]: Epoch 072 - training loss: 0.6381\n",
      "2024-10-21 00:54:55 [INFO]: Epoch 073 - training loss: 0.6267\n",
      "2024-10-21 00:54:57 [INFO]: Epoch 074 - training loss: 0.6344\n",
      "2024-10-21 00:55:00 [INFO]: Epoch 075 - training loss: 0.6293\n",
      "2024-10-21 00:55:03 [INFO]: Epoch 076 - training loss: 0.6362\n",
      "2024-10-21 00:55:06 [INFO]: Epoch 077 - training loss: 0.6274\n",
      "2024-10-21 00:55:09 [INFO]: Epoch 078 - training loss: 0.6305\n",
      "2024-10-21 00:55:11 [INFO]: Epoch 079 - training loss: 0.6336\n",
      "2024-10-21 00:55:14 [INFO]: Epoch 080 - training loss: 0.6335\n",
      "2024-10-21 00:55:17 [INFO]: Epoch 081 - training loss: 0.6274\n",
      "2024-10-21 00:55:20 [INFO]: Epoch 082 - training loss: 0.6324\n",
      "2024-10-21 00:55:23 [INFO]: Epoch 083 - training loss: 0.6276\n",
      "2024-10-21 00:55:25 [INFO]: Epoch 084 - training loss: 0.6292\n",
      "2024-10-21 00:55:28 [INFO]: Epoch 085 - training loss: 0.6335\n",
      "2024-10-21 00:55:31 [INFO]: Epoch 086 - training loss: 0.6259\n",
      "2024-10-21 00:55:34 [INFO]: Epoch 087 - training loss: 0.6295\n",
      "2024-10-21 00:55:37 [INFO]: Epoch 088 - training loss: 0.6359\n",
      "2024-10-21 00:55:39 [INFO]: Epoch 089 - training loss: 0.6311\n",
      "2024-10-21 00:55:42 [INFO]: Epoch 090 - training loss: 0.6281\n",
      "2024-10-21 00:55:45 [INFO]: Epoch 091 - training loss: 0.6266\n",
      "2024-10-21 00:55:48 [INFO]: Epoch 092 - training loss: 0.6306\n",
      "2024-10-21 00:55:50 [INFO]: Epoch 093 - training loss: 0.6252\n",
      "2024-10-21 00:55:53 [INFO]: Epoch 094 - training loss: 0.6315\n",
      "2024-10-21 00:55:56 [INFO]: Epoch 095 - training loss: 0.6207\n",
      "2024-10-21 00:55:58 [INFO]: Epoch 096 - training loss: 0.6269\n",
      "2024-10-21 00:56:01 [INFO]: Epoch 097 - training loss: 0.6203\n",
      "2024-10-21 00:56:04 [INFO]: Epoch 098 - training loss: 0.6296\n",
      "2024-10-21 00:56:06 [INFO]: Epoch 099 - training loss: 0.6282\n",
      "2024-10-21 00:56:09 [INFO]: Epoch 100 - training loss: 0.6293\n",
      "2024-10-21 00:56:09 [INFO]: Finished training. The best model is from epoch#97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 12/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 01:01:27 [INFO]: Epoch 001 - training loss: 0.6499\n",
      "2024-10-21 01:01:30 [INFO]: Epoch 002 - training loss: 0.6356\n",
      "2024-10-21 01:01:32 [INFO]: Epoch 003 - training loss: 0.6530\n",
      "2024-10-21 01:01:35 [INFO]: Epoch 004 - training loss: 0.6507\n",
      "2024-10-21 01:01:38 [INFO]: Epoch 005 - training loss: 0.6377\n",
      "2024-10-21 01:01:41 [INFO]: Epoch 006 - training loss: 0.6493\n",
      "2024-10-21 01:01:44 [INFO]: Epoch 007 - training loss: 0.6377\n",
      "2024-10-21 01:01:46 [INFO]: Epoch 008 - training loss: 0.6391\n",
      "2024-10-21 01:01:49 [INFO]: Epoch 009 - training loss: 0.6407\n",
      "2024-10-21 01:01:52 [INFO]: Epoch 010 - training loss: 0.6269\n",
      "2024-10-21 01:01:55 [INFO]: Epoch 011 - training loss: 0.6386\n",
      "2024-10-21 01:01:58 [INFO]: Epoch 012 - training loss: 0.6289\n",
      "2024-10-21 01:02:00 [INFO]: Epoch 013 - training loss: 0.6358\n",
      "2024-10-21 01:02:03 [INFO]: Epoch 014 - training loss: 0.6317\n",
      "2024-10-21 01:02:06 [INFO]: Epoch 015 - training loss: 0.6378\n",
      "2024-10-21 01:02:09 [INFO]: Epoch 016 - training loss: 0.6385\n",
      "2024-10-21 01:02:12 [INFO]: Epoch 017 - training loss: 0.6300\n",
      "2024-10-21 01:02:14 [INFO]: Epoch 018 - training loss: 0.6333\n",
      "2024-10-21 01:02:17 [INFO]: Epoch 019 - training loss: 0.6358\n",
      "2024-10-21 01:02:20 [INFO]: Epoch 020 - training loss: 0.6349\n",
      "2024-10-21 01:02:23 [INFO]: Epoch 021 - training loss: 0.6355\n",
      "2024-10-21 01:02:25 [INFO]: Epoch 022 - training loss: 0.6448\n",
      "2024-10-21 01:02:28 [INFO]: Epoch 023 - training loss: 0.6397\n",
      "2024-10-21 01:02:31 [INFO]: Epoch 024 - training loss: 0.6339\n",
      "2024-10-21 01:02:33 [INFO]: Epoch 025 - training loss: 0.6345\n",
      "2024-10-21 01:02:36 [INFO]: Epoch 026 - training loss: 0.6419\n",
      "2024-10-21 01:02:39 [INFO]: Epoch 027 - training loss: 0.6371\n",
      "2024-10-21 01:02:41 [INFO]: Epoch 028 - training loss: 0.6260\n",
      "2024-10-21 01:02:44 [INFO]: Epoch 029 - training loss: 0.6349\n",
      "2024-10-21 01:02:47 [INFO]: Epoch 030 - training loss: 0.6380\n",
      "2024-10-21 01:02:50 [INFO]: Epoch 031 - training loss: 0.6269\n",
      "2024-10-21 01:02:53 [INFO]: Epoch 032 - training loss: 0.6304\n",
      "2024-10-21 01:02:55 [INFO]: Epoch 033 - training loss: 0.6297\n",
      "2024-10-21 01:02:58 [INFO]: Epoch 034 - training loss: 0.6316\n",
      "2024-10-21 01:03:01 [INFO]: Epoch 035 - training loss: 0.6266\n",
      "2024-10-21 01:03:04 [INFO]: Epoch 036 - training loss: 0.6313\n",
      "2024-10-21 01:03:07 [INFO]: Epoch 037 - training loss: 0.6379\n",
      "2024-10-21 01:03:10 [INFO]: Epoch 038 - training loss: 0.6409\n",
      "2024-10-21 01:03:12 [INFO]: Epoch 039 - training loss: 0.6296\n",
      "2024-10-21 01:03:15 [INFO]: Epoch 040 - training loss: 0.6204\n",
      "2024-10-21 01:03:18 [INFO]: Epoch 041 - training loss: 0.6235\n",
      "2024-10-21 01:03:21 [INFO]: Epoch 042 - training loss: 0.6302\n",
      "2024-10-21 01:03:24 [INFO]: Epoch 043 - training loss: 0.6320\n",
      "2024-10-21 01:03:26 [INFO]: Epoch 044 - training loss: 0.6313\n",
      "2024-10-21 01:03:29 [INFO]: Epoch 045 - training loss: 0.6222\n",
      "2024-10-21 01:03:32 [INFO]: Epoch 046 - training loss: 0.6356\n",
      "2024-10-21 01:03:35 [INFO]: Epoch 047 - training loss: 0.6312\n",
      "2024-10-21 01:03:38 [INFO]: Epoch 048 - training loss: 0.6238\n",
      "2024-10-21 01:03:40 [INFO]: Epoch 049 - training loss: 0.6309\n",
      "2024-10-21 01:03:43 [INFO]: Epoch 050 - training loss: 0.6220\n",
      "2024-10-21 01:03:46 [INFO]: Epoch 051 - training loss: 0.6228\n",
      "2024-10-21 01:03:49 [INFO]: Epoch 052 - training loss: 0.6313\n",
      "2024-10-21 01:03:52 [INFO]: Epoch 053 - training loss: 0.6233\n",
      "2024-10-21 01:03:54 [INFO]: Epoch 054 - training loss: 0.6211\n",
      "2024-10-21 01:03:57 [INFO]: Epoch 055 - training loss: 0.6254\n",
      "2024-10-21 01:04:00 [INFO]: Epoch 056 - training loss: 0.6239\n",
      "2024-10-21 01:04:03 [INFO]: Epoch 057 - training loss: 0.6261\n",
      "2024-10-21 01:04:06 [INFO]: Epoch 058 - training loss: 0.6473\n",
      "2024-10-21 01:04:09 [INFO]: Epoch 059 - training loss: 0.6298\n",
      "2024-10-21 01:04:11 [INFO]: Epoch 060 - training loss: 0.6253\n",
      "2024-10-21 01:04:14 [INFO]: Epoch 061 - training loss: 0.6244\n",
      "2024-10-21 01:04:17 [INFO]: Epoch 062 - training loss: 0.6376\n",
      "2024-10-21 01:04:20 [INFO]: Epoch 063 - training loss: 0.6341\n",
      "2024-10-21 01:04:23 [INFO]: Epoch 064 - training loss: 0.6322\n",
      "2024-10-21 01:04:25 [INFO]: Epoch 065 - training loss: 0.6170\n",
      "2024-10-21 01:04:28 [INFO]: Epoch 066 - training loss: 0.6292\n",
      "2024-10-21 01:04:31 [INFO]: Epoch 067 - training loss: 0.6259\n",
      "2024-10-21 01:04:34 [INFO]: Epoch 068 - training loss: 0.6180\n",
      "2024-10-21 01:04:37 [INFO]: Epoch 069 - training loss: 0.6289\n",
      "2024-10-21 01:04:39 [INFO]: Epoch 070 - training loss: 0.6147\n",
      "2024-10-21 01:04:42 [INFO]: Epoch 071 - training loss: 0.6194\n",
      "2024-10-21 01:04:45 [INFO]: Epoch 072 - training loss: 0.6290\n",
      "2024-10-21 01:04:48 [INFO]: Epoch 073 - training loss: 0.6273\n",
      "2024-10-21 01:04:51 [INFO]: Epoch 074 - training loss: 0.6336\n",
      "2024-10-21 01:04:53 [INFO]: Epoch 075 - training loss: 0.6189\n",
      "2024-10-21 01:04:56 [INFO]: Epoch 076 - training loss: 0.6218\n",
      "2024-10-21 01:04:59 [INFO]: Epoch 077 - training loss: 0.6281\n",
      "2024-10-21 01:05:02 [INFO]: Epoch 078 - training loss: 0.6257\n",
      "2024-10-21 01:05:05 [INFO]: Epoch 079 - training loss: 0.6170\n",
      "2024-10-21 01:05:07 [INFO]: Epoch 080 - training loss: 0.6288\n",
      "2024-10-21 01:05:10 [INFO]: Epoch 081 - training loss: 0.6319\n",
      "2024-10-21 01:05:13 [INFO]: Epoch 082 - training loss: 0.6252\n",
      "2024-10-21 01:05:16 [INFO]: Epoch 083 - training loss: 0.6193\n",
      "2024-10-21 01:05:19 [INFO]: Epoch 084 - training loss: 0.6306\n",
      "2024-10-21 01:05:21 [INFO]: Epoch 085 - training loss: 0.6268\n",
      "2024-10-21 01:05:24 [INFO]: Epoch 086 - training loss: 0.6306\n",
      "2024-10-21 01:05:27 [INFO]: Epoch 087 - training loss: 0.6264\n",
      "2024-10-21 01:05:30 [INFO]: Epoch 088 - training loss: 0.6262\n",
      "2024-10-21 01:05:33 [INFO]: Epoch 089 - training loss: 0.6207\n",
      "2024-10-21 01:05:35 [INFO]: Epoch 090 - training loss: 0.6253\n",
      "2024-10-21 01:05:38 [INFO]: Epoch 091 - training loss: 0.6300\n",
      "2024-10-21 01:05:41 [INFO]: Epoch 092 - training loss: 0.6220\n",
      "2024-10-21 01:05:44 [INFO]: Epoch 093 - training loss: 0.6221\n",
      "2024-10-21 01:05:47 [INFO]: Epoch 094 - training loss: 0.6135\n",
      "2024-10-21 01:05:50 [INFO]: Epoch 095 - training loss: 0.6186\n",
      "2024-10-21 01:05:52 [INFO]: Epoch 096 - training loss: 0.6179\n",
      "2024-10-21 01:05:55 [INFO]: Epoch 097 - training loss: 0.6220\n",
      "2024-10-21 01:05:58 [INFO]: Epoch 098 - training loss: 0.6273\n",
      "2024-10-21 01:06:01 [INFO]: Epoch 099 - training loss: 0.6305\n",
      "2024-10-21 01:06:04 [INFO]: Epoch 100 - training loss: 0.6253\n",
      "2024-10-21 01:06:04 [INFO]: Finished training. The best model is from epoch#94.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 13/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 01:11:24 [INFO]: Epoch 001 - training loss: 0.6424\n",
      "2024-10-21 01:11:27 [INFO]: Epoch 002 - training loss: 0.6555\n",
      "2024-10-21 01:11:29 [INFO]: Epoch 003 - training loss: 0.6590\n",
      "2024-10-21 01:11:32 [INFO]: Epoch 004 - training loss: 0.6471\n",
      "2024-10-21 01:11:35 [INFO]: Epoch 005 - training loss: 0.6489\n",
      "2024-10-21 01:11:38 [INFO]: Epoch 006 - training loss: 0.6483\n",
      "2024-10-21 01:11:41 [INFO]: Epoch 007 - training loss: 0.6500\n",
      "2024-10-21 01:11:43 [INFO]: Epoch 008 - training loss: 0.6505\n",
      "2024-10-21 01:11:46 [INFO]: Epoch 009 - training loss: 0.6514\n",
      "2024-10-21 01:11:49 [INFO]: Epoch 010 - training loss: 0.6492\n",
      "2024-10-21 01:11:52 [INFO]: Epoch 011 - training loss: 0.6444\n",
      "2024-10-21 01:11:55 [INFO]: Epoch 012 - training loss: 0.6474\n",
      "2024-10-21 01:11:58 [INFO]: Epoch 013 - training loss: 0.6636\n",
      "2024-10-21 01:12:00 [INFO]: Epoch 014 - training loss: 0.6365\n",
      "2024-10-21 01:12:03 [INFO]: Epoch 015 - training loss: 0.6441\n",
      "2024-10-21 01:12:06 [INFO]: Epoch 016 - training loss: 0.6498\n",
      "2024-10-21 01:12:09 [INFO]: Epoch 017 - training loss: 0.6430\n",
      "2024-10-21 01:12:12 [INFO]: Epoch 018 - training loss: 0.6471\n",
      "2024-10-21 01:12:15 [INFO]: Epoch 019 - training loss: 0.6358\n",
      "2024-10-21 01:12:17 [INFO]: Epoch 020 - training loss: 0.6313\n",
      "2024-10-21 01:12:20 [INFO]: Epoch 021 - training loss: 0.6401\n",
      "2024-10-21 01:12:23 [INFO]: Epoch 022 - training loss: 0.6382\n",
      "2024-10-21 01:12:26 [INFO]: Epoch 023 - training loss: 0.6349\n",
      "2024-10-21 01:12:29 [INFO]: Epoch 024 - training loss: 0.6422\n",
      "2024-10-21 01:12:32 [INFO]: Epoch 025 - training loss: 0.6434\n",
      "2024-10-21 01:12:34 [INFO]: Epoch 026 - training loss: 0.6314\n",
      "2024-10-21 01:12:37 [INFO]: Epoch 027 - training loss: 0.6400\n",
      "2024-10-21 01:12:40 [INFO]: Epoch 028 - training loss: 0.6311\n",
      "2024-10-21 01:12:43 [INFO]: Epoch 029 - training loss: 0.6306\n",
      "2024-10-21 01:12:46 [INFO]: Epoch 030 - training loss: 0.6374\n",
      "2024-10-21 01:12:49 [INFO]: Epoch 031 - training loss: 0.6401\n",
      "2024-10-21 01:12:51 [INFO]: Epoch 032 - training loss: 0.6403\n",
      "2024-10-21 01:12:54 [INFO]: Epoch 033 - training loss: 0.6378\n",
      "2024-10-21 01:12:57 [INFO]: Epoch 034 - training loss: 0.6434\n",
      "2024-10-21 01:13:00 [INFO]: Epoch 035 - training loss: 0.6345\n",
      "2024-10-21 01:13:03 [INFO]: Epoch 036 - training loss: 0.6407\n",
      "2024-10-21 01:13:06 [INFO]: Epoch 037 - training loss: 0.6327\n",
      "2024-10-21 01:13:08 [INFO]: Epoch 038 - training loss: 0.6362\n",
      "2024-10-21 01:13:11 [INFO]: Epoch 039 - training loss: 0.6389\n",
      "2024-10-21 01:13:14 [INFO]: Epoch 040 - training loss: 0.6324\n",
      "2024-10-21 01:13:17 [INFO]: Epoch 041 - training loss: 0.6394\n",
      "2024-10-21 01:13:20 [INFO]: Epoch 042 - training loss: 0.6458\n",
      "2024-10-21 01:13:22 [INFO]: Epoch 043 - training loss: 0.6602\n",
      "2024-10-21 01:13:25 [INFO]: Epoch 044 - training loss: 0.6373\n",
      "2024-10-21 01:13:28 [INFO]: Epoch 045 - training loss: 0.6401\n",
      "2024-10-21 01:13:31 [INFO]: Epoch 046 - training loss: 0.6454\n",
      "2024-10-21 01:13:34 [INFO]: Epoch 047 - training loss: 0.6448\n",
      "2024-10-21 01:13:37 [INFO]: Epoch 048 - training loss: 0.6445\n",
      "2024-10-21 01:13:39 [INFO]: Epoch 049 - training loss: 0.6513\n",
      "2024-10-21 01:13:42 [INFO]: Epoch 050 - training loss: 0.6409\n",
      "2024-10-21 01:13:45 [INFO]: Epoch 051 - training loss: 0.6441\n",
      "2024-10-21 01:13:48 [INFO]: Epoch 052 - training loss: 0.6386\n",
      "2024-10-21 01:13:51 [INFO]: Epoch 053 - training loss: 0.6351\n",
      "2024-10-21 01:13:54 [INFO]: Epoch 054 - training loss: 0.6411\n",
      "2024-10-21 01:13:56 [INFO]: Epoch 055 - training loss: 0.6390\n",
      "2024-10-21 01:13:59 [INFO]: Epoch 056 - training loss: 0.6357\n",
      "2024-10-21 01:14:02 [INFO]: Epoch 057 - training loss: 0.6380\n",
      "2024-10-21 01:14:05 [INFO]: Epoch 058 - training loss: 0.6380\n",
      "2024-10-21 01:14:08 [INFO]: Epoch 059 - training loss: 0.6424\n",
      "2024-10-21 01:14:11 [INFO]: Epoch 060 - training loss: 0.6424\n",
      "2024-10-21 01:14:13 [INFO]: Epoch 061 - training loss: 0.6305\n",
      "2024-10-21 01:14:16 [INFO]: Epoch 062 - training loss: 0.6336\n",
      "2024-10-21 01:14:19 [INFO]: Epoch 063 - training loss: 0.6339\n",
      "2024-10-21 01:14:22 [INFO]: Epoch 064 - training loss: 0.6363\n",
      "2024-10-21 01:14:25 [INFO]: Epoch 065 - training loss: 0.6332\n",
      "2024-10-21 01:14:28 [INFO]: Epoch 066 - training loss: 0.6431\n",
      "2024-10-21 01:14:31 [INFO]: Epoch 067 - training loss: 0.6364\n",
      "2024-10-21 01:14:33 [INFO]: Epoch 068 - training loss: 0.6302\n",
      "2024-10-21 01:14:36 [INFO]: Epoch 069 - training loss: 0.6387\n",
      "2024-10-21 01:14:39 [INFO]: Epoch 070 - training loss: 0.6392\n",
      "2024-10-21 01:14:42 [INFO]: Epoch 071 - training loss: 0.6302\n",
      "2024-10-21 01:14:45 [INFO]: Epoch 072 - training loss: 0.6344\n",
      "2024-10-21 01:14:48 [INFO]: Epoch 073 - training loss: 0.6341\n",
      "2024-10-21 01:14:50 [INFO]: Epoch 074 - training loss: 0.6317\n",
      "2024-10-21 01:14:53 [INFO]: Epoch 075 - training loss: 0.6299\n",
      "2024-10-21 01:14:56 [INFO]: Epoch 076 - training loss: 0.6355\n",
      "2024-10-21 01:14:59 [INFO]: Epoch 077 - training loss: 0.6263\n",
      "2024-10-21 01:15:02 [INFO]: Epoch 078 - training loss: 0.6529\n",
      "2024-10-21 01:15:05 [INFO]: Epoch 079 - training loss: 0.6272\n",
      "2024-10-21 01:15:07 [INFO]: Epoch 080 - training loss: 0.6263\n",
      "2024-10-21 01:15:10 [INFO]: Epoch 081 - training loss: 0.6388\n",
      "2024-10-21 01:15:13 [INFO]: Epoch 082 - training loss: 0.6272\n",
      "2024-10-21 01:15:16 [INFO]: Epoch 083 - training loss: 0.6326\n",
      "2024-10-21 01:15:19 [INFO]: Epoch 084 - training loss: 0.6321\n",
      "2024-10-21 01:15:22 [INFO]: Epoch 085 - training loss: 0.6556\n",
      "2024-10-21 01:15:24 [INFO]: Epoch 086 - training loss: 0.6362\n",
      "2024-10-21 01:15:27 [INFO]: Epoch 087 - training loss: 0.6304\n",
      "2024-10-21 01:15:30 [INFO]: Epoch 088 - training loss: 0.6272\n",
      "2024-10-21 01:15:33 [INFO]: Epoch 089 - training loss: 0.6274\n",
      "2024-10-21 01:15:36 [INFO]: Epoch 090 - training loss: 0.6472\n",
      "2024-10-21 01:15:39 [INFO]: Epoch 091 - training loss: 0.6331\n",
      "2024-10-21 01:15:41 [INFO]: Epoch 092 - training loss: 0.6238\n",
      "2024-10-21 01:15:44 [INFO]: Epoch 093 - training loss: 0.6301\n",
      "2024-10-21 01:15:47 [INFO]: Epoch 094 - training loss: 0.6346\n",
      "2024-10-21 01:15:50 [INFO]: Epoch 095 - training loss: 0.6372\n",
      "2024-10-21 01:15:53 [INFO]: Epoch 096 - training loss: 0.6336\n",
      "2024-10-21 01:15:56 [INFO]: Epoch 097 - training loss: 0.6439\n",
      "2024-10-21 01:15:58 [INFO]: Epoch 098 - training loss: 0.6420\n",
      "2024-10-21 01:16:01 [INFO]: Epoch 099 - training loss: 0.6409\n",
      "2024-10-21 01:16:04 [INFO]: Epoch 100 - training loss: 0.6467\n",
      "2024-10-21 01:16:04 [INFO]: Finished training. The best model is from epoch#92.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 14/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 01:21:24 [INFO]: Epoch 001 - training loss: 0.6443\n",
      "2024-10-21 01:21:27 [INFO]: Epoch 002 - training loss: 0.6324\n",
      "2024-10-21 01:21:30 [INFO]: Epoch 003 - training loss: 0.6286\n",
      "2024-10-21 01:21:32 [INFO]: Epoch 004 - training loss: 0.6349\n",
      "2024-10-21 01:21:35 [INFO]: Epoch 005 - training loss: 0.6309\n",
      "2024-10-21 01:21:38 [INFO]: Epoch 006 - training loss: 0.6364\n",
      "2024-10-21 01:21:41 [INFO]: Epoch 007 - training loss: 0.6304\n",
      "2024-10-21 01:21:44 [INFO]: Epoch 008 - training loss: 0.6227\n",
      "2024-10-21 01:21:47 [INFO]: Epoch 009 - training loss: 0.6264\n",
      "2024-10-21 01:21:49 [INFO]: Epoch 010 - training loss: 0.6247\n",
      "2024-10-21 01:21:52 [INFO]: Epoch 011 - training loss: 0.6297\n",
      "2024-10-21 01:21:55 [INFO]: Epoch 012 - training loss: 0.6283\n",
      "2024-10-21 01:21:58 [INFO]: Epoch 013 - training loss: 0.6241\n",
      "2024-10-21 01:22:01 [INFO]: Epoch 014 - training loss: 0.6294\n",
      "2024-10-21 01:22:04 [INFO]: Epoch 015 - training loss: 0.6275\n",
      "2024-10-21 01:22:06 [INFO]: Epoch 016 - training loss: 0.6242\n",
      "2024-10-21 01:22:09 [INFO]: Epoch 017 - training loss: 0.6240\n",
      "2024-10-21 01:22:12 [INFO]: Epoch 018 - training loss: 0.6255\n",
      "2024-10-21 01:22:15 [INFO]: Epoch 019 - training loss: 0.6229\n",
      "2024-10-21 01:22:18 [INFO]: Epoch 020 - training loss: 0.6231\n",
      "2024-10-21 01:22:20 [INFO]: Epoch 021 - training loss: 0.6224\n",
      "2024-10-21 01:22:23 [INFO]: Epoch 022 - training loss: 0.6159\n",
      "2024-10-21 01:22:26 [INFO]: Epoch 023 - training loss: 0.6278\n",
      "2024-10-21 01:22:29 [INFO]: Epoch 024 - training loss: 0.6210\n",
      "2024-10-21 01:22:32 [INFO]: Epoch 025 - training loss: 0.6224\n",
      "2024-10-21 01:22:35 [INFO]: Epoch 026 - training loss: 0.6207\n",
      "2024-10-21 01:22:37 [INFO]: Epoch 027 - training loss: 0.6292\n",
      "2024-10-21 01:22:40 [INFO]: Epoch 028 - training loss: 0.6285\n",
      "2024-10-21 01:22:43 [INFO]: Epoch 029 - training loss: 0.6208\n",
      "2024-10-21 01:22:46 [INFO]: Epoch 030 - training loss: 0.6210\n",
      "2024-10-21 01:22:49 [INFO]: Epoch 031 - training loss: 0.6136\n",
      "2024-10-21 01:22:52 [INFO]: Epoch 032 - training loss: 0.6239\n",
      "2024-10-21 01:22:54 [INFO]: Epoch 033 - training loss: 0.6189\n",
      "2024-10-21 01:22:57 [INFO]: Epoch 034 - training loss: 0.6264\n",
      "2024-10-21 01:23:00 [INFO]: Epoch 035 - training loss: 0.6182\n",
      "2024-10-21 01:23:03 [INFO]: Epoch 036 - training loss: 0.6234\n",
      "2024-10-21 01:23:06 [INFO]: Epoch 037 - training loss: 0.6272\n",
      "2024-10-21 01:23:09 [INFO]: Epoch 038 - training loss: 0.6178\n",
      "2024-10-21 01:23:11 [INFO]: Epoch 039 - training loss: 0.6182\n",
      "2024-10-21 01:23:14 [INFO]: Epoch 040 - training loss: 0.6215\n",
      "2024-10-21 01:23:17 [INFO]: Epoch 041 - training loss: 0.6198\n",
      "2024-10-21 01:23:20 [INFO]: Epoch 042 - training loss: 0.6179\n",
      "2024-10-21 01:23:23 [INFO]: Epoch 043 - training loss: 0.6258\n",
      "2024-10-21 01:23:25 [INFO]: Epoch 044 - training loss: 0.6175\n",
      "2024-10-21 01:23:28 [INFO]: Epoch 045 - training loss: 0.6161\n",
      "2024-10-21 01:23:31 [INFO]: Epoch 046 - training loss: 0.6236\n",
      "2024-10-21 01:23:34 [INFO]: Epoch 047 - training loss: 0.6231\n",
      "2024-10-21 01:23:37 [INFO]: Epoch 048 - training loss: 0.6295\n",
      "2024-10-21 01:23:40 [INFO]: Epoch 049 - training loss: 0.6229\n",
      "2024-10-21 01:23:42 [INFO]: Epoch 050 - training loss: 0.6208\n",
      "2024-10-21 01:23:45 [INFO]: Epoch 051 - training loss: 0.6201\n",
      "2024-10-21 01:23:48 [INFO]: Epoch 052 - training loss: 0.6241\n",
      "2024-10-21 01:23:51 [INFO]: Epoch 053 - training loss: 0.6155\n",
      "2024-10-21 01:23:54 [INFO]: Epoch 054 - training loss: 0.6157\n",
      "2024-10-21 01:23:56 [INFO]: Epoch 055 - training loss: 0.6154\n",
      "2024-10-21 01:23:59 [INFO]: Epoch 056 - training loss: 0.6179\n",
      "2024-10-21 01:24:02 [INFO]: Epoch 057 - training loss: 0.6181\n",
      "2024-10-21 01:24:05 [INFO]: Epoch 058 - training loss: 0.6190\n",
      "2024-10-21 01:24:08 [INFO]: Epoch 059 - training loss: 0.6152\n",
      "2024-10-21 01:24:11 [INFO]: Epoch 060 - training loss: 0.6153\n",
      "2024-10-21 01:24:13 [INFO]: Epoch 061 - training loss: 0.6277\n",
      "2024-10-21 01:24:16 [INFO]: Epoch 062 - training loss: 0.6252\n",
      "2024-10-21 01:24:19 [INFO]: Epoch 063 - training loss: 0.6204\n",
      "2024-10-21 01:24:22 [INFO]: Epoch 064 - training loss: 0.6316\n",
      "2024-10-21 01:24:25 [INFO]: Epoch 065 - training loss: 0.6222\n",
      "2024-10-21 01:24:27 [INFO]: Epoch 066 - training loss: 0.6184\n",
      "2024-10-21 01:24:30 [INFO]: Epoch 067 - training loss: 0.6195\n",
      "2024-10-21 01:24:33 [INFO]: Epoch 068 - training loss: 0.6200\n",
      "2024-10-21 01:24:36 [INFO]: Epoch 069 - training loss: 0.6193\n",
      "2024-10-21 01:24:39 [INFO]: Epoch 070 - training loss: 0.6220\n",
      "2024-10-21 01:24:41 [INFO]: Epoch 071 - training loss: 0.6199\n",
      "2024-10-21 01:24:44 [INFO]: Epoch 072 - training loss: 0.6223\n",
      "2024-10-21 01:24:47 [INFO]: Epoch 073 - training loss: 0.6274\n",
      "2024-10-21 01:24:50 [INFO]: Epoch 074 - training loss: 0.6180\n",
      "2024-10-21 01:24:53 [INFO]: Epoch 075 - training loss: 0.6198\n",
      "2024-10-21 01:24:56 [INFO]: Epoch 076 - training loss: 0.6190\n",
      "2024-10-21 01:24:58 [INFO]: Epoch 077 - training loss: 0.6157\n",
      "2024-10-21 01:25:01 [INFO]: Epoch 078 - training loss: 0.6259\n",
      "2024-10-21 01:25:04 [INFO]: Epoch 079 - training loss: 0.6128\n",
      "2024-10-21 01:25:07 [INFO]: Epoch 080 - training loss: 0.6161\n",
      "2024-10-21 01:25:10 [INFO]: Epoch 081 - training loss: 0.6199\n",
      "2024-10-21 01:25:12 [INFO]: Epoch 082 - training loss: 0.6128\n",
      "2024-10-21 01:25:15 [INFO]: Epoch 083 - training loss: 0.6158\n",
      "2024-10-21 01:25:18 [INFO]: Epoch 084 - training loss: 0.6256\n",
      "2024-10-21 01:25:21 [INFO]: Epoch 085 - training loss: 0.6177\n",
      "2024-10-21 01:25:24 [INFO]: Epoch 086 - training loss: 0.6171\n",
      "2024-10-21 01:25:27 [INFO]: Epoch 087 - training loss: 0.6280\n",
      "2024-10-21 01:25:29 [INFO]: Epoch 088 - training loss: 0.6171\n",
      "2024-10-21 01:25:32 [INFO]: Epoch 089 - training loss: 0.6268\n",
      "2024-10-21 01:25:35 [INFO]: Epoch 090 - training loss: 0.6163\n",
      "2024-10-21 01:25:38 [INFO]: Epoch 091 - training loss: 0.6172\n",
      "2024-10-21 01:25:41 [INFO]: Epoch 092 - training loss: 0.6206\n",
      "2024-10-21 01:25:43 [INFO]: Epoch 093 - training loss: 0.6175\n",
      "2024-10-21 01:25:46 [INFO]: Epoch 094 - training loss: 0.6210\n",
      "2024-10-21 01:25:49 [INFO]: Epoch 095 - training loss: 0.6198\n",
      "2024-10-21 01:25:52 [INFO]: Epoch 096 - training loss: 0.6255\n",
      "2024-10-21 01:25:55 [INFO]: Epoch 097 - training loss: 0.6191\n",
      "2024-10-21 01:25:58 [INFO]: Epoch 098 - training loss: 0.6198\n",
      "2024-10-21 01:26:00 [INFO]: Epoch 099 - training loss: 0.6089\n",
      "2024-10-21 01:26:03 [INFO]: Epoch 100 - training loss: 0.6160\n",
      "2024-10-21 01:26:03 [INFO]: Finished training. The best model is from epoch#99.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 15/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 01:31:22 [INFO]: Epoch 001 - training loss: 0.6311\n",
      "2024-10-21 01:31:25 [INFO]: Epoch 002 - training loss: 0.6323\n",
      "2024-10-21 01:31:28 [INFO]: Epoch 003 - training loss: 0.6224\n",
      "2024-10-21 01:31:30 [INFO]: Epoch 004 - training loss: 0.6263\n",
      "2024-10-21 01:31:33 [INFO]: Epoch 005 - training loss: 0.6253\n",
      "2024-10-21 01:31:36 [INFO]: Epoch 006 - training loss: 0.6359\n",
      "2024-10-21 01:31:39 [INFO]: Epoch 007 - training loss: 0.6192\n",
      "2024-10-21 01:31:42 [INFO]: Epoch 008 - training loss: 0.6274\n",
      "2024-10-21 01:31:45 [INFO]: Epoch 009 - training loss: 0.6162\n",
      "2024-10-21 01:31:48 [INFO]: Epoch 010 - training loss: 0.6161\n",
      "2024-10-21 01:31:50 [INFO]: Epoch 011 - training loss: 0.6240\n",
      "2024-10-21 01:31:53 [INFO]: Epoch 012 - training loss: 0.6184\n",
      "2024-10-21 01:31:56 [INFO]: Epoch 013 - training loss: 0.6255\n",
      "2024-10-21 01:31:59 [INFO]: Epoch 014 - training loss: 0.6173\n",
      "2024-10-21 01:32:02 [INFO]: Epoch 015 - training loss: 0.6210\n",
      "2024-10-21 01:32:05 [INFO]: Epoch 016 - training loss: 0.6317\n",
      "2024-10-21 01:32:07 [INFO]: Epoch 017 - training loss: 0.6220\n",
      "2024-10-21 01:32:10 [INFO]: Epoch 018 - training loss: 0.6262\n",
      "2024-10-21 01:32:13 [INFO]: Epoch 019 - training loss: 0.6305\n",
      "2024-10-21 01:32:16 [INFO]: Epoch 020 - training loss: 0.6364\n",
      "2024-10-21 01:32:19 [INFO]: Epoch 021 - training loss: 0.6261\n",
      "2024-10-21 01:32:21 [INFO]: Epoch 022 - training loss: 0.6312\n",
      "2024-10-21 01:32:24 [INFO]: Epoch 023 - training loss: 0.6175\n",
      "2024-10-21 01:32:27 [INFO]: Epoch 024 - training loss: 0.6262\n",
      "2024-10-21 01:32:30 [INFO]: Epoch 025 - training loss: 0.6257\n",
      "2024-10-21 01:32:33 [INFO]: Epoch 026 - training loss: 0.6208\n",
      "2024-10-21 01:32:35 [INFO]: Epoch 027 - training loss: 0.6241\n",
      "2024-10-21 01:32:38 [INFO]: Epoch 028 - training loss: 0.6251\n",
      "2024-10-21 01:32:41 [INFO]: Epoch 029 - training loss: 0.6121\n",
      "2024-10-21 01:32:44 [INFO]: Epoch 030 - training loss: 0.6317\n",
      "2024-10-21 01:32:47 [INFO]: Epoch 031 - training loss: 0.6206\n",
      "2024-10-21 01:32:50 [INFO]: Epoch 032 - training loss: 0.6112\n",
      "2024-10-21 01:32:52 [INFO]: Epoch 033 - training loss: 0.6194\n",
      "2024-10-21 01:32:55 [INFO]: Epoch 034 - training loss: 0.6156\n",
      "2024-10-21 01:32:58 [INFO]: Epoch 035 - training loss: 0.6132\n",
      "2024-10-21 01:33:01 [INFO]: Epoch 036 - training loss: 0.6257\n",
      "2024-10-21 01:33:04 [INFO]: Epoch 037 - training loss: 0.6218\n",
      "2024-10-21 01:33:07 [INFO]: Epoch 038 - training loss: 0.6212\n",
      "2024-10-21 01:33:09 [INFO]: Epoch 039 - training loss: 0.6123\n",
      "2024-10-21 01:33:12 [INFO]: Epoch 040 - training loss: 0.6175\n",
      "2024-10-21 01:33:15 [INFO]: Epoch 041 - training loss: 0.6144\n",
      "2024-10-21 01:33:18 [INFO]: Epoch 042 - training loss: 0.6135\n",
      "2024-10-21 01:33:20 [INFO]: Epoch 043 - training loss: 0.6130\n",
      "2024-10-21 01:33:23 [INFO]: Epoch 044 - training loss: 0.6188\n",
      "2024-10-21 01:33:26 [INFO]: Epoch 045 - training loss: 0.6266\n",
      "2024-10-21 01:33:29 [INFO]: Epoch 046 - training loss: 0.6218\n",
      "2024-10-21 01:33:32 [INFO]: Epoch 047 - training loss: 0.6217\n",
      "2024-10-21 01:33:34 [INFO]: Epoch 048 - training loss: 0.6256\n",
      "2024-10-21 01:33:37 [INFO]: Epoch 049 - training loss: 0.6233\n",
      "2024-10-21 01:33:40 [INFO]: Epoch 050 - training loss: 0.6234\n",
      "2024-10-21 01:33:43 [INFO]: Epoch 051 - training loss: 0.6300\n",
      "2024-10-21 01:33:46 [INFO]: Epoch 052 - training loss: 0.6264\n",
      "2024-10-21 01:33:48 [INFO]: Epoch 053 - training loss: 0.6199\n",
      "2024-10-21 01:33:51 [INFO]: Epoch 054 - training loss: 0.6122\n",
      "2024-10-21 01:33:54 [INFO]: Epoch 055 - training loss: 0.6174\n",
      "2024-10-21 01:33:57 [INFO]: Epoch 056 - training loss: 0.6168\n",
      "2024-10-21 01:34:00 [INFO]: Epoch 057 - training loss: 0.6197\n",
      "2024-10-21 01:34:02 [INFO]: Epoch 058 - training loss: 0.6217\n",
      "2024-10-21 01:34:05 [INFO]: Epoch 059 - training loss: 0.6282\n",
      "2024-10-21 01:34:08 [INFO]: Epoch 060 - training loss: 0.6351\n",
      "2024-10-21 01:34:11 [INFO]: Epoch 061 - training loss: 0.6234\n",
      "2024-10-21 01:34:14 [INFO]: Epoch 062 - training loss: 0.6212\n",
      "2024-10-21 01:34:16 [INFO]: Epoch 063 - training loss: 0.6166\n",
      "2024-10-21 01:34:19 [INFO]: Epoch 064 - training loss: 0.6153\n",
      "2024-10-21 01:34:22 [INFO]: Epoch 065 - training loss: 0.6201\n",
      "2024-10-21 01:34:25 [INFO]: Epoch 066 - training loss: 0.6161\n",
      "2024-10-21 01:34:28 [INFO]: Epoch 067 - training loss: 0.6236\n",
      "2024-10-21 01:34:31 [INFO]: Epoch 068 - training loss: 0.6148\n",
      "2024-10-21 01:34:33 [INFO]: Epoch 069 - training loss: 0.6166\n",
      "2024-10-21 01:34:36 [INFO]: Epoch 070 - training loss: 0.6057\n",
      "2024-10-21 01:34:39 [INFO]: Epoch 071 - training loss: 0.6174\n",
      "2024-10-21 01:34:42 [INFO]: Epoch 072 - training loss: 0.6095\n",
      "2024-10-21 01:34:45 [INFO]: Epoch 073 - training loss: 0.6064\n",
      "2024-10-21 01:34:47 [INFO]: Epoch 074 - training loss: 0.6246\n",
      "2024-10-21 01:34:50 [INFO]: Epoch 075 - training loss: 0.6294\n",
      "2024-10-21 01:34:53 [INFO]: Epoch 076 - training loss: 0.6088\n",
      "2024-10-21 01:34:56 [INFO]: Epoch 077 - training loss: 0.6210\n",
      "2024-10-21 01:34:59 [INFO]: Epoch 078 - training loss: 0.6117\n",
      "2024-10-21 01:35:01 [INFO]: Epoch 079 - training loss: 0.6151\n",
      "2024-10-21 01:35:04 [INFO]: Epoch 080 - training loss: 0.6150\n",
      "2024-10-21 01:35:07 [INFO]: Epoch 081 - training loss: 0.6137\n",
      "2024-10-21 01:35:10 [INFO]: Epoch 082 - training loss: 0.6121\n",
      "2024-10-21 01:35:12 [INFO]: Epoch 083 - training loss: 0.6259\n",
      "2024-10-21 01:35:15 [INFO]: Epoch 084 - training loss: 0.6122\n",
      "2024-10-21 01:35:18 [INFO]: Epoch 085 - training loss: 0.6145\n",
      "2024-10-21 01:35:21 [INFO]: Epoch 086 - training loss: 0.6238\n",
      "2024-10-21 01:35:24 [INFO]: Epoch 087 - training loss: 0.6155\n",
      "2024-10-21 01:35:26 [INFO]: Epoch 088 - training loss: 0.6182\n",
      "2024-10-21 01:35:29 [INFO]: Epoch 089 - training loss: 0.6179\n",
      "2024-10-21 01:35:32 [INFO]: Epoch 090 - training loss: 0.6150\n",
      "2024-10-21 01:35:35 [INFO]: Epoch 091 - training loss: 0.6251\n",
      "2024-10-21 01:35:38 [INFO]: Epoch 092 - training loss: 0.6178\n",
      "2024-10-21 01:35:41 [INFO]: Epoch 093 - training loss: 0.6106\n",
      "2024-10-21 01:35:43 [INFO]: Epoch 094 - training loss: 0.6226\n",
      "2024-10-21 01:35:46 [INFO]: Epoch 095 - training loss: 0.6225\n",
      "2024-10-21 01:35:49 [INFO]: Epoch 096 - training loss: 0.6191\n",
      "2024-10-21 01:35:52 [INFO]: Epoch 097 - training loss: 0.6178\n",
      "2024-10-21 01:35:55 [INFO]: Epoch 098 - training loss: 0.6248\n",
      "2024-10-21 01:35:57 [INFO]: Epoch 099 - training loss: 0.6148\n",
      "2024-10-21 01:36:00 [INFO]: Epoch 100 - training loss: 0.6143\n",
      "2024-10-21 01:36:00 [INFO]: Finished training. The best model is from epoch#70.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 16/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 01:41:19 [INFO]: Epoch 001 - training loss: 0.6359\n",
      "2024-10-21 01:41:22 [INFO]: Epoch 002 - training loss: 0.6337\n",
      "2024-10-21 01:41:25 [INFO]: Epoch 003 - training loss: 0.6432\n",
      "2024-10-21 01:41:27 [INFO]: Epoch 004 - training loss: 0.6546\n",
      "2024-10-21 01:41:30 [INFO]: Epoch 005 - training loss: 0.6299\n",
      "2024-10-21 01:41:33 [INFO]: Epoch 006 - training loss: 0.6244\n",
      "2024-10-21 01:41:36 [INFO]: Epoch 007 - training loss: 0.6255\n",
      "2024-10-21 01:41:39 [INFO]: Epoch 008 - training loss: 0.6353\n",
      "2024-10-21 01:41:42 [INFO]: Epoch 009 - training loss: 0.6329\n",
      "2024-10-21 01:41:44 [INFO]: Epoch 010 - training loss: 0.6280\n",
      "2024-10-21 01:41:47 [INFO]: Epoch 011 - training loss: 0.6307\n",
      "2024-10-21 01:41:50 [INFO]: Epoch 012 - training loss: 0.6310\n",
      "2024-10-21 01:41:53 [INFO]: Epoch 013 - training loss: 0.6266\n",
      "2024-10-21 01:41:56 [INFO]: Epoch 014 - training loss: 0.6389\n",
      "2024-10-21 01:41:58 [INFO]: Epoch 015 - training loss: 0.6251\n",
      "2024-10-21 01:42:01 [INFO]: Epoch 016 - training loss: 0.6221\n",
      "2024-10-21 01:42:04 [INFO]: Epoch 017 - training loss: 0.6510\n",
      "2024-10-21 01:42:07 [INFO]: Epoch 018 - training loss: 0.6267\n",
      "2024-10-21 01:42:10 [INFO]: Epoch 019 - training loss: 0.6202\n",
      "2024-10-21 01:42:13 [INFO]: Epoch 020 - training loss: 0.6232\n",
      "2024-10-21 01:42:15 [INFO]: Epoch 021 - training loss: 0.6281\n",
      "2024-10-21 01:42:18 [INFO]: Epoch 022 - training loss: 0.6270\n",
      "2024-10-21 01:42:21 [INFO]: Epoch 023 - training loss: 0.6224\n",
      "2024-10-21 01:42:24 [INFO]: Epoch 024 - training loss: 0.6305\n",
      "2024-10-21 01:42:27 [INFO]: Epoch 025 - training loss: 0.6258\n",
      "2024-10-21 01:42:29 [INFO]: Epoch 026 - training loss: 0.6254\n",
      "2024-10-21 01:42:32 [INFO]: Epoch 027 - training loss: 0.6192\n",
      "2024-10-21 01:42:35 [INFO]: Epoch 028 - training loss: 0.6262\n",
      "2024-10-21 01:42:38 [INFO]: Epoch 029 - training loss: 0.6314\n",
      "2024-10-21 01:42:41 [INFO]: Epoch 030 - training loss: 0.6273\n",
      "2024-10-21 01:42:43 [INFO]: Epoch 031 - training loss: 0.6254\n",
      "2024-10-21 01:42:46 [INFO]: Epoch 032 - training loss: 0.6310\n",
      "2024-10-21 01:42:49 [INFO]: Epoch 033 - training loss: 0.6272\n",
      "2024-10-21 01:42:52 [INFO]: Epoch 034 - training loss: 0.6244\n",
      "2024-10-21 01:42:55 [INFO]: Epoch 035 - training loss: 0.6242\n",
      "2024-10-21 01:42:57 [INFO]: Epoch 036 - training loss: 0.6263\n",
      "2024-10-21 01:43:00 [INFO]: Epoch 037 - training loss: 0.6265\n",
      "2024-10-21 01:43:03 [INFO]: Epoch 038 - training loss: 0.6242\n",
      "2024-10-21 01:43:06 [INFO]: Epoch 039 - training loss: 0.6257\n",
      "2024-10-21 01:43:09 [INFO]: Epoch 040 - training loss: 0.6209\n",
      "2024-10-21 01:43:11 [INFO]: Epoch 041 - training loss: 0.6280\n",
      "2024-10-21 01:43:14 [INFO]: Epoch 042 - training loss: 0.6315\n",
      "2024-10-21 01:43:17 [INFO]: Epoch 043 - training loss: 0.6251\n",
      "2024-10-21 01:43:20 [INFO]: Epoch 044 - training loss: 0.6196\n",
      "2024-10-21 01:43:23 [INFO]: Epoch 045 - training loss: 0.6273\n",
      "2024-10-21 01:43:26 [INFO]: Epoch 046 - training loss: 0.6310\n",
      "2024-10-21 01:43:29 [INFO]: Epoch 047 - training loss: 0.6242\n",
      "2024-10-21 01:43:31 [INFO]: Epoch 048 - training loss: 0.6171\n",
      "2024-10-21 01:43:34 [INFO]: Epoch 049 - training loss: 0.6243\n",
      "2024-10-21 01:43:37 [INFO]: Epoch 050 - training loss: 0.6293\n",
      "2024-10-21 01:43:40 [INFO]: Epoch 051 - training loss: 0.6161\n",
      "2024-10-21 01:43:43 [INFO]: Epoch 052 - training loss: 0.6332\n",
      "2024-10-21 01:43:45 [INFO]: Epoch 053 - training loss: 0.6272\n",
      "2024-10-21 01:43:48 [INFO]: Epoch 054 - training loss: 0.6284\n",
      "2024-10-21 01:43:51 [INFO]: Epoch 055 - training loss: 0.6278\n",
      "2024-10-21 01:43:54 [INFO]: Epoch 056 - training loss: 0.6299\n",
      "2024-10-21 01:43:57 [INFO]: Epoch 057 - training loss: 0.6215\n",
      "2024-10-21 01:43:59 [INFO]: Epoch 058 - training loss: 0.6268\n",
      "2024-10-21 01:44:02 [INFO]: Epoch 059 - training loss: 0.6229\n",
      "2024-10-21 01:44:05 [INFO]: Epoch 060 - training loss: 0.6296\n",
      "2024-10-21 01:44:08 [INFO]: Epoch 061 - training loss: 0.6459\n",
      "2024-10-21 01:44:11 [INFO]: Epoch 062 - training loss: 0.6308\n",
      "2024-10-21 01:44:14 [INFO]: Epoch 063 - training loss: 0.6329\n",
      "2024-10-21 01:44:16 [INFO]: Epoch 064 - training loss: 0.6293\n",
      "2024-10-21 01:44:19 [INFO]: Epoch 065 - training loss: 0.6318\n",
      "2024-10-21 01:44:22 [INFO]: Epoch 066 - training loss: 0.6318\n",
      "2024-10-21 01:44:25 [INFO]: Epoch 067 - training loss: 0.6244\n",
      "2024-10-21 01:44:28 [INFO]: Epoch 068 - training loss: 0.6225\n",
      "2024-10-21 01:44:31 [INFO]: Epoch 069 - training loss: 0.6213\n",
      "2024-10-21 01:44:33 [INFO]: Epoch 070 - training loss: 0.6209\n",
      "2024-10-21 01:44:36 [INFO]: Epoch 071 - training loss: 0.6239\n",
      "2024-10-21 01:44:39 [INFO]: Epoch 072 - training loss: 0.6248\n",
      "2024-10-21 01:44:42 [INFO]: Epoch 073 - training loss: 0.6287\n",
      "2024-10-21 01:44:44 [INFO]: Epoch 074 - training loss: 0.6180\n",
      "2024-10-21 01:44:47 [INFO]: Epoch 075 - training loss: 0.6121\n",
      "2024-10-21 01:44:50 [INFO]: Epoch 076 - training loss: 0.6271\n",
      "2024-10-21 01:44:53 [INFO]: Epoch 077 - training loss: 0.6239\n",
      "2024-10-21 01:44:56 [INFO]: Epoch 078 - training loss: 0.6258\n",
      "2024-10-21 01:44:59 [INFO]: Epoch 079 - training loss: 0.6219\n",
      "2024-10-21 01:45:01 [INFO]: Epoch 080 - training loss: 0.6188\n",
      "2024-10-21 01:45:04 [INFO]: Epoch 081 - training loss: 0.6117\n",
      "2024-10-21 01:45:07 [INFO]: Epoch 082 - training loss: 0.6245\n",
      "2024-10-21 01:45:10 [INFO]: Epoch 083 - training loss: 0.6180\n",
      "2024-10-21 01:45:13 [INFO]: Epoch 084 - training loss: 0.6222\n",
      "2024-10-21 01:45:16 [INFO]: Epoch 085 - training loss: 0.6209\n",
      "2024-10-21 01:45:19 [INFO]: Epoch 086 - training loss: 0.6275\n",
      "2024-10-21 01:45:21 [INFO]: Epoch 087 - training loss: 0.6208\n",
      "2024-10-21 01:45:24 [INFO]: Epoch 088 - training loss: 0.6166\n",
      "2024-10-21 01:45:27 [INFO]: Epoch 089 - training loss: 0.6167\n",
      "2024-10-21 01:45:30 [INFO]: Epoch 090 - training loss: 0.6142\n",
      "2024-10-21 01:45:33 [INFO]: Epoch 091 - training loss: 0.6137\n",
      "2024-10-21 01:45:35 [INFO]: Epoch 092 - training loss: 0.6220\n",
      "2024-10-21 01:45:38 [INFO]: Epoch 093 - training loss: 0.6213\n",
      "2024-10-21 01:45:41 [INFO]: Epoch 094 - training loss: 0.6285\n",
      "2024-10-21 01:45:44 [INFO]: Epoch 095 - training loss: 0.6214\n",
      "2024-10-21 01:45:47 [INFO]: Epoch 096 - training loss: 0.6167\n",
      "2024-10-21 01:45:50 [INFO]: Epoch 097 - training loss: 0.6169\n",
      "2024-10-21 01:45:52 [INFO]: Epoch 098 - training loss: 0.6124\n",
      "2024-10-21 01:45:55 [INFO]: Epoch 099 - training loss: 0.6172\n",
      "2024-10-21 01:45:58 [INFO]: Epoch 100 - training loss: 0.6165\n",
      "2024-10-21 01:45:58 [INFO]: Finished training. The best model is from epoch#81.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 17/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 01:51:16 [INFO]: Epoch 001 - training loss: 0.6304\n",
      "2024-10-21 01:51:19 [INFO]: Epoch 002 - training loss: 0.6340\n",
      "2024-10-21 01:51:21 [INFO]: Epoch 003 - training loss: 0.6339\n",
      "2024-10-21 01:51:24 [INFO]: Epoch 004 - training loss: 0.6395\n",
      "2024-10-21 01:51:27 [INFO]: Epoch 005 - training loss: 0.6339\n",
      "2024-10-21 01:51:30 [INFO]: Epoch 006 - training loss: 0.6331\n",
      "2024-10-21 01:51:33 [INFO]: Epoch 007 - training loss: 0.6355\n",
      "2024-10-21 01:51:36 [INFO]: Epoch 008 - training loss: 0.6258\n",
      "2024-10-21 01:51:38 [INFO]: Epoch 009 - training loss: 0.6287\n",
      "2024-10-21 01:51:41 [INFO]: Epoch 010 - training loss: 0.6313\n",
      "2024-10-21 01:51:44 [INFO]: Epoch 011 - training loss: 0.6274\n",
      "2024-10-21 01:51:47 [INFO]: Epoch 012 - training loss: 0.6269\n",
      "2024-10-21 01:51:50 [INFO]: Epoch 013 - training loss: 0.6237\n",
      "2024-10-21 01:51:52 [INFO]: Epoch 014 - training loss: 0.6272\n",
      "2024-10-21 01:51:55 [INFO]: Epoch 015 - training loss: 0.6239\n",
      "2024-10-21 01:51:58 [INFO]: Epoch 016 - training loss: 0.6256\n",
      "2024-10-21 01:52:01 [INFO]: Epoch 017 - training loss: 0.6219\n",
      "2024-10-21 01:52:04 [INFO]: Epoch 018 - training loss: 0.6306\n",
      "2024-10-21 01:52:07 [INFO]: Epoch 019 - training loss: 0.6295\n",
      "2024-10-21 01:52:09 [INFO]: Epoch 020 - training loss: 0.6285\n",
      "2024-10-21 01:52:12 [INFO]: Epoch 021 - training loss: 0.6303\n",
      "2024-10-21 01:52:15 [INFO]: Epoch 022 - training loss: 0.6278\n",
      "2024-10-21 01:52:18 [INFO]: Epoch 023 - training loss: 0.6130\n",
      "2024-10-21 01:52:21 [INFO]: Epoch 024 - training loss: 0.6321\n",
      "2024-10-21 01:52:23 [INFO]: Epoch 025 - training loss: 0.6276\n",
      "2024-10-21 01:52:26 [INFO]: Epoch 026 - training loss: 0.6204\n",
      "2024-10-21 01:52:29 [INFO]: Epoch 027 - training loss: 0.6230\n",
      "2024-10-21 01:52:32 [INFO]: Epoch 028 - training loss: 0.6131\n",
      "2024-10-21 01:52:35 [INFO]: Epoch 029 - training loss: 0.6221\n",
      "2024-10-21 01:52:38 [INFO]: Epoch 030 - training loss: 0.6198\n",
      "2024-10-21 01:52:40 [INFO]: Epoch 031 - training loss: 0.6228\n",
      "2024-10-21 01:52:43 [INFO]: Epoch 032 - training loss: 0.6275\n",
      "2024-10-21 01:52:46 [INFO]: Epoch 033 - training loss: 0.6179\n",
      "2024-10-21 01:52:49 [INFO]: Epoch 034 - training loss: 0.6233\n",
      "2024-10-21 01:52:52 [INFO]: Epoch 035 - training loss: 0.6223\n",
      "2024-10-21 01:52:54 [INFO]: Epoch 036 - training loss: 0.6259\n",
      "2024-10-21 01:52:57 [INFO]: Epoch 037 - training loss: 0.6313\n",
      "2024-10-21 01:53:00 [INFO]: Epoch 038 - training loss: 0.6296\n",
      "2024-10-21 01:53:03 [INFO]: Epoch 039 - training loss: 0.6260\n",
      "2024-10-21 01:53:06 [INFO]: Epoch 040 - training loss: 0.6213\n",
      "2024-10-21 01:53:08 [INFO]: Epoch 041 - training loss: 0.6232\n",
      "2024-10-21 01:53:11 [INFO]: Epoch 042 - training loss: 0.6229\n",
      "2024-10-21 01:53:14 [INFO]: Epoch 043 - training loss: 0.6234\n",
      "2024-10-21 01:53:17 [INFO]: Epoch 044 - training loss: 0.6209\n",
      "2024-10-21 01:53:20 [INFO]: Epoch 045 - training loss: 0.6185\n",
      "2024-10-21 01:53:22 [INFO]: Epoch 046 - training loss: 0.6236\n",
      "2024-10-21 01:53:25 [INFO]: Epoch 047 - training loss: 0.6285\n",
      "2024-10-21 01:53:28 [INFO]: Epoch 048 - training loss: 0.6179\n",
      "2024-10-21 01:53:31 [INFO]: Epoch 049 - training loss: 0.6236\n",
      "2024-10-21 01:53:34 [INFO]: Epoch 050 - training loss: 0.6149\n",
      "2024-10-21 01:53:36 [INFO]: Epoch 051 - training loss: 0.6247\n",
      "2024-10-21 01:53:39 [INFO]: Epoch 052 - training loss: 0.6254\n",
      "2024-10-21 01:53:42 [INFO]: Epoch 053 - training loss: 0.6256\n",
      "2024-10-21 01:53:45 [INFO]: Epoch 054 - training loss: 0.6251\n",
      "2024-10-21 01:53:48 [INFO]: Epoch 055 - training loss: 0.6222\n",
      "2024-10-21 01:53:50 [INFO]: Epoch 056 - training loss: 0.6196\n",
      "2024-10-21 01:53:53 [INFO]: Epoch 057 - training loss: 0.6185\n",
      "2024-10-21 01:53:56 [INFO]: Epoch 058 - training loss: 0.6142\n",
      "2024-10-21 01:53:59 [INFO]: Epoch 059 - training loss: 0.6169\n",
      "2024-10-21 01:54:02 [INFO]: Epoch 060 - training loss: 0.6161\n",
      "2024-10-21 01:54:04 [INFO]: Epoch 061 - training loss: 0.6192\n",
      "2024-10-21 01:54:07 [INFO]: Epoch 062 - training loss: 0.6177\n",
      "2024-10-21 01:54:10 [INFO]: Epoch 063 - training loss: 0.6151\n",
      "2024-10-21 01:54:13 [INFO]: Epoch 064 - training loss: 0.6250\n",
      "2024-10-21 01:54:16 [INFO]: Epoch 065 - training loss: 0.6157\n",
      "2024-10-21 01:54:19 [INFO]: Epoch 066 - training loss: 0.6138\n",
      "2024-10-21 01:54:21 [INFO]: Epoch 067 - training loss: 0.6166\n",
      "2024-10-21 01:54:24 [INFO]: Epoch 068 - training loss: 0.6170\n",
      "2024-10-21 01:54:27 [INFO]: Epoch 069 - training loss: 0.6212\n",
      "2024-10-21 01:54:30 [INFO]: Epoch 070 - training loss: 0.6183\n",
      "2024-10-21 01:54:32 [INFO]: Epoch 071 - training loss: 0.6125\n",
      "2024-10-21 01:54:35 [INFO]: Epoch 072 - training loss: 0.6219\n",
      "2024-10-21 01:54:38 [INFO]: Epoch 073 - training loss: 0.6296\n",
      "2024-10-21 01:54:41 [INFO]: Epoch 074 - training loss: 0.6182\n",
      "2024-10-21 01:54:44 [INFO]: Epoch 075 - training loss: 0.6256\n",
      "2024-10-21 01:54:47 [INFO]: Epoch 076 - training loss: 0.6078\n",
      "2024-10-21 01:54:49 [INFO]: Epoch 077 - training loss: 0.6182\n",
      "2024-10-21 01:54:52 [INFO]: Epoch 078 - training loss: 0.6127\n",
      "2024-10-21 01:54:55 [INFO]: Epoch 079 - training loss: 0.6186\n",
      "2024-10-21 01:54:58 [INFO]: Epoch 080 - training loss: 0.6209\n",
      "2024-10-21 01:55:01 [INFO]: Epoch 081 - training loss: 0.6165\n",
      "2024-10-21 01:55:03 [INFO]: Epoch 082 - training loss: 0.6209\n",
      "2024-10-21 01:55:06 [INFO]: Epoch 083 - training loss: 0.6140\n",
      "2024-10-21 01:55:09 [INFO]: Epoch 084 - training loss: 0.6135\n",
      "2024-10-21 01:55:12 [INFO]: Epoch 085 - training loss: 0.6190\n",
      "2024-10-21 01:55:15 [INFO]: Epoch 086 - training loss: 0.6154\n",
      "2024-10-21 01:55:18 [INFO]: Epoch 087 - training loss: 0.6170\n",
      "2024-10-21 01:55:20 [INFO]: Epoch 088 - training loss: 0.6170\n",
      "2024-10-21 01:55:23 [INFO]: Epoch 089 - training loss: 0.6181\n",
      "2024-10-21 01:55:26 [INFO]: Epoch 090 - training loss: 0.6205\n",
      "2024-10-21 01:55:29 [INFO]: Epoch 091 - training loss: 0.6146\n",
      "2024-10-21 01:55:32 [INFO]: Epoch 092 - training loss: 0.6244\n",
      "2024-10-21 01:55:35 [INFO]: Epoch 093 - training loss: 0.6132\n",
      "2024-10-21 01:55:37 [INFO]: Epoch 094 - training loss: 0.6195\n",
      "2024-10-21 01:55:40 [INFO]: Epoch 095 - training loss: 0.6162\n",
      "2024-10-21 01:55:43 [INFO]: Epoch 096 - training loss: 0.6178\n",
      "2024-10-21 01:55:46 [INFO]: Epoch 097 - training loss: 0.6126\n",
      "2024-10-21 01:55:49 [INFO]: Epoch 098 - training loss: 0.6102\n",
      "2024-10-21 01:55:52 [INFO]: Epoch 099 - training loss: 0.6162\n",
      "2024-10-21 01:55:55 [INFO]: Epoch 100 - training loss: 0.6125\n",
      "2024-10-21 01:55:55 [INFO]: Finished training. The best model is from epoch#76.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 18/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:01:13 [INFO]: Epoch 001 - training loss: 0.6319\n",
      "2024-10-21 02:01:16 [INFO]: Epoch 002 - training loss: 0.6269\n",
      "2024-10-21 02:01:18 [INFO]: Epoch 003 - training loss: 0.6205\n",
      "2024-10-21 02:01:21 [INFO]: Epoch 004 - training loss: 0.6328\n",
      "2024-10-21 02:01:24 [INFO]: Epoch 005 - training loss: 0.6206\n",
      "2024-10-21 02:01:27 [INFO]: Epoch 006 - training loss: 0.6329\n",
      "2024-10-21 02:01:30 [INFO]: Epoch 007 - training loss: 0.6234\n",
      "2024-10-21 02:01:32 [INFO]: Epoch 008 - training loss: 0.6240\n",
      "2024-10-21 02:01:35 [INFO]: Epoch 009 - training loss: 0.6262\n",
      "2024-10-21 02:01:38 [INFO]: Epoch 010 - training loss: 0.6282\n",
      "2024-10-21 02:01:41 [INFO]: Epoch 011 - training loss: 0.6251\n",
      "2024-10-21 02:01:44 [INFO]: Epoch 012 - training loss: 0.6297\n",
      "2024-10-21 02:01:47 [INFO]: Epoch 013 - training loss: 0.6168\n",
      "2024-10-21 02:01:49 [INFO]: Epoch 014 - training loss: 0.6252\n",
      "2024-10-21 02:01:52 [INFO]: Epoch 015 - training loss: 0.6234\n",
      "2024-10-21 02:01:55 [INFO]: Epoch 016 - training loss: 0.6167\n",
      "2024-10-21 02:01:58 [INFO]: Epoch 017 - training loss: 0.6202\n",
      "2024-10-21 02:02:01 [INFO]: Epoch 018 - training loss: 0.6191\n",
      "2024-10-21 02:02:04 [INFO]: Epoch 019 - training loss: 0.6151\n",
      "2024-10-21 02:02:07 [INFO]: Epoch 020 - training loss: 0.6142\n",
      "2024-10-21 02:02:09 [INFO]: Epoch 021 - training loss: 0.6210\n",
      "2024-10-21 02:02:12 [INFO]: Epoch 022 - training loss: 0.6225\n",
      "2024-10-21 02:02:15 [INFO]: Epoch 023 - training loss: 0.6193\n",
      "2024-10-21 02:02:18 [INFO]: Epoch 024 - training loss: 0.6229\n",
      "2024-10-21 02:02:21 [INFO]: Epoch 025 - training loss: 0.6181\n",
      "2024-10-21 02:02:23 [INFO]: Epoch 026 - training loss: 0.6178\n",
      "2024-10-21 02:02:26 [INFO]: Epoch 027 - training loss: 0.6175\n",
      "2024-10-21 02:02:29 [INFO]: Epoch 028 - training loss: 0.6205\n",
      "2024-10-21 02:02:32 [INFO]: Epoch 029 - training loss: 0.6189\n",
      "2024-10-21 02:02:35 [INFO]: Epoch 030 - training loss: 0.6182\n",
      "2024-10-21 02:02:37 [INFO]: Epoch 031 - training loss: 0.6154\n",
      "2024-10-21 02:02:40 [INFO]: Epoch 032 - training loss: 0.6175\n",
      "2024-10-21 02:02:43 [INFO]: Epoch 033 - training loss: 0.6223\n",
      "2024-10-21 02:02:46 [INFO]: Epoch 034 - training loss: 0.6168\n",
      "2024-10-21 02:02:49 [INFO]: Epoch 035 - training loss: 0.6222\n",
      "2024-10-21 02:02:52 [INFO]: Epoch 036 - training loss: 0.6156\n",
      "2024-10-21 02:02:54 [INFO]: Epoch 037 - training loss: 0.6183\n",
      "2024-10-21 02:02:57 [INFO]: Epoch 038 - training loss: 0.6180\n",
      "2024-10-21 02:03:00 [INFO]: Epoch 039 - training loss: 0.6114\n",
      "2024-10-21 02:03:03 [INFO]: Epoch 040 - training loss: 0.6500\n",
      "2024-10-21 02:03:06 [INFO]: Epoch 041 - training loss: 0.6302\n",
      "2024-10-21 02:03:08 [INFO]: Epoch 042 - training loss: 0.6282\n",
      "2024-10-21 02:03:11 [INFO]: Epoch 043 - training loss: 0.6180\n",
      "2024-10-21 02:03:14 [INFO]: Epoch 044 - training loss: 0.6263\n",
      "2024-10-21 02:03:17 [INFO]: Epoch 045 - training loss: 0.6369\n",
      "2024-10-21 02:03:20 [INFO]: Epoch 046 - training loss: 0.6139\n",
      "2024-10-21 02:03:22 [INFO]: Epoch 047 - training loss: 0.6260\n",
      "2024-10-21 02:03:25 [INFO]: Epoch 048 - training loss: 0.6227\n",
      "2024-10-21 02:03:28 [INFO]: Epoch 049 - training loss: 0.6238\n",
      "2024-10-21 02:03:31 [INFO]: Epoch 050 - training loss: 0.6210\n",
      "2024-10-21 02:03:34 [INFO]: Epoch 051 - training loss: 0.6234\n",
      "2024-10-21 02:03:37 [INFO]: Epoch 052 - training loss: 0.6368\n",
      "2024-10-21 02:03:40 [INFO]: Epoch 053 - training loss: 0.6338\n",
      "2024-10-21 02:03:42 [INFO]: Epoch 054 - training loss: 0.6212\n",
      "2024-10-21 02:03:45 [INFO]: Epoch 055 - training loss: 0.6163\n",
      "2024-10-21 02:03:48 [INFO]: Epoch 056 - training loss: 0.6178\n",
      "2024-10-21 02:03:51 [INFO]: Epoch 057 - training loss: 0.6321\n",
      "2024-10-21 02:03:54 [INFO]: Epoch 058 - training loss: 0.6107\n",
      "2024-10-21 02:03:56 [INFO]: Epoch 059 - training loss: 0.6179\n",
      "2024-10-21 02:03:59 [INFO]: Epoch 060 - training loss: 0.6168\n",
      "2024-10-21 02:04:02 [INFO]: Epoch 061 - training loss: 0.6204\n",
      "2024-10-21 02:04:05 [INFO]: Epoch 062 - training loss: 0.6117\n",
      "2024-10-21 02:04:08 [INFO]: Epoch 063 - training loss: 0.6188\n",
      "2024-10-21 02:04:11 [INFO]: Epoch 064 - training loss: 0.6151\n",
      "2024-10-21 02:04:13 [INFO]: Epoch 065 - training loss: 0.6204\n",
      "2024-10-21 02:04:16 [INFO]: Epoch 066 - training loss: 0.6162\n",
      "2024-10-21 02:04:19 [INFO]: Epoch 067 - training loss: 0.6180\n",
      "2024-10-21 02:04:22 [INFO]: Epoch 068 - training loss: 0.6189\n",
      "2024-10-21 02:04:25 [INFO]: Epoch 069 - training loss: 0.6271\n",
      "2024-10-21 02:04:27 [INFO]: Epoch 070 - training loss: 0.6150\n",
      "2024-10-21 02:04:30 [INFO]: Epoch 071 - training loss: 0.6159\n",
      "2024-10-21 02:04:33 [INFO]: Epoch 072 - training loss: 0.6143\n",
      "2024-10-21 02:04:36 [INFO]: Epoch 073 - training loss: 0.6360\n",
      "2024-10-21 02:04:39 [INFO]: Epoch 074 - training loss: 0.6147\n",
      "2024-10-21 02:04:42 [INFO]: Epoch 075 - training loss: 0.6119\n",
      "2024-10-21 02:04:44 [INFO]: Epoch 076 - training loss: 0.6253\n",
      "2024-10-21 02:04:47 [INFO]: Epoch 077 - training loss: 0.6265\n",
      "2024-10-21 02:04:50 [INFO]: Epoch 078 - training loss: 0.6199\n",
      "2024-10-21 02:04:53 [INFO]: Epoch 079 - training loss: 0.6171\n",
      "2024-10-21 02:04:56 [INFO]: Epoch 080 - training loss: 0.6124\n",
      "2024-10-21 02:04:59 [INFO]: Epoch 081 - training loss: 0.6171\n",
      "2024-10-21 02:05:01 [INFO]: Epoch 082 - training loss: 0.6112\n",
      "2024-10-21 02:05:04 [INFO]: Epoch 083 - training loss: 0.6158\n",
      "2024-10-21 02:05:07 [INFO]: Epoch 084 - training loss: 0.6138\n",
      "2024-10-21 02:05:10 [INFO]: Epoch 085 - training loss: 0.6172\n",
      "2024-10-21 02:05:13 [INFO]: Epoch 086 - training loss: 0.6147\n",
      "2024-10-21 02:05:15 [INFO]: Epoch 087 - training loss: 0.6200\n",
      "2024-10-21 02:05:18 [INFO]: Epoch 088 - training loss: 0.6215\n",
      "2024-10-21 02:05:21 [INFO]: Epoch 089 - training loss: 0.6097\n",
      "2024-10-21 02:05:24 [INFO]: Epoch 090 - training loss: 0.6086\n",
      "2024-10-21 02:05:27 [INFO]: Epoch 091 - training loss: 0.6175\n",
      "2024-10-21 02:05:29 [INFO]: Epoch 092 - training loss: 0.6107\n",
      "2024-10-21 02:05:32 [INFO]: Epoch 093 - training loss: 0.6159\n",
      "2024-10-21 02:05:35 [INFO]: Epoch 094 - training loss: 0.6097\n",
      "2024-10-21 02:05:38 [INFO]: Epoch 095 - training loss: 0.6201\n",
      "2024-10-21 02:05:41 [INFO]: Epoch 096 - training loss: 0.6199\n",
      "2024-10-21 02:05:44 [INFO]: Epoch 097 - training loss: 0.6186\n",
      "2024-10-21 02:05:46 [INFO]: Epoch 098 - training loss: 0.6162\n",
      "2024-10-21 02:05:49 [INFO]: Epoch 099 - training loss: 0.6126\n",
      "2024-10-21 02:05:52 [INFO]: Epoch 100 - training loss: 0.6103\n",
      "2024-10-21 02:05:52 [INFO]: Finished training. The best model is from epoch#90.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 19/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:11:10 [INFO]: Epoch 001 - training loss: 0.6260\n",
      "2024-10-21 02:11:13 [INFO]: Epoch 002 - training loss: 0.6257\n",
      "2024-10-21 02:11:16 [INFO]: Epoch 003 - training loss: 0.6228\n",
      "2024-10-21 02:11:19 [INFO]: Epoch 004 - training loss: 0.6371\n",
      "2024-10-21 02:11:21 [INFO]: Epoch 005 - training loss: 0.6292\n",
      "2024-10-21 02:11:24 [INFO]: Epoch 006 - training loss: 0.6221\n",
      "2024-10-21 02:11:27 [INFO]: Epoch 007 - training loss: 0.6211\n",
      "2024-10-21 02:11:30 [INFO]: Epoch 008 - training loss: 0.6311\n",
      "2024-10-21 02:11:33 [INFO]: Epoch 009 - training loss: 0.6202\n",
      "2024-10-21 02:11:36 [INFO]: Epoch 010 - training loss: 0.6228\n",
      "2024-10-21 02:11:39 [INFO]: Epoch 011 - training loss: 0.6242\n",
      "2024-10-21 02:11:41 [INFO]: Epoch 012 - training loss: 0.6212\n",
      "2024-10-21 02:11:44 [INFO]: Epoch 013 - training loss: 0.6236\n",
      "2024-10-21 02:11:47 [INFO]: Epoch 014 - training loss: 0.6126\n",
      "2024-10-21 02:11:50 [INFO]: Epoch 015 - training loss: 0.6122\n",
      "2024-10-21 02:11:53 [INFO]: Epoch 016 - training loss: 0.6089\n",
      "2024-10-21 02:11:55 [INFO]: Epoch 017 - training loss: 0.6143\n",
      "2024-10-21 02:11:58 [INFO]: Epoch 018 - training loss: 0.6182\n",
      "2024-10-21 02:12:01 [INFO]: Epoch 019 - training loss: 0.6174\n",
      "2024-10-21 02:12:04 [INFO]: Epoch 020 - training loss: 0.6185\n",
      "2024-10-21 02:12:07 [INFO]: Epoch 021 - training loss: 0.6166\n",
      "2024-10-21 02:12:09 [INFO]: Epoch 022 - training loss: 0.6127\n",
      "2024-10-21 02:12:12 [INFO]: Epoch 023 - training loss: 0.6162\n",
      "2024-10-21 02:12:15 [INFO]: Epoch 024 - training loss: 0.6155\n",
      "2024-10-21 02:12:18 [INFO]: Epoch 025 - training loss: 0.6163\n",
      "2024-10-21 02:12:20 [INFO]: Epoch 026 - training loss: 0.6182\n",
      "2024-10-21 02:12:23 [INFO]: Epoch 027 - training loss: 0.6183\n",
      "2024-10-21 02:12:26 [INFO]: Epoch 028 - training loss: 0.6162\n",
      "2024-10-21 02:12:29 [INFO]: Epoch 029 - training loss: 0.6160\n",
      "2024-10-21 02:12:32 [INFO]: Epoch 030 - training loss: 0.6195\n",
      "2024-10-21 02:12:35 [INFO]: Epoch 031 - training loss: 0.6192\n",
      "2024-10-21 02:12:37 [INFO]: Epoch 032 - training loss: 0.6156\n",
      "2024-10-21 02:12:40 [INFO]: Epoch 033 - training loss: 0.6153\n",
      "2024-10-21 02:12:43 [INFO]: Epoch 034 - training loss: 0.6145\n",
      "2024-10-21 02:12:46 [INFO]: Epoch 035 - training loss: 0.6254\n",
      "2024-10-21 02:12:49 [INFO]: Epoch 036 - training loss: 0.6140\n",
      "2024-10-21 02:12:51 [INFO]: Epoch 037 - training loss: 0.6220\n",
      "2024-10-21 02:12:54 [INFO]: Epoch 038 - training loss: 0.6147\n",
      "2024-10-21 02:12:57 [INFO]: Epoch 039 - training loss: 0.6075\n",
      "2024-10-21 02:13:00 [INFO]: Epoch 040 - training loss: 0.6176\n",
      "2024-10-21 02:13:03 [INFO]: Epoch 041 - training loss: 0.6131\n",
      "2024-10-21 02:13:05 [INFO]: Epoch 042 - training loss: 0.6196\n",
      "2024-10-21 02:13:08 [INFO]: Epoch 043 - training loss: 0.6169\n",
      "2024-10-21 02:13:11 [INFO]: Epoch 044 - training loss: 0.6148\n",
      "2024-10-21 02:13:14 [INFO]: Epoch 045 - training loss: 0.6133\n",
      "2024-10-21 02:13:17 [INFO]: Epoch 046 - training loss: 0.6192\n",
      "2024-10-21 02:13:19 [INFO]: Epoch 047 - training loss: 0.6128\n",
      "2024-10-21 02:13:22 [INFO]: Epoch 048 - training loss: 0.6194\n",
      "2024-10-21 02:13:25 [INFO]: Epoch 049 - training loss: 0.6127\n",
      "2024-10-21 02:13:28 [INFO]: Epoch 050 - training loss: 0.6024\n",
      "2024-10-21 02:13:31 [INFO]: Epoch 051 - training loss: 0.6086\n",
      "2024-10-21 02:13:33 [INFO]: Epoch 052 - training loss: 0.6070\n",
      "2024-10-21 02:13:36 [INFO]: Epoch 053 - training loss: 0.6125\n",
      "2024-10-21 02:13:39 [INFO]: Epoch 054 - training loss: 0.6135\n",
      "2024-10-21 02:13:42 [INFO]: Epoch 055 - training loss: 0.6096\n",
      "2024-10-21 02:13:44 [INFO]: Epoch 056 - training loss: 0.6181\n",
      "2024-10-21 02:13:47 [INFO]: Epoch 057 - training loss: 0.6118\n",
      "2024-10-21 02:13:50 [INFO]: Epoch 058 - training loss: 0.6127\n",
      "2024-10-21 02:13:53 [INFO]: Epoch 059 - training loss: 0.6092\n",
      "2024-10-21 02:13:56 [INFO]: Epoch 060 - training loss: 0.6115\n",
      "2024-10-21 02:13:58 [INFO]: Epoch 061 - training loss: 0.6073\n",
      "2024-10-21 02:14:01 [INFO]: Epoch 062 - training loss: 0.6083\n",
      "2024-10-21 02:14:04 [INFO]: Epoch 063 - training loss: 0.6089\n",
      "2024-10-21 02:14:07 [INFO]: Epoch 064 - training loss: 0.6100\n",
      "2024-10-21 02:14:10 [INFO]: Epoch 065 - training loss: 0.6139\n",
      "2024-10-21 02:14:13 [INFO]: Epoch 066 - training loss: 0.6201\n",
      "2024-10-21 02:14:15 [INFO]: Epoch 067 - training loss: 0.6146\n",
      "2024-10-21 02:14:18 [INFO]: Epoch 068 - training loss: 0.6135\n",
      "2024-10-21 02:14:21 [INFO]: Epoch 069 - training loss: 0.6079\n",
      "2024-10-21 02:14:24 [INFO]: Epoch 070 - training loss: 0.6069\n",
      "2024-10-21 02:14:27 [INFO]: Epoch 071 - training loss: 0.6188\n",
      "2024-10-21 02:14:29 [INFO]: Epoch 072 - training loss: 0.6134\n",
      "2024-10-21 02:14:32 [INFO]: Epoch 073 - training loss: 0.6124\n",
      "2024-10-21 02:14:35 [INFO]: Epoch 074 - training loss: 0.6057\n",
      "2024-10-21 02:14:38 [INFO]: Epoch 075 - training loss: 0.6138\n",
      "2024-10-21 02:14:41 [INFO]: Epoch 076 - training loss: 0.6097\n",
      "2024-10-21 02:14:44 [INFO]: Epoch 077 - training loss: 0.6080\n",
      "2024-10-21 02:14:47 [INFO]: Epoch 078 - training loss: 0.6093\n",
      "2024-10-21 02:14:49 [INFO]: Epoch 079 - training loss: 0.6098\n",
      "2024-10-21 02:14:52 [INFO]: Epoch 080 - training loss: 0.6103\n",
      "2024-10-21 02:14:55 [INFO]: Epoch 081 - training loss: 0.6120\n",
      "2024-10-21 02:14:58 [INFO]: Epoch 082 - training loss: 0.6031\n",
      "2024-10-21 02:15:01 [INFO]: Epoch 083 - training loss: 0.6101\n",
      "2024-10-21 02:15:03 [INFO]: Epoch 084 - training loss: 0.6124\n",
      "2024-10-21 02:15:06 [INFO]: Epoch 085 - training loss: 0.6142\n",
      "2024-10-21 02:15:09 [INFO]: Epoch 086 - training loss: 0.6162\n",
      "2024-10-21 02:15:12 [INFO]: Epoch 087 - training loss: 0.6074\n",
      "2024-10-21 02:15:15 [INFO]: Epoch 088 - training loss: 0.6155\n",
      "2024-10-21 02:15:18 [INFO]: Epoch 089 - training loss: 0.6069\n",
      "2024-10-21 02:15:20 [INFO]: Epoch 090 - training loss: 0.6088\n",
      "2024-10-21 02:15:23 [INFO]: Epoch 091 - training loss: 0.6130\n",
      "2024-10-21 02:15:26 [INFO]: Epoch 092 - training loss: 0.6120\n",
      "2024-10-21 02:15:29 [INFO]: Epoch 093 - training loss: 0.6124\n",
      "2024-10-21 02:15:32 [INFO]: Epoch 094 - training loss: 0.6120\n",
      "2024-10-21 02:15:34 [INFO]: Epoch 095 - training loss: 0.6130\n",
      "2024-10-21 02:15:37 [INFO]: Epoch 096 - training loss: 0.6077\n",
      "2024-10-21 02:15:40 [INFO]: Epoch 097 - training loss: 0.6121\n",
      "2024-10-21 02:15:43 [INFO]: Epoch 098 - training loss: 0.6068\n",
      "2024-10-21 02:15:46 [INFO]: Epoch 099 - training loss: 0.6025\n",
      "2024-10-21 02:15:48 [INFO]: Epoch 100 - training loss: 0.6122\n",
      "2024-10-21 02:15:48 [INFO]: Finished training. The best model is from epoch#50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 20/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:21:07 [INFO]: Epoch 001 - training loss: 0.6328\n",
      "2024-10-21 02:21:10 [INFO]: Epoch 002 - training loss: 0.6296\n",
      "2024-10-21 02:21:12 [INFO]: Epoch 003 - training loss: 0.6297\n",
      "2024-10-21 02:21:15 [INFO]: Epoch 004 - training loss: 0.6323\n",
      "2024-10-21 02:21:18 [INFO]: Epoch 005 - training loss: 0.6250\n",
      "2024-10-21 02:21:21 [INFO]: Epoch 006 - training loss: 0.6228\n",
      "2024-10-21 02:21:24 [INFO]: Epoch 007 - training loss: 0.6299\n",
      "2024-10-21 02:21:26 [INFO]: Epoch 008 - training loss: 0.6272\n",
      "2024-10-21 02:21:29 [INFO]: Epoch 009 - training loss: 0.6186\n",
      "2024-10-21 02:21:32 [INFO]: Epoch 010 - training loss: 0.6135\n",
      "2024-10-21 02:21:35 [INFO]: Epoch 011 - training loss: 0.6249\n",
      "2024-10-21 02:21:38 [INFO]: Epoch 012 - training loss: 0.6314\n",
      "2024-10-21 02:21:41 [INFO]: Epoch 013 - training loss: 0.6210\n",
      "2024-10-21 02:21:43 [INFO]: Epoch 014 - training loss: 0.6254\n",
      "2024-10-21 02:21:46 [INFO]: Epoch 015 - training loss: 0.6184\n",
      "2024-10-21 02:21:49 [INFO]: Epoch 016 - training loss: 0.6207\n",
      "2024-10-21 02:21:52 [INFO]: Epoch 017 - training loss: 0.6227\n",
      "2024-10-21 02:21:55 [INFO]: Epoch 018 - training loss: 0.6250\n",
      "2024-10-21 02:21:58 [INFO]: Epoch 019 - training loss: 0.6203\n",
      "2024-10-21 02:22:00 [INFO]: Epoch 020 - training loss: 0.6143\n",
      "2024-10-21 02:22:03 [INFO]: Epoch 021 - training loss: 0.6236\n",
      "2024-10-21 02:22:06 [INFO]: Epoch 022 - training loss: 0.6175\n",
      "2024-10-21 02:22:09 [INFO]: Epoch 023 - training loss: 0.6281\n",
      "2024-10-21 02:22:11 [INFO]: Epoch 024 - training loss: 0.6210\n",
      "2024-10-21 02:22:14 [INFO]: Epoch 025 - training loss: 0.6182\n",
      "2024-10-21 02:22:17 [INFO]: Epoch 026 - training loss: 0.6131\n",
      "2024-10-21 02:22:20 [INFO]: Epoch 027 - training loss: 0.6151\n",
      "2024-10-21 02:22:23 [INFO]: Epoch 028 - training loss: 0.6167\n",
      "2024-10-21 02:22:26 [INFO]: Epoch 029 - training loss: 0.6248\n",
      "2024-10-21 02:22:28 [INFO]: Epoch 030 - training loss: 0.6266\n",
      "2024-10-21 02:22:31 [INFO]: Epoch 031 - training loss: 0.6164\n",
      "2024-10-21 02:22:34 [INFO]: Epoch 032 - training loss: 0.6109\n",
      "2024-10-21 02:22:37 [INFO]: Epoch 033 - training loss: 0.6142\n",
      "2024-10-21 02:22:40 [INFO]: Epoch 034 - training loss: 0.6166\n",
      "2024-10-21 02:22:42 [INFO]: Epoch 035 - training loss: 0.6220\n",
      "2024-10-21 02:22:45 [INFO]: Epoch 036 - training loss: 0.6256\n",
      "2024-10-21 02:22:48 [INFO]: Epoch 037 - training loss: 0.6181\n",
      "2024-10-21 02:22:51 [INFO]: Epoch 038 - training loss: 0.6224\n",
      "2024-10-21 02:22:54 [INFO]: Epoch 039 - training loss: 0.6120\n",
      "2024-10-21 02:22:57 [INFO]: Epoch 040 - training loss: 0.6106\n",
      "2024-10-21 02:22:59 [INFO]: Epoch 041 - training loss: 0.6161\n",
      "2024-10-21 02:23:02 [INFO]: Epoch 042 - training loss: 0.6181\n",
      "2024-10-21 02:23:05 [INFO]: Epoch 043 - training loss: 0.6106\n",
      "2024-10-21 02:23:08 [INFO]: Epoch 044 - training loss: 0.6095\n",
      "2024-10-21 02:23:11 [INFO]: Epoch 045 - training loss: 0.6182\n",
      "2024-10-21 02:23:13 [INFO]: Epoch 046 - training loss: 0.6145\n",
      "2024-10-21 02:23:16 [INFO]: Epoch 047 - training loss: 0.6236\n",
      "2024-10-21 02:23:19 [INFO]: Epoch 048 - training loss: 0.6178\n",
      "2024-10-21 02:23:22 [INFO]: Epoch 049 - training loss: 0.6179\n",
      "2024-10-21 02:23:25 [INFO]: Epoch 050 - training loss: 0.6203\n",
      "2024-10-21 02:23:28 [INFO]: Epoch 051 - training loss: 0.6139\n",
      "2024-10-21 02:23:30 [INFO]: Epoch 052 - training loss: 0.6154\n",
      "2024-10-21 02:23:33 [INFO]: Epoch 053 - training loss: 0.6241\n",
      "2024-10-21 02:23:36 [INFO]: Epoch 054 - training loss: 0.6203\n",
      "2024-10-21 02:23:39 [INFO]: Epoch 055 - training loss: 0.6100\n",
      "2024-10-21 02:23:42 [INFO]: Epoch 056 - training loss: 0.6198\n",
      "2024-10-21 02:23:44 [INFO]: Epoch 057 - training loss: 0.6093\n",
      "2024-10-21 02:23:47 [INFO]: Epoch 058 - training loss: 0.6176\n",
      "2024-10-21 02:23:50 [INFO]: Epoch 059 - training loss: 0.6166\n",
      "2024-10-21 02:23:53 [INFO]: Epoch 060 - training loss: 0.6176\n",
      "2024-10-21 02:23:56 [INFO]: Epoch 061 - training loss: 0.6129\n",
      "2024-10-21 02:23:59 [INFO]: Epoch 062 - training loss: 0.6123\n",
      "2024-10-21 02:24:02 [INFO]: Epoch 063 - training loss: 0.6050\n",
      "2024-10-21 02:24:04 [INFO]: Epoch 064 - training loss: 0.6169\n",
      "2024-10-21 02:24:07 [INFO]: Epoch 065 - training loss: 0.6033\n",
      "2024-10-21 02:24:10 [INFO]: Epoch 066 - training loss: 0.6118\n",
      "2024-10-21 02:24:13 [INFO]: Epoch 067 - training loss: 0.6168\n",
      "2024-10-21 02:24:16 [INFO]: Epoch 068 - training loss: 0.6171\n",
      "2024-10-21 02:24:18 [INFO]: Epoch 069 - training loss: 0.6151\n",
      "2024-10-21 02:24:21 [INFO]: Epoch 070 - training loss: 0.6226\n",
      "2024-10-21 02:24:24 [INFO]: Epoch 071 - training loss: 0.6104\n",
      "2024-10-21 02:24:27 [INFO]: Epoch 072 - training loss: 0.6136\n",
      "2024-10-21 02:24:30 [INFO]: Epoch 073 - training loss: 0.6151\n",
      "2024-10-21 02:24:32 [INFO]: Epoch 074 - training loss: 0.6157\n",
      "2024-10-21 02:24:35 [INFO]: Epoch 075 - training loss: 0.6218\n",
      "2024-10-21 02:24:38 [INFO]: Epoch 076 - training loss: 0.6141\n",
      "2024-10-21 02:24:41 [INFO]: Epoch 077 - training loss: 0.6108\n",
      "2024-10-21 02:24:44 [INFO]: Epoch 078 - training loss: 0.6120\n",
      "2024-10-21 02:24:47 [INFO]: Epoch 079 - training loss: 0.6156\n",
      "2024-10-21 02:24:49 [INFO]: Epoch 080 - training loss: 0.6115\n",
      "2024-10-21 02:24:52 [INFO]: Epoch 081 - training loss: 0.6166\n",
      "2024-10-21 02:24:55 [INFO]: Epoch 082 - training loss: 0.6162\n",
      "2024-10-21 02:24:58 [INFO]: Epoch 083 - training loss: 0.6147\n",
      "2024-10-21 02:25:01 [INFO]: Epoch 084 - training loss: 0.6080\n",
      "2024-10-21 02:25:03 [INFO]: Epoch 085 - training loss: 0.6035\n",
      "2024-10-21 02:25:06 [INFO]: Epoch 086 - training loss: 0.6058\n",
      "2024-10-21 02:25:09 [INFO]: Epoch 087 - training loss: 0.6037\n",
      "2024-10-21 02:25:12 [INFO]: Epoch 088 - training loss: 0.6072\n",
      "2024-10-21 02:25:15 [INFO]: Epoch 089 - training loss: 0.6090\n",
      "2024-10-21 02:25:18 [INFO]: Epoch 090 - training loss: 0.6068\n",
      "2024-10-21 02:25:20 [INFO]: Epoch 091 - training loss: 0.6129\n",
      "2024-10-21 02:25:23 [INFO]: Epoch 092 - training loss: 0.6117\n",
      "2024-10-21 02:25:26 [INFO]: Epoch 093 - training loss: 0.6061\n",
      "2024-10-21 02:25:29 [INFO]: Epoch 094 - training loss: 0.6152\n",
      "2024-10-21 02:25:32 [INFO]: Epoch 095 - training loss: 0.6172\n",
      "2024-10-21 02:25:35 [INFO]: Epoch 096 - training loss: 0.6107\n",
      "2024-10-21 02:25:37 [INFO]: Epoch 097 - training loss: 0.6053\n",
      "2024-10-21 02:25:40 [INFO]: Epoch 098 - training loss: 0.6103\n",
      "2024-10-21 02:25:43 [INFO]: Epoch 099 - training loss: 0.6176\n",
      "2024-10-21 02:25:46 [INFO]: Epoch 100 - training loss: 0.6054\n",
      "2024-10-21 02:25:46 [INFO]: Finished training. The best model is from epoch#65.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 21/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:31:04 [INFO]: Epoch 001 - training loss: 0.6414\n",
      "2024-10-21 02:31:07 [INFO]: Epoch 002 - training loss: 0.6234\n",
      "2024-10-21 02:31:10 [INFO]: Epoch 003 - training loss: 0.6180\n",
      "2024-10-21 02:31:13 [INFO]: Epoch 004 - training loss: 0.6186\n",
      "2024-10-21 02:31:15 [INFO]: Epoch 005 - training loss: 0.6346\n",
      "2024-10-21 02:31:18 [INFO]: Epoch 006 - training loss: 0.6210\n",
      "2024-10-21 02:31:21 [INFO]: Epoch 007 - training loss: 0.6292\n",
      "2024-10-21 02:31:24 [INFO]: Epoch 008 - training loss: 0.6322\n",
      "2024-10-21 02:31:27 [INFO]: Epoch 009 - training loss: 0.6217\n",
      "2024-10-21 02:31:29 [INFO]: Epoch 010 - training loss: 0.6158\n",
      "2024-10-21 02:31:32 [INFO]: Epoch 011 - training loss: 0.6217\n",
      "2024-10-21 02:31:35 [INFO]: Epoch 012 - training loss: 0.6201\n",
      "2024-10-21 02:31:38 [INFO]: Epoch 013 - training loss: 0.6247\n",
      "2024-10-21 02:31:41 [INFO]: Epoch 014 - training loss: 0.6244\n",
      "2024-10-21 02:31:44 [INFO]: Epoch 015 - training loss: 0.6150\n",
      "2024-10-21 02:31:46 [INFO]: Epoch 016 - training loss: 0.6120\n",
      "2024-10-21 02:31:49 [INFO]: Epoch 017 - training loss: 0.6190\n",
      "2024-10-21 02:31:52 [INFO]: Epoch 018 - training loss: 0.6145\n",
      "2024-10-21 02:31:55 [INFO]: Epoch 019 - training loss: 0.6147\n",
      "2024-10-21 02:31:58 [INFO]: Epoch 020 - training loss: 0.6167\n",
      "2024-10-21 02:32:00 [INFO]: Epoch 021 - training loss: 0.6205\n",
      "2024-10-21 02:32:03 [INFO]: Epoch 022 - training loss: 0.6256\n",
      "2024-10-21 02:32:06 [INFO]: Epoch 023 - training loss: 0.6168\n",
      "2024-10-21 02:32:09 [INFO]: Epoch 024 - training loss: 0.6113\n",
      "2024-10-21 02:32:11 [INFO]: Epoch 025 - training loss: 0.6145\n",
      "2024-10-21 02:32:14 [INFO]: Epoch 026 - training loss: 0.6186\n",
      "2024-10-21 02:32:17 [INFO]: Epoch 027 - training loss: 0.6213\n",
      "2024-10-21 02:32:20 [INFO]: Epoch 028 - training loss: 0.6191\n",
      "2024-10-21 02:32:23 [INFO]: Epoch 029 - training loss: 0.6136\n",
      "2024-10-21 02:32:25 [INFO]: Epoch 030 - training loss: 0.6114\n",
      "2024-10-21 02:32:28 [INFO]: Epoch 031 - training loss: 0.6251\n",
      "2024-10-21 02:32:31 [INFO]: Epoch 032 - training loss: 0.6192\n",
      "2024-10-21 02:32:34 [INFO]: Epoch 033 - training loss: 0.6108\n",
      "2024-10-21 02:32:37 [INFO]: Epoch 034 - training loss: 0.6179\n",
      "2024-10-21 02:32:40 [INFO]: Epoch 035 - training loss: 0.6137\n",
      "2024-10-21 02:32:42 [INFO]: Epoch 036 - training loss: 0.6125\n",
      "2024-10-21 02:32:45 [INFO]: Epoch 037 - training loss: 0.6178\n",
      "2024-10-21 02:32:48 [INFO]: Epoch 038 - training loss: 0.6171\n",
      "2024-10-21 02:32:51 [INFO]: Epoch 039 - training loss: 0.6352\n",
      "2024-10-21 02:32:54 [INFO]: Epoch 040 - training loss: 0.6246\n",
      "2024-10-21 02:32:56 [INFO]: Epoch 041 - training loss: 0.6179\n",
      "2024-10-21 02:32:59 [INFO]: Epoch 042 - training loss: 0.6141\n",
      "2024-10-21 02:33:02 [INFO]: Epoch 043 - training loss: 0.6196\n",
      "2024-10-21 02:33:05 [INFO]: Epoch 044 - training loss: 0.6271\n",
      "2024-10-21 02:33:08 [INFO]: Epoch 045 - training loss: 0.6187\n",
      "2024-10-21 02:33:11 [INFO]: Epoch 046 - training loss: 0.6236\n",
      "2024-10-21 02:33:14 [INFO]: Epoch 047 - training loss: 0.6158\n",
      "2024-10-21 02:33:16 [INFO]: Epoch 048 - training loss: 0.6132\n",
      "2024-10-21 02:33:19 [INFO]: Epoch 049 - training loss: 0.6170\n",
      "2024-10-21 02:33:22 [INFO]: Epoch 050 - training loss: 0.6150\n",
      "2024-10-21 02:33:25 [INFO]: Epoch 051 - training loss: 0.6171\n",
      "2024-10-21 02:33:28 [INFO]: Epoch 052 - training loss: 0.6238\n",
      "2024-10-21 02:33:31 [INFO]: Epoch 053 - training loss: 0.6234\n",
      "2024-10-21 02:33:33 [INFO]: Epoch 054 - training loss: 0.6105\n",
      "2024-10-21 02:33:36 [INFO]: Epoch 055 - training loss: 0.6070\n",
      "2024-10-21 02:33:39 [INFO]: Epoch 056 - training loss: 0.6112\n",
      "2024-10-21 02:33:42 [INFO]: Epoch 057 - training loss: 0.6095\n",
      "2024-10-21 02:33:45 [INFO]: Epoch 058 - training loss: 0.6052\n",
      "2024-10-21 02:33:47 [INFO]: Epoch 059 - training loss: 0.6131\n",
      "2024-10-21 02:33:50 [INFO]: Epoch 060 - training loss: 0.6058\n",
      "2024-10-21 02:33:53 [INFO]: Epoch 061 - training loss: 0.6145\n",
      "2024-10-21 02:33:56 [INFO]: Epoch 062 - training loss: 0.6068\n",
      "2024-10-21 02:33:59 [INFO]: Epoch 063 - training loss: 0.6187\n",
      "2024-10-21 02:34:02 [INFO]: Epoch 064 - training loss: 0.6165\n",
      "2024-10-21 02:34:04 [INFO]: Epoch 065 - training loss: 0.6122\n",
      "2024-10-21 02:34:07 [INFO]: Epoch 066 - training loss: 0.6098\n",
      "2024-10-21 02:34:10 [INFO]: Epoch 067 - training loss: 0.6129\n",
      "2024-10-21 02:34:13 [INFO]: Epoch 068 - training loss: 0.6102\n",
      "2024-10-21 02:34:16 [INFO]: Epoch 069 - training loss: 0.6219\n",
      "2024-10-21 02:34:19 [INFO]: Epoch 070 - training loss: 0.6133\n",
      "2024-10-21 02:34:21 [INFO]: Epoch 071 - training loss: 0.6114\n",
      "2024-10-21 02:34:24 [INFO]: Epoch 072 - training loss: 0.6139\n",
      "2024-10-21 02:34:27 [INFO]: Epoch 073 - training loss: 0.6250\n",
      "2024-10-21 02:34:30 [INFO]: Epoch 074 - training loss: 0.6141\n",
      "2024-10-21 02:34:33 [INFO]: Epoch 075 - training loss: 0.6232\n",
      "2024-10-21 02:34:36 [INFO]: Epoch 076 - training loss: 0.6127\n",
      "2024-10-21 02:34:38 [INFO]: Epoch 077 - training loss: 0.6175\n",
      "2024-10-21 02:34:41 [INFO]: Epoch 078 - training loss: 0.6133\n",
      "2024-10-21 02:34:44 [INFO]: Epoch 079 - training loss: 0.6137\n",
      "2024-10-21 02:34:47 [INFO]: Epoch 080 - training loss: 0.6195\n",
      "2024-10-21 02:34:50 [INFO]: Epoch 081 - training loss: 0.6037\n",
      "2024-10-21 02:34:53 [INFO]: Epoch 082 - training loss: 0.6005\n",
      "2024-10-21 02:34:55 [INFO]: Epoch 083 - training loss: 0.6070\n",
      "2024-10-21 02:34:58 [INFO]: Epoch 084 - training loss: 0.6117\n",
      "2024-10-21 02:35:01 [INFO]: Epoch 085 - training loss: 0.6095\n",
      "2024-10-21 02:35:04 [INFO]: Epoch 086 - training loss: 0.6113\n",
      "2024-10-21 02:35:07 [INFO]: Epoch 087 - training loss: 0.6264\n",
      "2024-10-21 02:35:09 [INFO]: Epoch 088 - training loss: 0.6156\n",
      "2024-10-21 02:35:12 [INFO]: Epoch 089 - training loss: 0.6066\n",
      "2024-10-21 02:35:15 [INFO]: Epoch 090 - training loss: 0.6065\n",
      "2024-10-21 02:35:17 [INFO]: Epoch 091 - training loss: 0.6094\n",
      "2024-10-21 02:35:20 [INFO]: Epoch 092 - training loss: 0.6060\n",
      "2024-10-21 02:35:23 [INFO]: Epoch 093 - training loss: 0.6080\n",
      "2024-10-21 02:35:26 [INFO]: Epoch 094 - training loss: 0.6052\n",
      "2024-10-21 02:35:28 [INFO]: Epoch 095 - training loss: 0.6126\n",
      "2024-10-21 02:35:31 [INFO]: Epoch 096 - training loss: 0.6139\n",
      "2024-10-21 02:35:34 [INFO]: Epoch 097 - training loss: 0.6165\n",
      "2024-10-21 02:35:37 [INFO]: Epoch 098 - training loss: 0.6085\n",
      "2024-10-21 02:35:40 [INFO]: Epoch 099 - training loss: 0.6113\n",
      "2024-10-21 02:35:42 [INFO]: Epoch 100 - training loss: 0.6084\n",
      "2024-10-21 02:35:42 [INFO]: Finished training. The best model is from epoch#82.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 22/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:41:00 [INFO]: Epoch 001 - training loss: 0.6208\n",
      "2024-10-21 02:41:03 [INFO]: Epoch 002 - training loss: 0.6281\n",
      "2024-10-21 02:41:06 [INFO]: Epoch 003 - training loss: 0.6296\n",
      "2024-10-21 02:41:09 [INFO]: Epoch 004 - training loss: 0.6208\n",
      "2024-10-21 02:41:12 [INFO]: Epoch 005 - training loss: 0.6177\n",
      "2024-10-21 02:41:15 [INFO]: Epoch 006 - training loss: 0.6223\n",
      "2024-10-21 02:41:17 [INFO]: Epoch 007 - training loss: 0.6158\n",
      "2024-10-21 02:41:20 [INFO]: Epoch 008 - training loss: 0.6213\n",
      "2024-10-21 02:41:23 [INFO]: Epoch 009 - training loss: 0.6225\n",
      "2024-10-21 02:41:26 [INFO]: Epoch 010 - training loss: 0.6171\n",
      "2024-10-21 02:41:29 [INFO]: Epoch 011 - training loss: 0.6170\n",
      "2024-10-21 02:41:31 [INFO]: Epoch 012 - training loss: 0.6221\n",
      "2024-10-21 02:41:34 [INFO]: Epoch 013 - training loss: 0.6107\n",
      "2024-10-21 02:41:37 [INFO]: Epoch 014 - training loss: 0.6215\n",
      "2024-10-21 02:41:40 [INFO]: Epoch 015 - training loss: 0.6055\n",
      "2024-10-21 02:41:43 [INFO]: Epoch 016 - training loss: 0.6175\n",
      "2024-10-21 02:41:45 [INFO]: Epoch 017 - training loss: 0.6181\n",
      "2024-10-21 02:41:48 [INFO]: Epoch 018 - training loss: 0.6205\n",
      "2024-10-21 02:41:51 [INFO]: Epoch 019 - training loss: 0.6156\n",
      "2024-10-21 02:41:54 [INFO]: Epoch 020 - training loss: 0.6240\n",
      "2024-10-21 02:41:57 [INFO]: Epoch 021 - training loss: 0.6162\n",
      "2024-10-21 02:42:00 [INFO]: Epoch 022 - training loss: 0.6132\n",
      "2024-10-21 02:42:03 [INFO]: Epoch 023 - training loss: 0.6056\n",
      "2024-10-21 02:42:05 [INFO]: Epoch 024 - training loss: 0.6121\n",
      "2024-10-21 02:42:08 [INFO]: Epoch 025 - training loss: 0.6155\n",
      "2024-10-21 02:42:11 [INFO]: Epoch 026 - training loss: 0.6081\n",
      "2024-10-21 02:42:14 [INFO]: Epoch 027 - training loss: 0.6136\n",
      "2024-10-21 02:42:16 [INFO]: Epoch 028 - training loss: 0.6120\n",
      "2024-10-21 02:42:19 [INFO]: Epoch 029 - training loss: 0.6105\n",
      "2024-10-21 02:42:22 [INFO]: Epoch 030 - training loss: 0.6165\n",
      "2024-10-21 02:42:24 [INFO]: Epoch 031 - training loss: 0.6162\n",
      "2024-10-21 02:42:27 [INFO]: Epoch 032 - training loss: 0.6144\n",
      "2024-10-21 02:42:30 [INFO]: Epoch 033 - training loss: 0.6209\n",
      "2024-10-21 02:42:33 [INFO]: Epoch 034 - training loss: 0.6092\n",
      "2024-10-21 02:42:35 [INFO]: Epoch 035 - training loss: 0.6166\n",
      "2024-10-21 02:42:38 [INFO]: Epoch 036 - training loss: 0.6169\n",
      "2024-10-21 02:42:41 [INFO]: Epoch 037 - training loss: 0.6101\n",
      "2024-10-21 02:42:44 [INFO]: Epoch 038 - training loss: 0.6027\n",
      "2024-10-21 02:42:46 [INFO]: Epoch 039 - training loss: 0.6093\n",
      "2024-10-21 02:42:49 [INFO]: Epoch 040 - training loss: 0.6152\n",
      "2024-10-21 02:42:52 [INFO]: Epoch 041 - training loss: 0.6126\n",
      "2024-10-21 02:42:55 [INFO]: Epoch 042 - training loss: 0.6164\n",
      "2024-10-21 02:42:58 [INFO]: Epoch 043 - training loss: 0.6135\n",
      "2024-10-21 02:43:00 [INFO]: Epoch 044 - training loss: 0.6138\n",
      "2024-10-21 02:43:03 [INFO]: Epoch 045 - training loss: 0.6229\n",
      "2024-10-21 02:43:06 [INFO]: Epoch 046 - training loss: 0.6122\n",
      "2024-10-21 02:43:09 [INFO]: Epoch 047 - training loss: 0.6084\n",
      "2024-10-21 02:43:12 [INFO]: Epoch 048 - training loss: 0.6111\n",
      "2024-10-21 02:43:15 [INFO]: Epoch 049 - training loss: 0.6103\n",
      "2024-10-21 02:43:17 [INFO]: Epoch 050 - training loss: 0.6141\n",
      "2024-10-21 02:43:20 [INFO]: Epoch 051 - training loss: 0.6089\n",
      "2024-10-21 02:43:23 [INFO]: Epoch 052 - training loss: 0.6025\n",
      "2024-10-21 02:43:26 [INFO]: Epoch 053 - training loss: 0.6134\n",
      "2024-10-21 02:43:29 [INFO]: Epoch 054 - training loss: 0.6148\n",
      "2024-10-21 02:43:31 [INFO]: Epoch 055 - training loss: 0.6070\n",
      "2024-10-21 02:43:34 [INFO]: Epoch 056 - training loss: 0.6166\n",
      "2024-10-21 02:43:37 [INFO]: Epoch 057 - training loss: 0.6106\n",
      "2024-10-21 02:43:40 [INFO]: Epoch 058 - training loss: 0.6107\n",
      "2024-10-21 02:43:43 [INFO]: Epoch 059 - training loss: 0.6137\n",
      "2024-10-21 02:43:45 [INFO]: Epoch 060 - training loss: 0.6116\n",
      "2024-10-21 02:43:48 [INFO]: Epoch 061 - training loss: 0.6170\n",
      "2024-10-21 02:43:51 [INFO]: Epoch 062 - training loss: 0.6094\n",
      "2024-10-21 02:43:54 [INFO]: Epoch 063 - training loss: 0.6090\n",
      "2024-10-21 02:43:57 [INFO]: Epoch 064 - training loss: 0.6117\n",
      "2024-10-21 02:43:59 [INFO]: Epoch 065 - training loss: 0.6061\n",
      "2024-10-21 02:44:02 [INFO]: Epoch 066 - training loss: 0.6148\n",
      "2024-10-21 02:44:05 [INFO]: Epoch 067 - training loss: 0.6190\n",
      "2024-10-21 02:44:08 [INFO]: Epoch 068 - training loss: 0.6123\n",
      "2024-10-21 02:44:11 [INFO]: Epoch 069 - training loss: 0.6138\n",
      "2024-10-21 02:44:13 [INFO]: Epoch 070 - training loss: 0.6096\n",
      "2024-10-21 02:44:16 [INFO]: Epoch 071 - training loss: 0.6093\n",
      "2024-10-21 02:44:19 [INFO]: Epoch 072 - training loss: 0.6228\n",
      "2024-10-21 02:44:22 [INFO]: Epoch 073 - training loss: 0.6045\n",
      "2024-10-21 02:44:25 [INFO]: Epoch 074 - training loss: 0.6118\n",
      "2024-10-21 02:44:27 [INFO]: Epoch 075 - training loss: 0.5999\n",
      "2024-10-21 02:44:30 [INFO]: Epoch 076 - training loss: 0.6112\n",
      "2024-10-21 02:44:33 [INFO]: Epoch 077 - training loss: 0.6140\n",
      "2024-10-21 02:44:36 [INFO]: Epoch 078 - training loss: 0.6076\n",
      "2024-10-21 02:44:39 [INFO]: Epoch 079 - training loss: 0.6078\n",
      "2024-10-21 02:44:41 [INFO]: Epoch 080 - training loss: 0.6061\n",
      "2024-10-21 02:44:44 [INFO]: Epoch 081 - training loss: 0.6056\n",
      "2024-10-21 02:44:47 [INFO]: Epoch 082 - training loss: 0.6076\n",
      "2024-10-21 02:44:50 [INFO]: Epoch 083 - training loss: 0.6046\n",
      "2024-10-21 02:44:53 [INFO]: Epoch 084 - training loss: 0.6066\n",
      "2024-10-21 02:44:56 [INFO]: Epoch 085 - training loss: 0.6009\n",
      "2024-10-21 02:44:59 [INFO]: Epoch 086 - training loss: 0.6087\n",
      "2024-10-21 02:45:01 [INFO]: Epoch 087 - training loss: 0.6086\n",
      "2024-10-21 02:45:04 [INFO]: Epoch 088 - training loss: 0.6172\n",
      "2024-10-21 02:45:07 [INFO]: Epoch 089 - training loss: 0.6110\n",
      "2024-10-21 02:45:10 [INFO]: Epoch 090 - training loss: 0.6095\n",
      "2024-10-21 02:45:13 [INFO]: Epoch 091 - training loss: 0.6049\n",
      "2024-10-21 02:45:15 [INFO]: Epoch 092 - training loss: 0.6124\n",
      "2024-10-21 02:45:18 [INFO]: Epoch 093 - training loss: 0.6072\n",
      "2024-10-21 02:45:21 [INFO]: Epoch 094 - training loss: 0.6131\n",
      "2024-10-21 02:45:24 [INFO]: Epoch 095 - training loss: 0.6125\n",
      "2024-10-21 02:45:27 [INFO]: Epoch 096 - training loss: 0.6128\n",
      "2024-10-21 02:45:29 [INFO]: Epoch 097 - training loss: 0.6076\n",
      "2024-10-21 02:45:32 [INFO]: Epoch 098 - training loss: 0.6079\n",
      "2024-10-21 02:45:35 [INFO]: Epoch 099 - training loss: 0.6103\n",
      "2024-10-21 02:45:38 [INFO]: Epoch 100 - training loss: 0.6109\n",
      "2024-10-21 02:45:38 [INFO]: Finished training. The best model is from epoch#75.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 23/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 02:50:57 [INFO]: Epoch 001 - training loss: 0.6240\n",
      "2024-10-21 02:51:00 [INFO]: Epoch 002 - training loss: 0.6170\n",
      "2024-10-21 02:51:03 [INFO]: Epoch 003 - training loss: 0.6257\n",
      "2024-10-21 02:51:06 [INFO]: Epoch 004 - training loss: 0.6107\n",
      "2024-10-21 02:51:09 [INFO]: Epoch 005 - training loss: 0.6204\n",
      "2024-10-21 02:51:11 [INFO]: Epoch 006 - training loss: 0.6139\n",
      "2024-10-21 02:51:14 [INFO]: Epoch 007 - training loss: 0.6257\n",
      "2024-10-21 02:51:17 [INFO]: Epoch 008 - training loss: 0.6141\n",
      "2024-10-21 02:51:20 [INFO]: Epoch 009 - training loss: 0.6174\n",
      "2024-10-21 02:51:23 [INFO]: Epoch 010 - training loss: 0.6206\n",
      "2024-10-21 02:51:26 [INFO]: Epoch 011 - training loss: 0.6107\n",
      "2024-10-21 02:51:28 [INFO]: Epoch 012 - training loss: 0.6286\n",
      "2024-10-21 02:51:31 [INFO]: Epoch 013 - training loss: 0.6118\n",
      "2024-10-21 02:51:34 [INFO]: Epoch 014 - training loss: 0.6086\n",
      "2024-10-21 02:51:37 [INFO]: Epoch 015 - training loss: 0.6122\n",
      "2024-10-21 02:51:40 [INFO]: Epoch 016 - training loss: 0.6172\n",
      "2024-10-21 02:51:42 [INFO]: Epoch 017 - training loss: 0.6182\n",
      "2024-10-21 02:51:45 [INFO]: Epoch 018 - training loss: 0.6104\n",
      "2024-10-21 02:51:48 [INFO]: Epoch 019 - training loss: 0.6122\n",
      "2024-10-21 02:51:51 [INFO]: Epoch 020 - training loss: 0.6082\n",
      "2024-10-21 02:51:54 [INFO]: Epoch 021 - training loss: 0.6066\n",
      "2024-10-21 02:51:57 [INFO]: Epoch 022 - training loss: 0.6137\n",
      "2024-10-21 02:52:00 [INFO]: Epoch 023 - training loss: 0.6087\n",
      "2024-10-21 02:52:02 [INFO]: Epoch 024 - training loss: 0.6056\n",
      "2024-10-21 02:52:05 [INFO]: Epoch 025 - training loss: 0.6150\n",
      "2024-10-21 02:52:08 [INFO]: Epoch 026 - training loss: 0.6216\n",
      "2024-10-21 02:52:11 [INFO]: Epoch 027 - training loss: 0.6211\n",
      "2024-10-21 02:52:14 [INFO]: Epoch 028 - training loss: 0.6044\n",
      "2024-10-21 02:52:16 [INFO]: Epoch 029 - training loss: 0.6014\n",
      "2024-10-21 02:52:19 [INFO]: Epoch 030 - training loss: 0.6091\n",
      "2024-10-21 02:52:22 [INFO]: Epoch 031 - training loss: 0.6132\n",
      "2024-10-21 02:52:25 [INFO]: Epoch 032 - training loss: 0.6068\n",
      "2024-10-21 02:52:28 [INFO]: Epoch 033 - training loss: 0.6130\n",
      "2024-10-21 02:52:30 [INFO]: Epoch 034 - training loss: 0.6092\n",
      "2024-10-21 02:52:33 [INFO]: Epoch 035 - training loss: 0.6015\n",
      "2024-10-21 02:52:36 [INFO]: Epoch 036 - training loss: 0.6072\n",
      "2024-10-21 02:52:39 [INFO]: Epoch 037 - training loss: 0.6097\n",
      "2024-10-21 02:52:42 [INFO]: Epoch 038 - training loss: 0.6065\n",
      "2024-10-21 02:52:45 [INFO]: Epoch 039 - training loss: 0.6202\n",
      "2024-10-21 02:52:47 [INFO]: Epoch 040 - training loss: 0.6094\n",
      "2024-10-21 02:52:50 [INFO]: Epoch 041 - training loss: 0.6086\n",
      "2024-10-21 02:52:53 [INFO]: Epoch 042 - training loss: 0.6133\n",
      "2024-10-21 02:52:56 [INFO]: Epoch 043 - training loss: 0.6036\n",
      "2024-10-21 02:52:59 [INFO]: Epoch 044 - training loss: 0.6046\n",
      "2024-10-21 02:53:01 [INFO]: Epoch 045 - training loss: 0.6057\n",
      "2024-10-21 02:53:04 [INFO]: Epoch 046 - training loss: 0.6083\n",
      "2024-10-21 02:53:07 [INFO]: Epoch 047 - training loss: 0.6085\n",
      "2024-10-21 02:53:10 [INFO]: Epoch 048 - training loss: 0.6003\n",
      "2024-10-21 02:53:13 [INFO]: Epoch 049 - training loss: 0.6044\n",
      "2024-10-21 02:53:15 [INFO]: Epoch 050 - training loss: 0.6060\n",
      "2024-10-21 02:53:18 [INFO]: Epoch 051 - training loss: 0.6059\n",
      "2024-10-21 02:53:21 [INFO]: Epoch 052 - training loss: 0.6014\n",
      "2024-10-21 02:53:24 [INFO]: Epoch 053 - training loss: 0.6075\n",
      "2024-10-21 02:53:27 [INFO]: Epoch 054 - training loss: 0.6004\n",
      "2024-10-21 02:53:29 [INFO]: Epoch 055 - training loss: 0.6066\n",
      "2024-10-21 02:53:32 [INFO]: Epoch 056 - training loss: 0.6037\n",
      "2024-10-21 02:53:35 [INFO]: Epoch 057 - training loss: 0.6053\n",
      "2024-10-21 02:53:37 [INFO]: Epoch 058 - training loss: 0.6048\n",
      "2024-10-21 02:53:40 [INFO]: Epoch 059 - training loss: 0.6102\n",
      "2024-10-21 02:53:43 [INFO]: Epoch 060 - training loss: 0.6057\n",
      "2024-10-21 02:53:45 [INFO]: Epoch 061 - training loss: 0.6029\n",
      "2024-10-21 02:53:48 [INFO]: Epoch 062 - training loss: 0.6046\n",
      "2024-10-21 02:53:51 [INFO]: Epoch 063 - training loss: 0.6072\n",
      "2024-10-21 02:53:54 [INFO]: Epoch 064 - training loss: 0.6051\n",
      "2024-10-21 02:53:56 [INFO]: Epoch 065 - training loss: 0.6022\n",
      "2024-10-21 02:53:59 [INFO]: Epoch 066 - training loss: 0.6027\n",
      "2024-10-21 02:54:02 [INFO]: Epoch 067 - training loss: 0.6033\n",
      "2024-10-21 02:54:05 [INFO]: Epoch 068 - training loss: 0.5971\n",
      "2024-10-21 02:54:07 [INFO]: Epoch 069 - training loss: 0.6072\n",
      "2024-10-21 02:54:10 [INFO]: Epoch 070 - training loss: 0.6034\n",
      "2024-10-21 02:54:13 [INFO]: Epoch 071 - training loss: 0.6057\n",
      "2024-10-21 02:54:16 [INFO]: Epoch 072 - training loss: 0.5948\n",
      "2024-10-21 02:54:19 [INFO]: Epoch 073 - training loss: 0.6093\n",
      "2024-10-21 02:54:22 [INFO]: Epoch 074 - training loss: 0.6104\n",
      "2024-10-21 02:54:24 [INFO]: Epoch 075 - training loss: 0.6059\n",
      "2024-10-21 02:54:27 [INFO]: Epoch 076 - training loss: 0.6198\n",
      "2024-10-21 02:54:30 [INFO]: Epoch 077 - training loss: 0.5999\n",
      "2024-10-21 02:54:33 [INFO]: Epoch 078 - training loss: 0.5992\n",
      "2024-10-21 02:54:36 [INFO]: Epoch 079 - training loss: 0.5972\n",
      "2024-10-21 02:54:39 [INFO]: Epoch 080 - training loss: 0.5960\n",
      "2024-10-21 02:54:41 [INFO]: Epoch 081 - training loss: 0.6070\n",
      "2024-10-21 02:54:44 [INFO]: Epoch 082 - training loss: 0.6039\n",
      "2024-10-21 02:54:47 [INFO]: Epoch 083 - training loss: 0.6055\n",
      "2024-10-21 02:54:50 [INFO]: Epoch 084 - training loss: 0.6064\n",
      "2024-10-21 02:54:53 [INFO]: Epoch 085 - training loss: 0.6015\n",
      "2024-10-21 02:54:56 [INFO]: Epoch 086 - training loss: 0.6056\n",
      "2024-10-21 02:54:58 [INFO]: Epoch 087 - training loss: 0.6017\n",
      "2024-10-21 02:55:01 [INFO]: Epoch 088 - training loss: 0.6091\n",
      "2024-10-21 02:55:04 [INFO]: Epoch 089 - training loss: 0.5957\n",
      "2024-10-21 02:55:07 [INFO]: Epoch 090 - training loss: 0.6071\n",
      "2024-10-21 02:55:10 [INFO]: Epoch 091 - training loss: 0.6043\n",
      "2024-10-21 02:55:12 [INFO]: Epoch 092 - training loss: 0.6017\n",
      "2024-10-21 02:55:15 [INFO]: Epoch 093 - training loss: 0.6053\n",
      "2024-10-21 02:55:18 [INFO]: Epoch 094 - training loss: 0.6044\n",
      "2024-10-21 02:55:21 [INFO]: Epoch 095 - training loss: 0.6038\n",
      "2024-10-21 02:55:24 [INFO]: Epoch 096 - training loss: 0.5977\n",
      "2024-10-21 02:55:27 [INFO]: Epoch 097 - training loss: 0.6062\n",
      "2024-10-21 02:55:29 [INFO]: Epoch 098 - training loss: 0.6027\n",
      "2024-10-21 02:55:32 [INFO]: Epoch 099 - training loss: 0.6032\n",
      "2024-10-21 02:55:35 [INFO]: Epoch 100 - training loss: 0.6006\n",
      "2024-10-21 02:55:35 [INFO]: Finished training. The best model is from epoch#72.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 24/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 03:00:53 [INFO]: Epoch 001 - training loss: 0.6147\n",
      "2024-10-21 03:00:55 [INFO]: Epoch 002 - training loss: 0.6190\n",
      "2024-10-21 03:00:58 [INFO]: Epoch 003 - training loss: 0.6160\n",
      "2024-10-21 03:01:01 [INFO]: Epoch 004 - training loss: 0.6130\n",
      "2024-10-21 03:01:04 [INFO]: Epoch 005 - training loss: 0.6080\n",
      "2024-10-21 03:01:07 [INFO]: Epoch 006 - training loss: 0.6008\n",
      "2024-10-21 03:01:10 [INFO]: Epoch 007 - training loss: 0.6138\n",
      "2024-10-21 03:01:12 [INFO]: Epoch 008 - training loss: 0.6156\n",
      "2024-10-21 03:01:15 [INFO]: Epoch 009 - training loss: 0.6108\n",
      "2024-10-21 03:01:18 [INFO]: Epoch 010 - training loss: 0.6148\n",
      "2024-10-21 03:01:21 [INFO]: Epoch 011 - training loss: 0.6069\n",
      "2024-10-21 03:01:24 [INFO]: Epoch 012 - training loss: 0.6092\n",
      "2024-10-21 03:01:27 [INFO]: Epoch 013 - training loss: 0.6037\n",
      "2024-10-21 03:01:29 [INFO]: Epoch 014 - training loss: 0.6006\n",
      "2024-10-21 03:01:32 [INFO]: Epoch 015 - training loss: 0.6005\n",
      "2024-10-21 03:01:35 [INFO]: Epoch 016 - training loss: 0.6034\n",
      "2024-10-21 03:01:38 [INFO]: Epoch 017 - training loss: 0.6028\n",
      "2024-10-21 03:01:41 [INFO]: Epoch 018 - training loss: 0.6042\n",
      "2024-10-21 03:01:44 [INFO]: Epoch 019 - training loss: 0.6068\n",
      "2024-10-21 03:01:46 [INFO]: Epoch 020 - training loss: 0.6009\n",
      "2024-10-21 03:01:49 [INFO]: Epoch 021 - training loss: 0.6066\n",
      "2024-10-21 03:01:52 [INFO]: Epoch 022 - training loss: 0.6002\n",
      "2024-10-21 03:01:55 [INFO]: Epoch 023 - training loss: 0.6104\n",
      "2024-10-21 03:01:58 [INFO]: Epoch 024 - training loss: 0.6029\n",
      "2024-10-21 03:02:00 [INFO]: Epoch 025 - training loss: 0.6052\n",
      "2024-10-21 03:02:03 [INFO]: Epoch 026 - training loss: 0.6121\n",
      "2024-10-21 03:02:06 [INFO]: Epoch 027 - training loss: 0.6090\n",
      "2024-10-21 03:02:09 [INFO]: Epoch 028 - training loss: 0.6065\n",
      "2024-10-21 03:02:12 [INFO]: Epoch 029 - training loss: 0.5978\n",
      "2024-10-21 03:02:15 [INFO]: Epoch 030 - training loss: 0.6043\n",
      "2024-10-21 03:02:17 [INFO]: Epoch 031 - training loss: 0.5990\n",
      "2024-10-21 03:02:20 [INFO]: Epoch 032 - training loss: 0.6014\n",
      "2024-10-21 03:02:23 [INFO]: Epoch 033 - training loss: 0.6006\n",
      "2024-10-21 03:02:26 [INFO]: Epoch 034 - training loss: 0.6041\n",
      "2024-10-21 03:02:29 [INFO]: Epoch 035 - training loss: 0.5993\n",
      "2024-10-21 03:02:31 [INFO]: Epoch 036 - training loss: 0.6000\n",
      "2024-10-21 03:02:34 [INFO]: Epoch 037 - training loss: 0.6032\n",
      "2024-10-21 03:02:37 [INFO]: Epoch 038 - training loss: 0.6000\n",
      "2024-10-21 03:02:40 [INFO]: Epoch 039 - training loss: 0.6050\n",
      "2024-10-21 03:02:43 [INFO]: Epoch 040 - training loss: 0.5996\n",
      "2024-10-21 03:02:45 [INFO]: Epoch 041 - training loss: 0.5939\n",
      "2024-10-21 03:02:48 [INFO]: Epoch 042 - training loss: 0.5991\n",
      "2024-10-21 03:02:51 [INFO]: Epoch 043 - training loss: 0.5977\n",
      "2024-10-21 03:02:54 [INFO]: Epoch 044 - training loss: 0.6018\n",
      "2024-10-21 03:02:57 [INFO]: Epoch 045 - training loss: 0.5996\n",
      "2024-10-21 03:02:59 [INFO]: Epoch 046 - training loss: 0.5982\n",
      "2024-10-21 03:03:02 [INFO]: Epoch 047 - training loss: 0.5978\n",
      "2024-10-21 03:03:05 [INFO]: Epoch 048 - training loss: 0.6035\n",
      "2024-10-21 03:03:08 [INFO]: Epoch 049 - training loss: 0.5987\n",
      "2024-10-21 03:03:11 [INFO]: Epoch 050 - training loss: 0.6086\n",
      "2024-10-21 03:03:14 [INFO]: Epoch 051 - training loss: 0.6012\n",
      "2024-10-21 03:03:16 [INFO]: Epoch 052 - training loss: 0.6020\n",
      "2024-10-21 03:03:19 [INFO]: Epoch 053 - training loss: 0.5998\n",
      "2024-10-21 03:03:22 [INFO]: Epoch 054 - training loss: 0.6028\n",
      "2024-10-21 03:03:25 [INFO]: Epoch 055 - training loss: 0.5964\n",
      "2024-10-21 03:03:28 [INFO]: Epoch 056 - training loss: 0.6004\n",
      "2024-10-21 03:03:31 [INFO]: Epoch 057 - training loss: 0.5989\n",
      "2024-10-21 03:03:33 [INFO]: Epoch 058 - training loss: 0.6094\n",
      "2024-10-21 03:03:36 [INFO]: Epoch 059 - training loss: 0.5955\n",
      "2024-10-21 03:03:39 [INFO]: Epoch 060 - training loss: 0.6059\n",
      "2024-10-21 03:03:42 [INFO]: Epoch 061 - training loss: 0.5988\n",
      "2024-10-21 03:03:45 [INFO]: Epoch 062 - training loss: 0.5977\n",
      "2024-10-21 03:03:47 [INFO]: Epoch 063 - training loss: 0.5996\n",
      "2024-10-21 03:03:50 [INFO]: Epoch 064 - training loss: 0.6098\n",
      "2024-10-21 03:03:53 [INFO]: Epoch 065 - training loss: 0.6079\n",
      "2024-10-21 03:03:56 [INFO]: Epoch 066 - training loss: 0.5993\n",
      "2024-10-21 03:03:59 [INFO]: Epoch 067 - training loss: 0.5975\n",
      "2024-10-21 03:04:01 [INFO]: Epoch 068 - training loss: 0.5977\n",
      "2024-10-21 03:04:04 [INFO]: Epoch 069 - training loss: 0.6075\n",
      "2024-10-21 03:04:07 [INFO]: Epoch 070 - training loss: 0.5958\n",
      "2024-10-21 03:04:10 [INFO]: Epoch 071 - training loss: 0.5945\n",
      "2024-10-21 03:04:13 [INFO]: Epoch 072 - training loss: 0.6011\n",
      "2024-10-21 03:04:16 [INFO]: Epoch 073 - training loss: 0.5974\n",
      "2024-10-21 03:04:18 [INFO]: Epoch 074 - training loss: 0.5967\n",
      "2024-10-21 03:04:21 [INFO]: Epoch 075 - training loss: 0.5974\n",
      "2024-10-21 03:04:24 [INFO]: Epoch 076 - training loss: 0.6059\n",
      "2024-10-21 03:04:27 [INFO]: Epoch 077 - training loss: 0.5973\n",
      "2024-10-21 03:04:30 [INFO]: Epoch 078 - training loss: 0.5921\n",
      "2024-10-21 03:04:33 [INFO]: Epoch 079 - training loss: 0.5949\n",
      "2024-10-21 03:04:35 [INFO]: Epoch 080 - training loss: 0.5952\n",
      "2024-10-21 03:04:38 [INFO]: Epoch 081 - training loss: 0.6057\n",
      "2024-10-21 03:04:41 [INFO]: Epoch 082 - training loss: 0.6051\n",
      "2024-10-21 03:04:44 [INFO]: Epoch 083 - training loss: 0.5945\n",
      "2024-10-21 03:04:47 [INFO]: Epoch 084 - training loss: 0.6034\n",
      "2024-10-21 03:04:50 [INFO]: Epoch 085 - training loss: 0.5967\n",
      "2024-10-21 03:04:52 [INFO]: Epoch 086 - training loss: 0.6036\n",
      "2024-10-21 03:04:55 [INFO]: Epoch 087 - training loss: 0.5975\n",
      "2024-10-21 03:04:58 [INFO]: Epoch 088 - training loss: 0.6005\n",
      "2024-10-21 03:05:01 [INFO]: Epoch 089 - training loss: 0.5988\n",
      "2024-10-21 03:05:04 [INFO]: Epoch 090 - training loss: 0.5981\n",
      "2024-10-21 03:05:06 [INFO]: Epoch 091 - training loss: 0.5935\n",
      "2024-10-21 03:05:09 [INFO]: Epoch 092 - training loss: 0.5930\n",
      "2024-10-21 03:05:12 [INFO]: Epoch 093 - training loss: 0.6021\n",
      "2024-10-21 03:05:15 [INFO]: Epoch 094 - training loss: 0.5995\n",
      "2024-10-21 03:05:18 [INFO]: Epoch 095 - training loss: 0.6027\n",
      "2024-10-21 03:05:20 [INFO]: Epoch 096 - training loss: 0.5996\n",
      "2024-10-21 03:05:23 [INFO]: Epoch 097 - training loss: 0.6047\n",
      "2024-10-21 03:05:26 [INFO]: Epoch 098 - training loss: 0.6076\n",
      "2024-10-21 03:05:29 [INFO]: Epoch 099 - training loss: 0.5933\n",
      "2024-10-21 03:05:32 [INFO]: Epoch 100 - training loss: 0.6032\n",
      "2024-10-21 03:05:32 [INFO]: Finished training. The best model is from epoch#78.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 25/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 03:10:49 [INFO]: Epoch 001 - training loss: 0.6153\n",
      "2024-10-21 03:10:52 [INFO]: Epoch 002 - training loss: 0.6225\n",
      "2024-10-21 03:10:55 [INFO]: Epoch 003 - training loss: 0.6166\n",
      "2024-10-21 03:10:58 [INFO]: Epoch 004 - training loss: 0.6168\n",
      "2024-10-21 03:11:01 [INFO]: Epoch 005 - training loss: 0.6328\n",
      "2024-10-21 03:11:04 [INFO]: Epoch 006 - training loss: 0.6180\n",
      "2024-10-21 03:11:06 [INFO]: Epoch 007 - training loss: 0.6126\n",
      "2024-10-21 03:11:09 [INFO]: Epoch 008 - training loss: 0.6081\n",
      "2024-10-21 03:11:12 [INFO]: Epoch 009 - training loss: 0.6105\n",
      "2024-10-21 03:11:15 [INFO]: Epoch 010 - training loss: 0.6142\n",
      "2024-10-21 03:11:18 [INFO]: Epoch 011 - training loss: 0.6087\n",
      "2024-10-21 03:11:20 [INFO]: Epoch 012 - training loss: 0.6097\n",
      "2024-10-21 03:11:23 [INFO]: Epoch 013 - training loss: 0.6034\n",
      "2024-10-21 03:11:26 [INFO]: Epoch 014 - training loss: 0.6146\n",
      "2024-10-21 03:11:29 [INFO]: Epoch 015 - training loss: 0.6062\n",
      "2024-10-21 03:11:32 [INFO]: Epoch 016 - training loss: 0.6081\n",
      "2024-10-21 03:11:34 [INFO]: Epoch 017 - training loss: 0.6086\n",
      "2024-10-21 03:11:37 [INFO]: Epoch 018 - training loss: 0.6112\n",
      "2024-10-21 03:11:40 [INFO]: Epoch 019 - training loss: 0.6076\n",
      "2024-10-21 03:11:43 [INFO]: Epoch 020 - training loss: 0.6132\n",
      "2024-10-21 03:11:46 [INFO]: Epoch 021 - training loss: 0.6040\n",
      "2024-10-21 03:11:49 [INFO]: Epoch 022 - training loss: 0.6029\n",
      "2024-10-21 03:11:51 [INFO]: Epoch 023 - training loss: 0.6013\n",
      "2024-10-21 03:11:54 [INFO]: Epoch 024 - training loss: 0.6204\n",
      "2024-10-21 03:11:57 [INFO]: Epoch 025 - training loss: 0.6001\n",
      "2024-10-21 03:12:00 [INFO]: Epoch 026 - training loss: 0.5959\n",
      "2024-10-21 03:12:03 [INFO]: Epoch 027 - training loss: 0.5968\n",
      "2024-10-21 03:12:05 [INFO]: Epoch 028 - training loss: 0.6051\n",
      "2024-10-21 03:12:08 [INFO]: Epoch 029 - training loss: 0.6115\n",
      "2024-10-21 03:12:11 [INFO]: Epoch 030 - training loss: 0.6047\n",
      "2024-10-21 03:12:14 [INFO]: Epoch 031 - training loss: 0.6012\n",
      "2024-10-21 03:12:17 [INFO]: Epoch 032 - training loss: 0.6071\n",
      "2024-10-21 03:12:20 [INFO]: Epoch 033 - training loss: 0.6012\n",
      "2024-10-21 03:12:22 [INFO]: Epoch 034 - training loss: 0.6087\n",
      "2024-10-21 03:12:25 [INFO]: Epoch 035 - training loss: 0.6079\n",
      "2024-10-21 03:12:28 [INFO]: Epoch 036 - training loss: 0.6080\n",
      "2024-10-21 03:12:31 [INFO]: Epoch 037 - training loss: 0.6083\n",
      "2024-10-21 03:12:34 [INFO]: Epoch 038 - training loss: 0.6117\n",
      "2024-10-21 03:12:37 [INFO]: Epoch 039 - training loss: 0.6063\n",
      "2024-10-21 03:12:39 [INFO]: Epoch 040 - training loss: 0.6054\n",
      "2024-10-21 03:12:42 [INFO]: Epoch 041 - training loss: 0.6048\n",
      "2024-10-21 03:12:45 [INFO]: Epoch 042 - training loss: 0.6017\n",
      "2024-10-21 03:12:48 [INFO]: Epoch 043 - training loss: 0.6097\n",
      "2024-10-21 03:12:51 [INFO]: Epoch 044 - training loss: 0.6009\n",
      "2024-10-21 03:12:54 [INFO]: Epoch 045 - training loss: 0.6025\n",
      "2024-10-21 03:12:56 [INFO]: Epoch 046 - training loss: 0.6018\n",
      "2024-10-21 03:12:59 [INFO]: Epoch 047 - training loss: 0.5958\n",
      "2024-10-21 03:13:02 [INFO]: Epoch 048 - training loss: 0.6010\n",
      "2024-10-21 03:13:05 [INFO]: Epoch 049 - training loss: 0.6078\n",
      "2024-10-21 03:13:08 [INFO]: Epoch 050 - training loss: 0.5988\n",
      "2024-10-21 03:13:10 [INFO]: Epoch 051 - training loss: 0.6068\n",
      "2024-10-21 03:13:13 [INFO]: Epoch 052 - training loss: 0.6028\n",
      "2024-10-21 03:13:16 [INFO]: Epoch 053 - training loss: 0.6051\n",
      "2024-10-21 03:13:19 [INFO]: Epoch 054 - training loss: 0.6153\n",
      "2024-10-21 03:13:22 [INFO]: Epoch 055 - training loss: 0.6169\n",
      "2024-10-21 03:13:24 [INFO]: Epoch 056 - training loss: 0.6148\n",
      "2024-10-21 03:13:27 [INFO]: Epoch 057 - training loss: 0.6019\n",
      "2024-10-21 03:13:30 [INFO]: Epoch 058 - training loss: 0.6000\n",
      "2024-10-21 03:13:33 [INFO]: Epoch 059 - training loss: 0.6060\n",
      "2024-10-21 03:13:36 [INFO]: Epoch 060 - training loss: 0.5992\n",
      "2024-10-21 03:13:38 [INFO]: Epoch 061 - training loss: 0.6056\n",
      "2024-10-21 03:13:41 [INFO]: Epoch 062 - training loss: 0.6053\n",
      "2024-10-21 03:13:44 [INFO]: Epoch 063 - training loss: 0.5982\n",
      "2024-10-21 03:13:47 [INFO]: Epoch 064 - training loss: 0.6043\n",
      "2024-10-21 03:13:50 [INFO]: Epoch 065 - training loss: 0.6109\n",
      "2024-10-21 03:13:52 [INFO]: Epoch 066 - training loss: 0.5985\n",
      "2024-10-21 03:13:55 [INFO]: Epoch 067 - training loss: 0.6093\n",
      "2024-10-21 03:13:58 [INFO]: Epoch 068 - training loss: 0.6137\n",
      "2024-10-21 03:14:01 [INFO]: Epoch 069 - training loss: 0.6004\n",
      "2024-10-21 03:14:04 [INFO]: Epoch 070 - training loss: 0.6037\n",
      "2024-10-21 03:14:06 [INFO]: Epoch 071 - training loss: 0.6092\n",
      "2024-10-21 03:14:09 [INFO]: Epoch 072 - training loss: 0.5992\n",
      "2024-10-21 03:14:12 [INFO]: Epoch 073 - training loss: 0.6005\n",
      "2024-10-21 03:14:15 [INFO]: Epoch 074 - training loss: 0.6067\n",
      "2024-10-21 03:14:18 [INFO]: Epoch 075 - training loss: 0.6000\n",
      "2024-10-21 03:14:20 [INFO]: Epoch 076 - training loss: 0.6028\n",
      "2024-10-21 03:14:23 [INFO]: Epoch 077 - training loss: 0.5978\n",
      "2024-10-21 03:14:26 [INFO]: Epoch 078 - training loss: 0.5965\n",
      "2024-10-21 03:14:29 [INFO]: Epoch 079 - training loss: 0.6057\n",
      "2024-10-21 03:14:32 [INFO]: Epoch 080 - training loss: 0.6047\n",
      "2024-10-21 03:14:35 [INFO]: Epoch 081 - training loss: 0.5966\n",
      "2024-10-21 03:14:37 [INFO]: Epoch 082 - training loss: 0.6033\n",
      "2024-10-21 03:14:40 [INFO]: Epoch 083 - training loss: 0.6005\n",
      "2024-10-21 03:14:43 [INFO]: Epoch 084 - training loss: 0.5951\n",
      "2024-10-21 03:14:46 [INFO]: Epoch 085 - training loss: 0.6005\n",
      "2024-10-21 03:14:49 [INFO]: Epoch 086 - training loss: 0.6023\n",
      "2024-10-21 03:14:52 [INFO]: Epoch 087 - training loss: 0.6039\n",
      "2024-10-21 03:14:54 [INFO]: Epoch 088 - training loss: 0.6032\n",
      "2024-10-21 03:14:57 [INFO]: Epoch 089 - training loss: 0.6106\n",
      "2024-10-21 03:15:00 [INFO]: Epoch 090 - training loss: 0.6129\n",
      "2024-10-21 03:15:03 [INFO]: Epoch 091 - training loss: 0.6062\n",
      "2024-10-21 03:15:06 [INFO]: Epoch 092 - training loss: 0.6089\n",
      "2024-10-21 03:15:09 [INFO]: Epoch 093 - training loss: 0.6032\n",
      "2024-10-21 03:15:11 [INFO]: Epoch 094 - training loss: 0.6076\n",
      "2024-10-21 03:15:14 [INFO]: Epoch 095 - training loss: 0.5975\n",
      "2024-10-21 03:15:17 [INFO]: Epoch 096 - training loss: 0.6020\n",
      "2024-10-21 03:15:20 [INFO]: Epoch 097 - training loss: 0.6035\n",
      "2024-10-21 03:15:23 [INFO]: Epoch 098 - training loss: 0.6025\n",
      "2024-10-21 03:15:26 [INFO]: Epoch 099 - training loss: 0.6026\n",
      "2024-10-21 03:15:29 [INFO]: Epoch 100 - training loss: 0.5986\n",
      "2024-10-21 03:15:29 [INFO]: Finished training. The best model is from epoch#84.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 26/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 03:20:46 [INFO]: Epoch 001 - training loss: 0.6099\n",
      "2024-10-21 03:20:49 [INFO]: Epoch 002 - training loss: 0.6155\n",
      "2024-10-21 03:20:52 [INFO]: Epoch 003 - training loss: 0.6179\n",
      "2024-10-21 03:20:55 [INFO]: Epoch 004 - training loss: 0.6142\n",
      "2024-10-21 03:20:58 [INFO]: Epoch 005 - training loss: 0.6103\n",
      "2024-10-21 03:21:00 [INFO]: Epoch 006 - training loss: 0.6234\n",
      "2024-10-21 03:21:03 [INFO]: Epoch 007 - training loss: 0.6028\n",
      "2024-10-21 03:21:06 [INFO]: Epoch 008 - training loss: 0.6105\n",
      "2024-10-21 03:21:09 [INFO]: Epoch 009 - training loss: 0.6061\n",
      "2024-10-21 03:21:12 [INFO]: Epoch 010 - training loss: 0.6064\n",
      "2024-10-21 03:21:14 [INFO]: Epoch 011 - training loss: 0.6186\n",
      "2024-10-21 03:21:17 [INFO]: Epoch 012 - training loss: 0.6084\n",
      "2024-10-21 03:21:20 [INFO]: Epoch 013 - training loss: 0.6109\n",
      "2024-10-21 03:21:23 [INFO]: Epoch 014 - training loss: 0.6029\n",
      "2024-10-21 03:21:26 [INFO]: Epoch 015 - training loss: 0.6111\n",
      "2024-10-21 03:21:28 [INFO]: Epoch 016 - training loss: 0.6098\n",
      "2024-10-21 03:21:31 [INFO]: Epoch 017 - training loss: 0.6090\n",
      "2024-10-21 03:21:34 [INFO]: Epoch 018 - training loss: 0.6035\n",
      "2024-10-21 03:21:37 [INFO]: Epoch 019 - training loss: 0.5987\n",
      "2024-10-21 03:21:40 [INFO]: Epoch 020 - training loss: 0.6005\n",
      "2024-10-21 03:21:43 [INFO]: Epoch 021 - training loss: 0.5992\n",
      "2024-10-21 03:21:46 [INFO]: Epoch 022 - training loss: 0.6011\n",
      "2024-10-21 03:21:48 [INFO]: Epoch 023 - training loss: 0.5967\n",
      "2024-10-21 03:21:51 [INFO]: Epoch 024 - training loss: 0.5984\n",
      "2024-10-21 03:21:54 [INFO]: Epoch 025 - training loss: 0.5987\n",
      "2024-10-21 03:21:57 [INFO]: Epoch 026 - training loss: 0.5965\n",
      "2024-10-21 03:22:00 [INFO]: Epoch 027 - training loss: 0.6048\n",
      "2024-10-21 03:22:03 [INFO]: Epoch 028 - training loss: 0.6031\n",
      "2024-10-21 03:22:05 [INFO]: Epoch 029 - training loss: 0.5967\n",
      "2024-10-21 03:22:08 [INFO]: Epoch 030 - training loss: 0.5866\n",
      "2024-10-21 03:22:11 [INFO]: Epoch 031 - training loss: 0.6085\n",
      "2024-10-21 03:22:14 [INFO]: Epoch 032 - training loss: 0.6014\n",
      "2024-10-21 03:22:17 [INFO]: Epoch 033 - training loss: 0.6033\n",
      "2024-10-21 03:22:19 [INFO]: Epoch 034 - training loss: 0.6028\n",
      "2024-10-21 03:22:22 [INFO]: Epoch 035 - training loss: 0.6011\n",
      "2024-10-21 03:22:25 [INFO]: Epoch 036 - training loss: 0.6069\n",
      "2024-10-21 03:22:28 [INFO]: Epoch 037 - training loss: 0.5938\n",
      "2024-10-21 03:22:31 [INFO]: Epoch 038 - training loss: 0.5931\n",
      "2024-10-21 03:22:34 [INFO]: Epoch 039 - training loss: 0.5942\n",
      "2024-10-21 03:22:37 [INFO]: Epoch 040 - training loss: 0.6021\n",
      "2024-10-21 03:22:39 [INFO]: Epoch 041 - training loss: 0.6004\n",
      "2024-10-21 03:22:42 [INFO]: Epoch 042 - training loss: 0.5972\n",
      "2024-10-21 03:22:45 [INFO]: Epoch 043 - training loss: 0.6082\n",
      "2024-10-21 03:22:48 [INFO]: Epoch 044 - training loss: 0.6005\n",
      "2024-10-21 03:22:51 [INFO]: Epoch 045 - training loss: 0.5967\n",
      "2024-10-21 03:22:54 [INFO]: Epoch 046 - training loss: 0.5942\n",
      "2024-10-21 03:22:56 [INFO]: Epoch 047 - training loss: 0.6006\n",
      "2024-10-21 03:22:59 [INFO]: Epoch 048 - training loss: 0.5937\n",
      "2024-10-21 03:23:02 [INFO]: Epoch 049 - training loss: 0.6010\n",
      "2024-10-21 03:23:05 [INFO]: Epoch 050 - training loss: 0.5979\n",
      "2024-10-21 03:23:08 [INFO]: Epoch 051 - training loss: 0.6090\n",
      "2024-10-21 03:23:11 [INFO]: Epoch 052 - training loss: 0.5931\n",
      "2024-10-21 03:23:13 [INFO]: Epoch 053 - training loss: 0.5992\n",
      "2024-10-21 03:23:16 [INFO]: Epoch 054 - training loss: 0.6013\n",
      "2024-10-21 03:23:19 [INFO]: Epoch 055 - training loss: 0.5975\n",
      "2024-10-21 03:23:22 [INFO]: Epoch 056 - training loss: 0.5992\n",
      "2024-10-21 03:23:25 [INFO]: Epoch 057 - training loss: 0.5985\n",
      "2024-10-21 03:23:28 [INFO]: Epoch 058 - training loss: 0.5951\n",
      "2024-10-21 03:23:30 [INFO]: Epoch 059 - training loss: 0.5923\n",
      "2024-10-21 03:23:33 [INFO]: Epoch 060 - training loss: 0.5977\n",
      "2024-10-21 03:23:36 [INFO]: Epoch 061 - training loss: 0.6013\n",
      "2024-10-21 03:23:39 [INFO]: Epoch 062 - training loss: 0.5918\n",
      "2024-10-21 03:23:42 [INFO]: Epoch 063 - training loss: 0.6005\n",
      "2024-10-21 03:23:44 [INFO]: Epoch 064 - training loss: 0.6033\n",
      "2024-10-21 03:23:47 [INFO]: Epoch 065 - training loss: 0.5994\n",
      "2024-10-21 03:23:50 [INFO]: Epoch 066 - training loss: 0.6025\n",
      "2024-10-21 03:23:53 [INFO]: Epoch 067 - training loss: 0.5942\n",
      "2024-10-21 03:23:56 [INFO]: Epoch 068 - training loss: 0.5951\n",
      "2024-10-21 03:23:59 [INFO]: Epoch 069 - training loss: 0.5978\n",
      "2024-10-21 03:24:01 [INFO]: Epoch 070 - training loss: 0.5914\n",
      "2024-10-21 03:24:04 [INFO]: Epoch 071 - training loss: 0.5973\n",
      "2024-10-21 03:24:07 [INFO]: Epoch 072 - training loss: 0.5973\n",
      "2024-10-21 03:24:10 [INFO]: Epoch 073 - training loss: 0.5972\n",
      "2024-10-21 03:24:13 [INFO]: Epoch 074 - training loss: 0.6021\n",
      "2024-10-21 03:24:15 [INFO]: Epoch 075 - training loss: 0.6053\n",
      "2024-10-21 03:24:18 [INFO]: Epoch 076 - training loss: 0.6028\n",
      "2024-10-21 03:24:21 [INFO]: Epoch 077 - training loss: 0.6010\n",
      "2024-10-21 03:24:24 [INFO]: Epoch 078 - training loss: 0.5977\n",
      "2024-10-21 03:24:27 [INFO]: Epoch 079 - training loss: 0.5875\n",
      "2024-10-21 03:24:30 [INFO]: Epoch 080 - training loss: 0.5999\n",
      "2024-10-21 03:24:32 [INFO]: Epoch 081 - training loss: 0.5913\n",
      "2024-10-21 03:24:35 [INFO]: Epoch 082 - training loss: 0.5940\n",
      "2024-10-21 03:24:38 [INFO]: Epoch 083 - training loss: 0.6024\n",
      "2024-10-21 03:24:41 [INFO]: Epoch 084 - training loss: 0.6012\n",
      "2024-10-21 03:24:44 [INFO]: Epoch 085 - training loss: 0.5958\n",
      "2024-10-21 03:24:47 [INFO]: Epoch 086 - training loss: 0.5962\n",
      "2024-10-21 03:24:49 [INFO]: Epoch 087 - training loss: 0.6023\n",
      "2024-10-21 03:24:52 [INFO]: Epoch 088 - training loss: 0.6001\n",
      "2024-10-21 03:24:55 [INFO]: Epoch 089 - training loss: 0.5950\n",
      "2024-10-21 03:24:58 [INFO]: Epoch 090 - training loss: 0.6018\n",
      "2024-10-21 03:25:01 [INFO]: Epoch 091 - training loss: 0.5977\n",
      "2024-10-21 03:25:03 [INFO]: Epoch 092 - training loss: 0.5880\n",
      "2024-10-21 03:25:06 [INFO]: Epoch 093 - training loss: 0.6020\n",
      "2024-10-21 03:25:09 [INFO]: Epoch 094 - training loss: 0.5960\n",
      "2024-10-21 03:25:12 [INFO]: Epoch 095 - training loss: 0.5929\n",
      "2024-10-21 03:25:15 [INFO]: Epoch 096 - training loss: 0.5949\n",
      "2024-10-21 03:25:17 [INFO]: Epoch 097 - training loss: 0.5975\n",
      "2024-10-21 03:25:20 [INFO]: Epoch 098 - training loss: 0.5962\n",
      "2024-10-21 03:25:23 [INFO]: Epoch 099 - training loss: 0.5952\n",
      "2024-10-21 03:25:26 [INFO]: Epoch 100 - training loss: 0.5984\n",
      "2024-10-21 03:25:26 [INFO]: Finished training. The best model is from epoch#30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 27/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 03:30:44 [INFO]: Epoch 001 - training loss: 0.6048\n",
      "2024-10-21 03:30:47 [INFO]: Epoch 002 - training loss: 0.6053\n",
      "2024-10-21 03:30:50 [INFO]: Epoch 003 - training loss: 0.6102\n",
      "2024-10-21 03:30:52 [INFO]: Epoch 004 - training loss: 0.6080\n",
      "2024-10-21 03:30:55 [INFO]: Epoch 005 - training loss: 0.6020\n",
      "2024-10-21 03:30:58 [INFO]: Epoch 006 - training loss: 0.6012\n",
      "2024-10-21 03:31:01 [INFO]: Epoch 007 - training loss: 0.6109\n",
      "2024-10-21 03:31:04 [INFO]: Epoch 008 - training loss: 0.6017\n",
      "2024-10-21 03:31:07 [INFO]: Epoch 009 - training loss: 0.6055\n",
      "2024-10-21 03:31:09 [INFO]: Epoch 010 - training loss: 0.5965\n",
      "2024-10-21 03:31:12 [INFO]: Epoch 011 - training loss: 0.6014\n",
      "2024-10-21 03:31:15 [INFO]: Epoch 012 - training loss: 0.6064\n",
      "2024-10-21 03:31:18 [INFO]: Epoch 013 - training loss: 0.5970\n",
      "2024-10-21 03:31:21 [INFO]: Epoch 014 - training loss: 0.6003\n",
      "2024-10-21 03:31:24 [INFO]: Epoch 015 - training loss: 0.6059\n",
      "2024-10-21 03:31:26 [INFO]: Epoch 016 - training loss: 0.6118\n",
      "2024-10-21 03:31:29 [INFO]: Epoch 017 - training loss: 0.6097\n",
      "2024-10-21 03:31:32 [INFO]: Epoch 018 - training loss: 0.5942\n",
      "2024-10-21 03:31:35 [INFO]: Epoch 019 - training loss: 0.6018\n",
      "2024-10-21 03:31:38 [INFO]: Epoch 020 - training loss: 0.6035\n",
      "2024-10-21 03:31:40 [INFO]: Epoch 021 - training loss: 0.6044\n",
      "2024-10-21 03:31:43 [INFO]: Epoch 022 - training loss: 0.6012\n",
      "2024-10-21 03:31:46 [INFO]: Epoch 023 - training loss: 0.6002\n",
      "2024-10-21 03:31:49 [INFO]: Epoch 024 - training loss: 0.6052\n",
      "2024-10-21 03:31:52 [INFO]: Epoch 025 - training loss: 0.6084\n",
      "2024-10-21 03:31:55 [INFO]: Epoch 026 - training loss: 0.6015\n",
      "2024-10-21 03:31:57 [INFO]: Epoch 027 - training loss: 0.6033\n",
      "2024-10-21 03:32:00 [INFO]: Epoch 028 - training loss: 0.6009\n",
      "2024-10-21 03:32:03 [INFO]: Epoch 029 - training loss: 0.6017\n",
      "2024-10-21 03:32:06 [INFO]: Epoch 030 - training loss: 0.5971\n",
      "2024-10-21 03:32:09 [INFO]: Epoch 031 - training loss: 0.6025\n",
      "2024-10-21 03:32:12 [INFO]: Epoch 032 - training loss: 0.5957\n",
      "2024-10-21 03:32:14 [INFO]: Epoch 033 - training loss: 0.5985\n",
      "2024-10-21 03:32:17 [INFO]: Epoch 034 - training loss: 0.5969\n",
      "2024-10-21 03:32:20 [INFO]: Epoch 035 - training loss: 0.6069\n",
      "2024-10-21 03:32:23 [INFO]: Epoch 036 - training loss: 0.6061\n",
      "2024-10-21 03:32:26 [INFO]: Epoch 037 - training loss: 0.5950\n",
      "2024-10-21 03:32:29 [INFO]: Epoch 038 - training loss: 0.6010\n",
      "2024-10-21 03:32:31 [INFO]: Epoch 039 - training loss: 0.6034\n",
      "2024-10-21 03:32:34 [INFO]: Epoch 040 - training loss: 0.5999\n",
      "2024-10-21 03:32:37 [INFO]: Epoch 041 - training loss: 0.5987\n",
      "2024-10-21 03:32:40 [INFO]: Epoch 042 - training loss: 0.6064\n",
      "2024-10-21 03:32:43 [INFO]: Epoch 043 - training loss: 0.5996\n",
      "2024-10-21 03:32:45 [INFO]: Epoch 044 - training loss: 0.6056\n",
      "2024-10-21 03:32:48 [INFO]: Epoch 045 - training loss: 0.6122\n",
      "2024-10-21 03:32:51 [INFO]: Epoch 046 - training loss: 0.5970\n",
      "2024-10-21 03:32:54 [INFO]: Epoch 047 - training loss: 0.6030\n",
      "2024-10-21 03:32:57 [INFO]: Epoch 048 - training loss: 0.6037\n",
      "2024-10-21 03:33:00 [INFO]: Epoch 049 - training loss: 0.6035\n",
      "2024-10-21 03:33:02 [INFO]: Epoch 050 - training loss: 0.6020\n",
      "2024-10-21 03:33:05 [INFO]: Epoch 051 - training loss: 0.6010\n",
      "2024-10-21 03:33:08 [INFO]: Epoch 052 - training loss: 0.6026\n",
      "2024-10-21 03:33:11 [INFO]: Epoch 053 - training loss: 0.5994\n",
      "2024-10-21 03:33:14 [INFO]: Epoch 054 - training loss: 0.6043\n",
      "2024-10-21 03:33:16 [INFO]: Epoch 055 - training loss: 0.6103\n",
      "2024-10-21 03:33:19 [INFO]: Epoch 056 - training loss: 0.5995\n",
      "2024-10-21 03:33:22 [INFO]: Epoch 057 - training loss: 0.6038\n",
      "2024-10-21 03:33:25 [INFO]: Epoch 058 - training loss: 0.6041\n",
      "2024-10-21 03:33:28 [INFO]: Epoch 059 - training loss: 0.6012\n",
      "2024-10-21 03:33:30 [INFO]: Epoch 060 - training loss: 0.6018\n",
      "2024-10-21 03:33:33 [INFO]: Epoch 061 - training loss: 0.6021\n",
      "2024-10-21 03:33:36 [INFO]: Epoch 062 - training loss: 0.6030\n",
      "2024-10-21 03:33:39 [INFO]: Epoch 063 - training loss: 0.5989\n",
      "2024-10-21 03:33:42 [INFO]: Epoch 064 - training loss: 0.6041\n",
      "2024-10-21 03:33:45 [INFO]: Epoch 065 - training loss: 0.5951\n",
      "2024-10-21 03:33:47 [INFO]: Epoch 066 - training loss: 0.5962\n",
      "2024-10-21 03:33:50 [INFO]: Epoch 067 - training loss: 0.5968\n",
      "2024-10-21 03:33:53 [INFO]: Epoch 068 - training loss: 0.5957\n",
      "2024-10-21 03:33:56 [INFO]: Epoch 069 - training loss: 0.6022\n",
      "2024-10-21 03:33:59 [INFO]: Epoch 070 - training loss: 0.5929\n",
      "2024-10-21 03:34:02 [INFO]: Epoch 071 - training loss: 0.6011\n",
      "2024-10-21 03:34:04 [INFO]: Epoch 072 - training loss: 0.6079\n",
      "2024-10-21 03:34:07 [INFO]: Epoch 073 - training loss: 0.6045\n",
      "2024-10-21 03:34:10 [INFO]: Epoch 074 - training loss: 0.6041\n",
      "2024-10-21 03:34:13 [INFO]: Epoch 075 - training loss: 0.5949\n",
      "2024-10-21 03:34:15 [INFO]: Epoch 076 - training loss: 0.6009\n",
      "2024-10-21 03:34:18 [INFO]: Epoch 077 - training loss: 0.5897\n",
      "2024-10-21 03:34:21 [INFO]: Epoch 078 - training loss: 0.5964\n",
      "2024-10-21 03:34:24 [INFO]: Epoch 079 - training loss: 0.5899\n",
      "2024-10-21 03:34:27 [INFO]: Epoch 080 - training loss: 0.5943\n",
      "2024-10-21 03:34:29 [INFO]: Epoch 081 - training loss: 0.5925\n",
      "2024-10-21 03:34:32 [INFO]: Epoch 082 - training loss: 0.6024\n",
      "2024-10-21 03:34:35 [INFO]: Epoch 083 - training loss: 0.5952\n",
      "2024-10-21 03:34:38 [INFO]: Epoch 084 - training loss: 0.5973\n",
      "2024-10-21 03:34:41 [INFO]: Epoch 085 - training loss: 0.5971\n",
      "2024-10-21 03:34:44 [INFO]: Epoch 086 - training loss: 0.5944\n",
      "2024-10-21 03:34:46 [INFO]: Epoch 087 - training loss: 0.5948\n",
      "2024-10-21 03:34:49 [INFO]: Epoch 088 - training loss: 0.5918\n",
      "2024-10-21 03:34:52 [INFO]: Epoch 089 - training loss: 0.5982\n",
      "2024-10-21 03:34:55 [INFO]: Epoch 090 - training loss: 0.5956\n",
      "2024-10-21 03:34:58 [INFO]: Epoch 091 - training loss: 0.5981\n",
      "2024-10-21 03:35:01 [INFO]: Epoch 092 - training loss: 0.5986\n",
      "2024-10-21 03:35:04 [INFO]: Epoch 093 - training loss: 0.5916\n",
      "2024-10-21 03:35:06 [INFO]: Epoch 094 - training loss: 0.5943\n",
      "2024-10-21 03:35:09 [INFO]: Epoch 095 - training loss: 0.5880\n",
      "2024-10-21 03:35:12 [INFO]: Epoch 096 - training loss: 0.5953\n",
      "2024-10-21 03:35:15 [INFO]: Epoch 097 - training loss: 0.5996\n",
      "2024-10-21 03:35:18 [INFO]: Epoch 098 - training loss: 0.5932\n",
      "2024-10-21 03:35:20 [INFO]: Epoch 099 - training loss: 0.5956\n",
      "2024-10-21 03:35:23 [INFO]: Epoch 100 - training loss: 0.5952\n",
      "2024-10-21 03:35:23 [INFO]: Finished training. The best model is from epoch#95.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 28/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Training BRITS on fold 28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 03:40:43 [INFO]: Epoch 001 - training loss: 0.6168\n",
      "2024-10-21 03:40:46 [INFO]: Epoch 002 - training loss: 0.6215\n",
      "2024-10-21 03:40:49 [INFO]: Epoch 003 - training loss: 0.6156\n",
      "2024-10-21 03:40:51 [INFO]: Epoch 004 - training loss: 0.6140\n",
      "2024-10-21 03:40:54 [INFO]: Epoch 005 - training loss: 0.6107\n",
      "2024-10-21 03:40:57 [INFO]: Epoch 006 - training loss: 0.6121\n",
      "2024-10-21 03:41:00 [INFO]: Epoch 007 - training loss: 0.6071\n",
      "2024-10-21 03:41:03 [INFO]: Epoch 008 - training loss: 0.6105\n",
      "2024-10-21 03:41:06 [INFO]: Epoch 009 - training loss: 0.5975\n",
      "2024-10-21 03:41:08 [INFO]: Epoch 010 - training loss: 0.6022\n",
      "2024-10-21 03:41:11 [INFO]: Epoch 011 - training loss: 0.5993\n",
      "2024-10-21 03:41:14 [INFO]: Epoch 012 - training loss: 0.6070\n",
      "2024-10-21 03:41:17 [INFO]: Epoch 013 - training loss: 0.6068\n",
      "2024-10-21 03:41:20 [INFO]: Epoch 014 - training loss: 0.6056\n",
      "2024-10-21 03:41:23 [INFO]: Epoch 015 - training loss: 0.6014\n",
      "2024-10-21 03:41:25 [INFO]: Epoch 016 - training loss: 0.6037\n",
      "2024-10-21 03:41:28 [INFO]: Epoch 017 - training loss: 0.6071\n",
      "2024-10-21 03:41:31 [INFO]: Epoch 018 - training loss: 0.6089\n",
      "2024-10-21 03:41:34 [INFO]: Epoch 019 - training loss: 0.6069\n",
      "2024-10-21 03:41:37 [INFO]: Epoch 020 - training loss: 0.6061\n",
      "2024-10-21 03:41:39 [INFO]: Epoch 021 - training loss: 0.6065\n",
      "2024-10-21 03:41:42 [INFO]: Epoch 022 - training loss: 0.6063\n",
      "2024-10-21 03:41:45 [INFO]: Epoch 023 - training loss: 0.6048\n",
      "2024-10-21 03:41:48 [INFO]: Epoch 024 - training loss: 0.6021\n",
      "2024-10-21 03:41:51 [INFO]: Epoch 025 - training loss: 0.5989\n",
      "2024-10-21 03:41:54 [INFO]: Epoch 026 - training loss: 0.6026\n",
      "2024-10-21 03:41:57 [INFO]: Epoch 027 - training loss: 0.6005\n",
      "2024-10-21 03:41:59 [INFO]: Epoch 028 - training loss: 0.5985\n",
      "2024-10-21 03:42:02 [INFO]: Epoch 029 - training loss: 0.6028\n",
      "2024-10-21 03:42:05 [INFO]: Epoch 030 - training loss: 0.5977\n",
      "2024-10-21 03:42:08 [INFO]: Epoch 031 - training loss: 0.6057\n",
      "2024-10-21 03:42:11 [INFO]: Epoch 032 - training loss: 0.6013\n",
      "2024-10-21 03:42:13 [INFO]: Epoch 033 - training loss: 0.5981\n",
      "2024-10-21 03:42:16 [INFO]: Epoch 034 - training loss: 0.6048\n",
      "2024-10-21 03:42:19 [INFO]: Epoch 035 - training loss: 0.6042\n",
      "2024-10-21 03:42:22 [INFO]: Epoch 036 - training loss: 0.5999\n",
      "2024-10-21 03:42:25 [INFO]: Epoch 037 - training loss: 0.6167\n",
      "2024-10-21 03:42:28 [INFO]: Epoch 038 - training loss: 0.6057\n",
      "2024-10-21 03:42:30 [INFO]: Epoch 039 - training loss: 0.5968\n",
      "2024-10-21 03:42:33 [INFO]: Epoch 040 - training loss: 0.6017\n",
      "2024-10-21 03:42:36 [INFO]: Epoch 041 - training loss: 0.6014\n",
      "2024-10-21 03:42:39 [INFO]: Epoch 042 - training loss: 0.5992\n",
      "2024-10-21 03:42:42 [INFO]: Epoch 043 - training loss: 0.6028\n",
      "2024-10-21 03:42:44 [INFO]: Epoch 044 - training loss: 0.6042\n",
      "2024-10-21 03:42:47 [INFO]: Epoch 045 - training loss: 0.5997\n",
      "2024-10-21 03:42:50 [INFO]: Epoch 046 - training loss: 0.6000\n",
      "2024-10-21 03:42:53 [INFO]: Epoch 047 - training loss: 0.5972\n",
      "2024-10-21 03:42:56 [INFO]: Epoch 048 - training loss: 0.5909\n",
      "2024-10-21 03:42:58 [INFO]: Epoch 049 - training loss: 0.5961\n",
      "2024-10-21 03:43:01 [INFO]: Epoch 050 - training loss: 0.5916\n",
      "2024-10-21 03:43:04 [INFO]: Epoch 051 - training loss: 0.6001\n",
      "2024-10-21 03:43:07 [INFO]: Epoch 052 - training loss: 0.5877\n",
      "2024-10-21 03:43:10 [INFO]: Epoch 053 - training loss: 0.6037\n",
      "2024-10-21 03:43:13 [INFO]: Epoch 054 - training loss: 0.6049\n",
      "2024-10-21 03:43:15 [INFO]: Epoch 055 - training loss: 0.5927\n",
      "2024-10-21 03:43:18 [INFO]: Epoch 056 - training loss: 0.6019\n",
      "2024-10-21 03:43:21 [INFO]: Epoch 057 - training loss: 0.6077\n",
      "2024-10-21 03:43:24 [INFO]: Epoch 058 - training loss: 0.5926\n",
      "2024-10-21 03:43:27 [INFO]: Epoch 059 - training loss: 0.6139\n",
      "2024-10-21 03:43:30 [INFO]: Epoch 060 - training loss: 0.6053\n",
      "2024-10-21 03:43:33 [INFO]: Epoch 061 - training loss: 0.6076\n",
      "2024-10-21 03:43:35 [INFO]: Epoch 062 - training loss: 0.6037\n",
      "2024-10-21 03:43:38 [INFO]: Epoch 063 - training loss: 0.5955\n",
      "2024-10-21 03:43:41 [INFO]: Epoch 064 - training loss: 0.5998\n",
      "2024-10-21 03:43:44 [INFO]: Epoch 065 - training loss: 0.5966\n",
      "2024-10-21 03:43:47 [INFO]: Epoch 066 - training loss: 0.6049\n",
      "2024-10-21 03:43:50 [INFO]: Epoch 067 - training loss: 0.6008\n",
      "2024-10-21 03:43:52 [INFO]: Epoch 068 - training loss: 0.6118\n",
      "2024-10-21 03:43:55 [INFO]: Epoch 069 - training loss: 0.6061\n",
      "2024-10-21 03:43:58 [INFO]: Epoch 070 - training loss: 0.6042\n",
      "2024-10-21 03:44:01 [INFO]: Epoch 071 - training loss: 0.6008\n",
      "2024-10-21 03:44:04 [INFO]: Epoch 072 - training loss: 0.6096\n",
      "2024-10-21 03:44:06 [INFO]: Epoch 073 - training loss: 0.5959\n",
      "2024-10-21 03:44:09 [INFO]: Epoch 074 - training loss: 0.6011\n",
      "2024-10-21 03:44:12 [INFO]: Epoch 075 - training loss: 0.5982\n",
      "2024-10-21 03:44:15 [INFO]: Epoch 076 - training loss: 0.6035\n",
      "2024-10-21 03:44:18 [INFO]: Epoch 077 - training loss: 0.6013\n",
      "2024-10-21 03:44:20 [INFO]: Epoch 078 - training loss: 0.6012\n",
      "2024-10-21 03:44:23 [INFO]: Epoch 079 - training loss: 0.6022\n",
      "2024-10-21 03:44:26 [INFO]: Epoch 080 - training loss: 0.6046\n",
      "2024-10-21 03:44:29 [INFO]: Epoch 081 - training loss: 0.5991\n",
      "2024-10-21 03:44:32 [INFO]: Epoch 082 - training loss: 0.5995\n",
      "2024-10-21 03:44:35 [INFO]: Epoch 083 - training loss: 0.5957\n",
      "2024-10-21 03:44:38 [INFO]: Epoch 084 - training loss: 0.5928\n",
      "2024-10-21 03:44:41 [INFO]: Epoch 085 - training loss: 0.5992\n",
      "2024-10-21 03:44:43 [INFO]: Epoch 086 - training loss: 0.5975\n",
      "2024-10-21 03:44:46 [INFO]: Epoch 087 - training loss: 0.6061\n",
      "2024-10-21 03:44:49 [INFO]: Epoch 088 - training loss: 0.5972\n",
      "2024-10-21 03:44:52 [INFO]: Epoch 089 - training loss: 0.5942\n",
      "2024-10-21 03:44:55 [INFO]: Epoch 090 - training loss: 0.5981\n",
      "2024-10-21 03:44:58 [INFO]: Epoch 091 - training loss: 0.5987\n",
      "2024-10-21 03:45:00 [INFO]: Epoch 092 - training loss: 0.5910\n",
      "2024-10-21 03:45:03 [INFO]: Epoch 093 - training loss: 0.6013\n",
      "2024-10-21 03:45:06 [INFO]: Epoch 094 - training loss: 0.6013\n",
      "2024-10-21 03:45:09 [INFO]: Epoch 095 - training loss: 0.5917\n",
      "2024-10-21 03:45:12 [INFO]: Epoch 096 - training loss: 0.5992\n",
      "2024-10-21 03:45:15 [INFO]: Epoch 097 - training loss: 0.5981\n",
      "2024-10-21 03:45:17 [INFO]: Epoch 098 - training loss: 0.5927\n",
      "2024-10-21 03:45:20 [INFO]: Epoch 099 - training loss: 0.5965\n",
      "2024-10-21 03:45:23 [INFO]: Epoch 100 - training loss: 0.6049\n",
      "2024-10-21 03:45:23 [INFO]: Finished training. The best model is from epoch#52.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 29/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Training BRITS on fold 29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 03:50:43 [INFO]: Epoch 001 - training loss: 0.6031\n",
      "2024-10-21 03:50:45 [INFO]: Epoch 002 - training loss: 0.6231\n",
      "2024-10-21 03:50:48 [INFO]: Epoch 003 - training loss: 0.6330\n",
      "2024-10-21 03:50:51 [INFO]: Epoch 004 - training loss: 0.6229\n",
      "2024-10-21 03:50:54 [INFO]: Epoch 005 - training loss: 0.6237\n",
      "2024-10-21 03:50:57 [INFO]: Epoch 006 - training loss: 0.6272\n",
      "2024-10-21 03:51:00 [INFO]: Epoch 007 - training loss: 0.6262\n",
      "2024-10-21 03:51:02 [INFO]: Epoch 008 - training loss: 0.6185\n",
      "2024-10-21 03:51:05 [INFO]: Epoch 009 - training loss: 0.6160\n",
      "2024-10-21 03:51:08 [INFO]: Epoch 010 - training loss: 0.6085\n",
      "2024-10-21 03:51:11 [INFO]: Epoch 011 - training loss: 0.6130\n",
      "2024-10-21 03:51:14 [INFO]: Epoch 012 - training loss: 0.6068\n",
      "2024-10-21 03:51:17 [INFO]: Epoch 013 - training loss: 0.5985\n",
      "2024-10-21 03:51:19 [INFO]: Epoch 014 - training loss: 0.6070\n",
      "2024-10-21 03:51:22 [INFO]: Epoch 015 - training loss: 0.6044\n",
      "2024-10-21 03:51:25 [INFO]: Epoch 016 - training loss: 0.5964\n",
      "2024-10-21 03:51:28 [INFO]: Epoch 017 - training loss: 0.6006\n",
      "2024-10-21 03:51:31 [INFO]: Epoch 018 - training loss: 0.6010\n",
      "2024-10-21 03:51:34 [INFO]: Epoch 019 - training loss: 0.5948\n",
      "2024-10-21 03:51:36 [INFO]: Epoch 020 - training loss: 0.5995\n",
      "2024-10-21 03:51:39 [INFO]: Epoch 021 - training loss: 0.5994\n",
      "2024-10-21 03:51:42 [INFO]: Epoch 022 - training loss: 0.5983\n",
      "2024-10-21 03:51:45 [INFO]: Epoch 023 - training loss: 0.5966\n",
      "2024-10-21 03:51:48 [INFO]: Epoch 024 - training loss: 0.5952\n",
      "2024-10-21 03:51:50 [INFO]: Epoch 025 - training loss: 0.5959\n",
      "2024-10-21 03:51:53 [INFO]: Epoch 026 - training loss: 0.5960\n",
      "2024-10-21 03:51:56 [INFO]: Epoch 027 - training loss: 0.5997\n",
      "2024-10-21 03:51:59 [INFO]: Epoch 028 - training loss: 0.5964\n",
      "2024-10-21 03:52:02 [INFO]: Epoch 029 - training loss: 0.5993\n",
      "2024-10-21 03:52:04 [INFO]: Epoch 030 - training loss: 0.5949\n",
      "2024-10-21 03:52:07 [INFO]: Epoch 031 - training loss: 0.5927\n",
      "2024-10-21 03:52:10 [INFO]: Epoch 032 - training loss: 0.5960\n",
      "2024-10-21 03:52:13 [INFO]: Epoch 033 - training loss: 0.5925\n",
      "2024-10-21 03:52:16 [INFO]: Epoch 034 - training loss: 0.5925\n",
      "2024-10-21 03:52:18 [INFO]: Epoch 035 - training loss: 0.5939\n",
      "2024-10-21 03:52:21 [INFO]: Epoch 036 - training loss: 0.5943\n",
      "2024-10-21 03:52:24 [INFO]: Epoch 037 - training loss: 0.5893\n",
      "2024-10-21 03:52:27 [INFO]: Epoch 038 - training loss: 0.5928\n",
      "2024-10-21 03:52:30 [INFO]: Epoch 039 - training loss: 0.5943\n",
      "2024-10-21 03:52:33 [INFO]: Epoch 040 - training loss: 0.5943\n",
      "2024-10-21 03:52:35 [INFO]: Epoch 041 - training loss: 0.5951\n",
      "2024-10-21 03:52:38 [INFO]: Epoch 042 - training loss: 0.5958\n",
      "2024-10-21 03:52:41 [INFO]: Epoch 043 - training loss: 0.5930\n",
      "2024-10-21 03:52:44 [INFO]: Epoch 044 - training loss: 0.5880\n",
      "2024-10-21 03:52:47 [INFO]: Epoch 045 - training loss: 0.5923\n",
      "2024-10-21 03:52:50 [INFO]: Epoch 046 - training loss: 0.5974\n",
      "2024-10-21 03:52:52 [INFO]: Epoch 047 - training loss: 0.5904\n",
      "2024-10-21 03:52:55 [INFO]: Epoch 048 - training loss: 0.6009\n",
      "2024-10-21 03:52:58 [INFO]: Epoch 049 - training loss: 0.5927\n",
      "2024-10-21 03:53:01 [INFO]: Epoch 050 - training loss: 0.5958\n",
      "2024-10-21 03:53:04 [INFO]: Epoch 051 - training loss: 0.5925\n",
      "2024-10-21 03:53:06 [INFO]: Epoch 052 - training loss: 0.5975\n",
      "2024-10-21 03:53:09 [INFO]: Epoch 053 - training loss: 0.6064\n",
      "2024-10-21 03:53:12 [INFO]: Epoch 054 - training loss: 0.6113\n",
      "2024-10-21 03:53:15 [INFO]: Epoch 055 - training loss: 0.6072\n",
      "2024-10-21 03:53:18 [INFO]: Epoch 056 - training loss: 0.6102\n",
      "2024-10-21 03:53:20 [INFO]: Epoch 057 - training loss: 0.6120\n",
      "2024-10-21 03:53:23 [INFO]: Epoch 058 - training loss: 0.6071\n",
      "2024-10-21 03:53:26 [INFO]: Epoch 059 - training loss: 0.6111\n",
      "2024-10-21 03:53:29 [INFO]: Epoch 060 - training loss: 0.6044\n",
      "2024-10-21 03:53:32 [INFO]: Epoch 061 - training loss: 0.6007\n",
      "2024-10-21 03:53:34 [INFO]: Epoch 062 - training loss: 0.5975\n",
      "2024-10-21 03:53:37 [INFO]: Epoch 063 - training loss: 0.6016\n",
      "2024-10-21 03:53:40 [INFO]: Epoch 064 - training loss: 0.5998\n",
      "2024-10-21 03:53:43 [INFO]: Epoch 065 - training loss: 0.5979\n",
      "2024-10-21 03:53:46 [INFO]: Epoch 066 - training loss: 0.5933\n",
      "2024-10-21 03:53:48 [INFO]: Epoch 067 - training loss: 0.5968\n",
      "2024-10-21 03:53:51 [INFO]: Epoch 068 - training loss: 0.6010\n",
      "2024-10-21 03:53:54 [INFO]: Epoch 069 - training loss: 0.5922\n",
      "2024-10-21 03:53:57 [INFO]: Epoch 070 - training loss: 0.6054\n",
      "2024-10-21 03:54:00 [INFO]: Epoch 071 - training loss: 0.5983\n",
      "2024-10-21 03:54:03 [INFO]: Epoch 072 - training loss: 0.5928\n",
      "2024-10-21 03:54:05 [INFO]: Epoch 073 - training loss: 0.5918\n",
      "2024-10-21 03:54:08 [INFO]: Epoch 074 - training loss: 0.5931\n",
      "2024-10-21 03:54:11 [INFO]: Epoch 075 - training loss: 0.5960\n",
      "2024-10-21 03:54:14 [INFO]: Epoch 076 - training loss: 0.5932\n",
      "2024-10-21 03:54:17 [INFO]: Epoch 077 - training loss: 0.5985\n",
      "2024-10-21 03:54:19 [INFO]: Epoch 078 - training loss: 0.5965\n",
      "2024-10-21 03:54:22 [INFO]: Epoch 079 - training loss: 0.5936\n",
      "2024-10-21 03:54:25 [INFO]: Epoch 080 - training loss: 0.5898\n",
      "2024-10-21 03:54:28 [INFO]: Epoch 081 - training loss: 0.5897\n",
      "2024-10-21 03:54:31 [INFO]: Epoch 082 - training loss: 0.5925\n",
      "2024-10-21 03:54:34 [INFO]: Epoch 083 - training loss: 0.5943\n",
      "2024-10-21 03:54:36 [INFO]: Epoch 084 - training loss: 0.5957\n",
      "2024-10-21 03:54:39 [INFO]: Epoch 085 - training loss: 0.5904\n",
      "2024-10-21 03:54:42 [INFO]: Epoch 086 - training loss: 0.5861\n",
      "2024-10-21 03:54:45 [INFO]: Epoch 087 - training loss: 0.5849\n",
      "2024-10-21 03:54:48 [INFO]: Epoch 088 - training loss: 0.5891\n",
      "2024-10-21 03:54:50 [INFO]: Epoch 089 - training loss: 0.5931\n",
      "2024-10-21 03:54:53 [INFO]: Epoch 090 - training loss: 0.5873\n",
      "2024-10-21 03:54:56 [INFO]: Epoch 091 - training loss: 0.5940\n",
      "2024-10-21 03:54:59 [INFO]: Epoch 092 - training loss: 0.5994\n",
      "2024-10-21 03:55:02 [INFO]: Epoch 093 - training loss: 0.5913\n",
      "2024-10-21 03:55:04 [INFO]: Epoch 094 - training loss: 0.5916\n",
      "2024-10-21 03:55:07 [INFO]: Epoch 095 - training loss: 0.5921\n",
      "2024-10-21 03:55:10 [INFO]: Epoch 096 - training loss: 0.5829\n",
      "2024-10-21 03:55:13 [INFO]: Epoch 097 - training loss: 0.5788\n",
      "2024-10-21 03:55:16 [INFO]: Epoch 098 - training loss: 0.5888\n",
      "2024-10-21 03:55:19 [INFO]: Epoch 099 - training loss: 0.5938\n",
      "2024-10-21 03:55:21 [INFO]: Epoch 100 - training loss: 0.5930\n",
      "2024-10-21 03:55:21 [INFO]: Finished training. The best model is from epoch#97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n"
     ]
    }
   ],
   "source": [
    "fold_scores_brits = evaluate_folds_brits(Xs, ys, resample_fold_idxs, window_idxs, brits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save BRITS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"ECG200_brits_results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(fold_scores_brits, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test CDREC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evlaute_folds_cdrec(Xs, ys, fold_idxs, window_idxs):\n",
    "    fold_scores = dict()\n",
    "    for fold in range(0, len(fold_idxs)):\n",
    "        print(f\"Evaluating fold {fold}/{len(fold_idxs)-1}...\")\n",
    "        # make the splits\n",
    "        X_train_fold = Xs[fold_idxs[fold][\"train\"]]\n",
    "        y_train_fold = ys[fold_idxs[fold][\"train\"]]\n",
    "        X_test_fold = Xs[fold_idxs[fold][\"test\"]]\n",
    "        y_test_fold = ys[fold_idxs[fold][\"test\"]]\n",
    "        # check class distributions\n",
    "        counts_tr = np.unique(y_train_fold, return_counts=True)[1]\n",
    "        print(f\"Training class distribution: {counts_tr/np.sum(counts_tr)}\")\n",
    "        counts_te = np.unique(y_test_fold, return_counts=True)[1]\n",
    "        print(f\"Testing class distribution: {counts_te/np.sum(counts_te)}\")\n",
    "        print(f\"Computing CDrec on fold {fold}...\")\n",
    "        percent_missing_score = dict()\n",
    "        for pm in window_idxs:\n",
    "            print(f\"Imputing {pm}% missing data over {len(window_idxs[pm])} windows...\")\n",
    "            per_window_scores = dict()\n",
    "            for (idx, widx) in enumerate(window_idxs[pm]):\n",
    "                X_test_corrupted = X_test_fold.copy()\n",
    "                X_test_corrupted[:, widx] = np.nan\n",
    "                mask = np.isnan(X_test_corrupted) # mask ensures only misisng values are imputed\n",
    "                Xdata = np.concatenate([X_train_fold.squeeze(), X_test_corrupted.squeeze()])\n",
    "                cdrec_imputed_raw = CDrec(matrix=Xdata) # using default paramss\n",
    "                cdrec_imputed = cdrec_imputed_raw[X_train_original.shape[0]:][:].reshape([-1, 96, 1]) # only the test data from the concatenated matrix            \n",
    "                errs = [calc_mae(cdrec_imputed[i], X_test_fold[i], mask[i]) for i in range(0, X_test_fold.shape[0])] # get individual errors for uncertainty quantification\n",
    "                per_window_scores[idx] = errs\n",
    "            percent_missing_score[pm] = per_window_scores\n",
    "        fold_scores[fold] = percent_missing_score\n",
    "    return fold_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 0/29...\n",
      "Training class distribution: [0.31 0.69]\n",
      "Testing class distribution: [0.36 0.64]\n",
      "Computing CDrec on fold 0...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 1/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Computing CDrec on fold 1...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 2/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Computing CDrec on fold 2...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 3/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Computing CDrec on fold 3...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 4/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Computing CDrec on fold 4...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 5/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Computing CDrec on fold 5...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 6/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Computing CDrec on fold 6...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 7/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Computing CDrec on fold 7...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 8/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Computing CDrec on fold 8...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 9/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Computing CDrec on fold 9...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 10/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Computing CDrec on fold 10...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 11/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Computing CDrec on fold 11...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 12/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Computing CDrec on fold 12...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 13/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Computing CDrec on fold 13...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 14/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Computing CDrec on fold 14...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 15/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Computing CDrec on fold 15...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 16/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Computing CDrec on fold 16...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 17/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Computing CDrec on fold 17...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 18/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Computing CDrec on fold 18...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 19/29...\n",
      "Training class distribution: [0.34 0.66]\n",
      "Testing class distribution: [0.33 0.67]\n",
      "Computing CDrec on fold 19...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 20/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Computing CDrec on fold 20...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 21/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Computing CDrec on fold 21...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n",
      "Imputing 85% missing data over 14 windows...\n",
      "Imputing 95% missing data over 5 windows...\n",
      "Evaluating fold 22/29...\n",
      "Training class distribution: [0.33 0.67]\n",
      "Testing class distribution: [0.34 0.66]\n",
      "Computing CDrec on fold 22...\n",
      "Imputing 5% missing data over 15 windows...\n",
      "Imputing 15% missing data over 15 windows...\n",
      "Imputing 25% missing data over 15 windows...\n",
      "Imputing 35% missing data over 15 windows...\n",
      "Imputing 45% missing data over 15 windows...\n",
      "Imputing 55% missing data over 15 windows...\n",
      "Imputing 65% missing data over 15 windows...\n",
      "Imputing 75% missing data over 15 windows...\n"
     ]
    }
   ],
   "source": [
    "fold_scores_cdrec = evlaute_folds_cdrec(Xs, ys, resample_fold_idxs, window_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_corrupted = X_test_original.copy()\n",
    "for samp in X_test_corrupted:\n",
    "    samp[range(5, 10)] = np.nan\n",
    "mask = np.isnan(X_test_corrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdata = np.concatenate([X_train_original.squeeze(), X_test_corrupted.squeeze()])\n",
    "all(Xdata[0] == X_train.squeeze()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_original.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6759018631807423\n"
     ]
    }
   ],
   "source": [
    "cdrec_impute_raw = CDrec(matrix=Xdata)\n",
    "cdrec_imputed = cdrec_impute_raw[X_train_original.shape[0]:][:].reshape([-1, 96,1])\n",
    "err = calc_mae(cdrec_imputed, X_test_original, mask)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'MAE: 0.42340245241718155')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 7\n",
    "plt.plot(X_test_original[idx])\n",
    "plt.plot(cdrec_imputed[idx])\n",
    "err = calc_mae(cdrec_imputed[idx], X_test_original[idx], mask[idx])\n",
    "plt.title(f\"MAE: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
