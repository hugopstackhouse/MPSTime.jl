{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 12:51:00 [INFO]: Have set the random seed as 1234 for numpy and pytorch.\n"
     ]
    }
   ],
   "source": [
    "import pypots\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tsdb\n",
    "import benchpots\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import CSDI, MRNN, BRITS\n",
    "\n",
    "from pypots.utils.random import set_random_seed\n",
    "from pypots.utils.metrics import calc_mae\n",
    "\n",
    "#from cdrec.python.recovery import centroid_recovery as CDrec\n",
    "\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 12:51:04 [INFO]: You're using dataset ucr_uea_ECG200, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/ucr_uea_datasets\n",
      "2024-10-18 12:51:04 [INFO]: Dataset ucr_uea_ECG200 has already been downloaded. Processing directly...\n",
      "2024-10-18 12:51:04 [INFO]: Dataset ucr_uea_ECG200 has already been cached. Loading from cache directly...\n",
      "2024-10-18 12:51:04 [INFO]: Loaded successfully!\n",
      "2024-10-18 12:51:04 [WARNING]: rate is 0, no missing values are artificially added.\n",
      "2024-10-18 12:51:04 [INFO]: Total sample number: 200\n",
      "2024-10-18 12:51:04 [INFO]: Training set size: 80 (40.00%)\n",
      "2024-10-18 12:51:04 [INFO]: Validation set size: 20 (10.00%)\n",
      "2024-10-18 12:51:04 [INFO]: Test set size: 100 (50.00%)\n",
      "2024-10-18 12:51:04 [INFO]: Number of steps: 1\n",
      "2024-10-18 12:51:04 [INFO]: Number of features: 96\n",
      "2024-10-18 12:51:04 [INFO]: Train set missing rate: 0.00%\n",
      "2024-10-18 12:51:04 [INFO]: Validating set missing rate: 0.00%\n",
      "2024-10-18 12:51:04 [INFO]: Test set missing rate: 0.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_steps', 'n_features', 'scaler', 'train_X', 'train_y', 'val_X', 'val_y', 'test_X', 'test_y', 'label_encoder'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecg200_2 = benchpots.datasets.preprocess_ucr_uea_datasets('ucr_uea_ECG200', 0)\n",
    "ecg200_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 12:51:31 [INFO]: You're using dataset ucr_uea_ECG200, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/ucr_uea_datasets\n",
      "2024-10-18 12:51:31 [INFO]: Dataset ucr_uea_ECG200 has already been downloaded. Processing directly...\n",
      "2024-10-18 12:51:31 [INFO]: Dataset ucr_uea_ECG200 has already been cached. Loading from cache directly...\n",
      "2024-10-18 12:51:31 [INFO]: Loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "ecg200 = tsdb.load('ucr_uea_ECG200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ecg200['X_train'].transpose([0,2,1])\n",
    "y_train = ecg200['y_train']\n",
    "\n",
    "X_test = ecg200['X_test'].transpose([0,2,1])\n",
    "y_test = ecg200['y_test']\n",
    "n_steps = len(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 12:51:55 [INFO]: No given device, using default device: cpu\n",
      "2024-10-18 12:51:55 [INFO]: Model files will be saved to results/csdi/20241018_T125155\n",
      "2024-10-18 12:51:55 [INFO]: Tensorboard file will be saved to results/csdi/20241018_T125155/tensorboard\n",
      "/Users/joshua/opt/anaconda3/envs/imputation/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "2024-10-18 12:51:55 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,693,601\n"
     ]
    }
   ],
   "source": [
    "csdi = CSDI(\n",
    "    n_steps=n_steps,\n",
    "    n_features=1,\n",
    "    n_layers=6,\n",
    "    n_heads=2,\n",
    "    n_channels=128,\n",
    "    d_time_embedding=64,\n",
    "    d_feature_embedding=32,\n",
    "    d_diffusion_embedding=128,\n",
    "    target_strategy=\"random\",\n",
    "    n_diffusion_steps=50,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=100,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    #patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files \n",
    "    saving_path=\"results/csdi\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcsdi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/imputation/lib/python3.10/site-packages/pypots/imputation/csdi/model.py:331\u001b[0m, in \u001b[0;36mCSDI.fit\u001b[0;34m(self, train_set, val_set, file_type, n_sampling_times)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    325\u001b[0m     train_set: Union[\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# Step 1: wrap the input data with classes Dataset and DataLoader\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     training_set \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetForCSDI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_X_ori\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     training_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    338\u001b[0m         training_set,\n\u001b[1;32m    339\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    340\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    341\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    343\u001b[0m     val_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/imputation/lib/python3.10/site-packages/pypots/imputation/csdi/data.py:37\u001b[0m, in \u001b[0;36mDatasetForCSDI.__init__\u001b[0;34m(self, data, target_strategy, return_X_ori, file_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     32\u001b[0m     data: Union[\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     file_type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m ):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_X_ori\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_X_ori\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_X_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m target_strategy \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmix\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_strategy \u001b[38;5;241m=\u001b[39m target_strategy\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/imputation/lib/python3.10/site-packages/pypots/data/dataset/base.py:150\u001b[0m, in \u001b[0;36mBaseDataset.__init__\u001b[0;34m(self, data, return_X_ori, return_X_pred, return_y, file_type)\u001b[0m\n\u001b[1;32m    148\u001b[0m X_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    149\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_ori, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_pred, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_array_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ori\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtensor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_X_ori:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Only when X_ori is given and fixed, we fill the missing values in X here in advance.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# Otherwise, we may need original X with missing values to generate X_ori, e.g. in DatasetForSAITS.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_mask \u001b[38;5;241m=\u001b[39m fill_and_get_mask_torch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/imputation/lib/python3.10/site-packages/pypots/data/dataset/base.py:280\u001b[0m, in \u001b[0;36mBaseDataset._check_array_input\u001b[0;34m(X, X_ori, X_pred, y, out_dtype)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out_dtype \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    277\u001b[0m ], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_dtype should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# change the data type of X\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mturn_data_into_specified_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# check the shape of X here\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/imputation/lib/python3.10/site-packages/pypots/data/utils.py:26\u001b[0m, in \u001b[0;36mturn_data_into_specified_dtype\u001b[0;34m(data, dtype)\u001b[0m\n\u001b[1;32m     24\u001b[0m     data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data) \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m---> 26\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m data\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata should be an instance of list/np.ndarray/torch.Tensor, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "csdi.fit(train_set={'X':X_train})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
