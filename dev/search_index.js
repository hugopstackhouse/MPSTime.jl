var documenterSearchIndex = {"docs":
[{"location":"tutorial/#Tutorial","page":"Tutorial: Classification","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"This tutorial for MPSTime will take you through the basic steps needed to fit an MPS to a time-series dataset.","category":"page"},{"location":"tutorial/#Demo-dataset","page":"Tutorial: Classification","title":"Demo dataset","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"First, import or generate your data. Here, we generate a two class \"noisy trendy sine\" dataset for the sake of demonstration, but if you have a dataset in mind, you can skip to the next section. Our demonstration dataset consists of a sine function with a randomised phase, plus a linear trend, plus some normally distributed noise. Each time series in class c at time t is given by:","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"x^c_t = sinleft(frac2pi20t + psiright) + fracmtT + sigma_c n_t","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"where m is the slope of a linear trend, psi in 0 2pi) is a uniformly random phase offset, sigma_c is the noise scale, and n_t sim mathcalN(01) are  normally distributed random variables. ","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"The two classes will be distinguished by their noise levels. The class one time series x^1 have sigma_1 = 01, and the class two time series x^2 have sigma_2 = 09. The below code sets this up","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"# fix rng seed\nusing Random\nrng = Xoshiro(1)\n\n\n# trendy sine function\nfunction trendy_sine(T::Integer, n_inst::Integer, noise_std::Real, rng)\n    X = Matrix{Float64}(undef, n_inst, T)\n    ts = 1:T\n    for series in eachrow(X)\n        phase = 2 * pi * rand(rng)\n        @. series = sin(pi/10 *ts + phase) + 3 * ts / T + noise_std * randn(rng) \n    end\n    return X\nend\n\n# dataset size\nntimepoints = 100\nntrain_instances = 300\nntest_instances = 200\n\n# define data. Class one has sigma = 0.1, class 2 has sigma = 0.9\nX_train = vcat(trendy_sine(ntimepoints, ntrain_instances ÷ 2, 0.1, rng), trendy_sine(ntimepoints, ntrain_instances ÷ 2, 0.9, rng));\ny_train = vcat(fill(1, ntrain_instances ÷ 2), fill(2, ntrain_instances ÷ 2));\nX_test = vcat(trendy_sine(ntimepoints, ntest_instances ÷ 2, 0.1, rng), trendy_sine(ntimepoints, ntest_instances ÷ 2, 0.9, rng));\ny_test = vcat(fill(1, ntest_instances ÷ 2), fill(2, ntest_instances ÷ 2));\n","category":"page"},{"location":"tutorial/#Training-an-MPS","page":"Tutorial: Classification","title":"Training an MPS","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"For the most basic use of fitMPS, select your hyperparameters, and run the fitMPS function. Some (truncated) output from our noisy trendy sine datam with default hyperparameters is given below. ","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"julia> opts = MPSOptions() # calling this with no arguments gives default hyperparameters\njulia> mps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, opts);\nGenerating initial weight MPS with bond dimension χ_init = 4\n        using random state 1234.\nInitialising train states.\nInitialising test states.\nUsing 1 iterations per update.\nTraining KL Div. 122.43591167452153 | Training acc. 0.51.\nTest KL Div. 121.83350501986212 | Testing acc. 0.55.\n\nTest conf: [55 45; 45 55].\nUsing optimiser CustomGD with the \"TSGO\" algorithm\nStarting backward sweeep: [1/5]\nBackward sweep finished.\nStarting forward sweep: [1/5]\n    ...\n\nMPS normalised!\n\nTraining KL Div. -18.149569463050405 | Training acc. 1.0.\nTest KL Div. -1.2806885386973699 | Testing acc. 0.925.\n\nTest conf: [100 0; 15 85]. ","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"fitMPS doesn't use X_test or y_test for anything except printing performance evaluations, so it is safe to leave them blank. For unsupervised learning, input a dataset with only one class, or only pass X_train ( y_train has a default value of zeros(Int, size(X_train, 1)) ).","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"The mps::TrainedMPS can be passed directly to classify for classification, or init_imputation_problem to set up an imputation problem. The info info provides a short training summary, which can be pretty-printed with sweep_summary.","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"You can use test_states to print a summary of the MPS performance on the test set.","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"Julia> get_training_summary(mps, test_states; print_stats=true)\n\n         Overlap Matrix\n┌──────┬───────────┬───────────┐\n│      │   |ψ1⟩    │   |ψ2⟩    │\n├──────┼───────────┼───────────┤\n│ ⟨ψ1| │ 5.022e-01 │ 2.216e-04 │\n├──────┼───────────┼───────────┤\n│ ⟨ψ2| │ 2.216e-04 │ 4.978e-01 │\n└──────┴───────────┴───────────┘\n          Confusion Matrix\n┌──────────┬───────────┬───────────┐\n│          │ Pred. |1⟩ │ Pred. |2⟩ │\n├──────────┼───────────┼───────────┤\n│ True |1⟩ │       100 │         0 │\n├──────────┼───────────┼───────────┤\n│ True |2⟩ │        15 │        85 │\n└──────────┴───────────┴───────────┘\n┌───────────────────┬───────────┬──────────┬──────────┬─────────────┬─────────┬───────────┐\n│ test_balanced_acc │ train_acc │ test_acc │ f1_score │ specificity │  recall │ precision │\n│           Float64 │   Float64 │  Float64 │  Float64 │     Float64 │ Float64 │   Float64 │\n├───────────────────┼───────────┼──────────┼──────────┼─────────────┼─────────┼───────────┤\n│             0.925 │       1.0 │    0.925 │ 0.924576 │       0.925 │   0.925 │  0.934783 │\n└───────────────────┴───────────┴──────────┴──────────┴─────────────┴─────────┴───────────┘","category":"page"},{"location":"tutorial/#Hyperparameters","page":"Tutorial: Classification","title":"Hyperparameters","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"There are number of hyperparameters and data preprocessing options that can be specified using MPSOptions(; key=value)","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"MPSOptions","category":"page"},{"location":"tutorial/#MPSTime.MPSOptions","page":"Tutorial: Classification","title":"MPSTime.MPSOptions","text":"MPSOptions(; <Keyword Arguments>)\n\nSet the hyperparameters and other options for fitMPS. \n\nFields:\n\nLogging\n\nverbosity::Int=1: How much debug/progress info to print to the terminal while optimising the MPS. Higher numbers mean more output\nlog_level::Int=3: How much statistical output. 0 for nothing, >0 to print losses, accuracies, and confusion matrix at each step (noticeable) computational overhead) #TODO implement finer grain control\ntrack_cost::Bool=false: Whether to print the cost at each Bond tensor site to the terminal while training, mostly useful for debugging new cost functions or optimisers (HUGE computational overhead)\n\nMPS Training Hyperparameters\n\nnsweeps::Int=5: Number of MPS optimisation sweeps to perform (Both forwards and Backwards)\nchi_max::Int=25: Maximum bond dimension allowed within the MPS during the SVD step\neta::Float64=0.01: The learning rate. For gradient descent methods, this is the step size. For Optim and OptimKit this serves as the initial step size guess input into the linesearch\nd::Int=5: The dimension of the feature map or \"Encoding\". This is the true maximum dimension of the feature vectors. For a splitting encoding, d = numsplits * auxbasis_dim\ncutoff::Float64=1E-10: Size based cutoff for the number of singular values in the SVD (See Itensors SVD documentation)\ndtype::DataType=Float64 or ComplexF64 depending on encoding: The datatype of the elements of the MPS. Supports the arbitrary precsion types such as BigFloat and Complex{BigFloat}\nexit_early::Bool=false: Stops training if training accuracy is 1 at the end of any sweep.\n\nEncoding Options\n\nencoding::Symbol=:Legendre: The encoding to use, including :Stoudenmire, :Fourier, :Legendre, :SLTD, :Custom, etc. see Encoding docs for a complete list. Can be just a time (in)dependent orthonormal basis, or a time (in)dependent basis mapped onto a number of \"splits\" which distribute tighter basis functions where the sites of a timeseries are more likely to be measured.  \nprojected_basis::Bool=false: Whether toproject a basis onto the training data at each time. Normally, when specifying a basis of dimension d, the first d lowest order terms are used. When project=true, the training data is used to construct a pdf of the possible timeseries amplitudes at each time point. The first d largest terms of this pdf expanded in a series are used to select the basis terms.\naux_basis_dim::Int=2: Unused for standard encodings. If the encoding is a SplitBasis, serves as the auxilliary dimension of a basis mapped onto the split encoding, so that the number of histogram bins = d / auxbasisdim. \nencode_classes_separately::Bool=false: Only relevant for data driven bases. If true, then data is split up by class before being encoded. Functionally, this causes the encoding method to vary depending on the class\n\nData Preprocessing and MPS initialisation\n\nsigmoid_transform::Bool: Whether to apply a sigmoid transform to the data before minmaxing. This has the form\n\nboldsymbolX = left(1 + exp-fracboldsymbolX-m_boldsymbolXr_boldsymbolX  135right)^-1\n\nwhere boldsymbolX is the un-normalized time-series data matrix, m_boldsymbolX is the median of boldsymbolX and r_boldsymbolXis its interquartile range.\n\nminmax::Bool: Whether to apply a minmax norm to [0,1] before encoding. This has the form\n\nboldsymbolX =  fracboldsymbolX - x_textminx_textmax - x_textmin\n\nwhere boldsymbolX is the scaled robust-sigmoid transformed data matrix, x_textmin and x_textmax are the minimum and maximum of boldsymbolX.\n\ndata_bounds::Tuple{Float64, Float64} = (0.,1.): The region to bound the data to if minmax=true. This is separate from the encoding domain. All encodings expect data to be scaled scaled between 0 and 1. Setting the data bounds a bit away from [0,1] can help when your basis has poor support near its boundaries.\ninit_rng::Int: Random seed used to generate the initial MPS\nchi_init::Int: Initial bond dimension of the random MPS\n\nLoss Functions and Optimisation Methods\n\nloss_grad::Symbol=:KLD: The type of cost function to use for training the MPS, typically Mean Squared Error (:MSE) or KL Divergence (:MSE), but can also be a weighted sum of the two (:Mixed)\nbbopt::Symbol=:TSGO: Which local Optimiser to use, builtin options are symbol gradient descent (:GD), or gradient descent with a TSGO rule (:TSGO). Other options are Conjugate Gradient descent using either the Optim or OptimKit packages (:Optim or :OptimKit respectively). The CGD methods work well for MSE based loss functions, but seem to perform poorly for KLD base loss functions.\nrescale::Tuple{Bool,Bool}=(false,true): Has the form rescale = (before::Bool, after::Bool). Where to enforce the normalisation of the MPS during training, either calling normalise!(Bond Tensor) before or after BT is updated. Note that for an MPS that starts in canonical form, rescale = (true,true) will train identically to rescale = (false, true) but may be less performant.\nupdate_iters::Int=1: Maximum number of optimiser iterations to perform for each bond tensor optimisation. E.G. The number of steps of (Conjugate) Gradient Descent used by TSGO, Optim or OptimKit\ntrain_classes_separately::Bool=false: Whether the the trainer optimises the total MPS loss over all classes or whether it considers each class as a separate problem. Should make very little diffence\n\nDebug\n\nreturn_encoding_meta_info::Bool=false: Debug flag: Whether to return the normalised data as well as the histogram bins for the splitbasis types\n\n\n\n\n\nConvert the internal Options type into a serialisable MPSOptions.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"You can also print a formatted table of options with print_opts (beware long output)","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"print_opts(opts)","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"┌────────────┬──────────────┬──────────────────────────┬────────┬───────────────────────────┬──────────┬─────────┬─────────┬──────────┬──────────────────┬───────────────────┬───────────┬─────────────────────────┬──────────┬───────┬────────────┬───────────────────────────┬─────────┬───────────────────┬───────────────┬────────┬───────────┬───────────┬─────────────────┬─────────┐\n│ track_cost │ update_iters │ train_classes_separately │ minmax │ return_encoding_meta_info │    dtype │ nsweeps │  cutoff │ chi_init │         encoding │           rescale │ loss_grad │             data_bounds │ init_rng │     d │ exit_early │ encode_classes_separately │     eta │ sigmoid_transform │ aux_basis_dim │  bbopt │ log_level │ verbosity │ projected_basis │ chi_max │\n│       Bool │        Int64 │                     Bool │   Bool │                      Bool │ DataType │   Int64 │ Float64 │    Int64 │           Symbol │ Tuple{Bool, Bool} │    Symbol │ Tuple{Float64, Float64} │    Int64 │ Int64 │       Bool │                      Bool │ Float64 │              Bool │         Int64 │ Symbol │     Int64 │     Int64 │            Bool │   Int64 │\n├────────────┼──────────────┼──────────────────────────┼────────┼───────────────────────────┼──────────┼─────────┼─────────┼──────────┼──────────────────┼───────────────────┼───────────┼─────────────────────────┼──────────┼───────┼────────────┼───────────────────────────┼─────────┼───────────────────┼───────────────┼────────┼───────────┼───────────┼─────────────────┼─────────┤\n│      false │            1 │                    false │   true │                     false │  Float64 │       5 │ 1.0e-10 │        4 │ Legendre_No_Norm │     (false, true) │       KLD │              (0.0, 1.0) │     1234 │     5 │      false │                     false │    0.01 │              true │             2 │   TSGO │         3 │         1 │           false │      25 │\n└────────────┴──────────────┴──────────────────────────┴────────┴───────────────────────────┴──────────┴─────────┴─────────┴──────────┴──────────────────┴───────────────────┴───────────┴─────────────────────────┴──────────┴───────┴────────────┴───────────────────────────┴─────────┴───────────────────┴───────────────┴────────┴───────────┴───────────┴─────────────────┴─────────┘\n\njulia> ","category":"page"},{"location":"tutorial/#Classification","page":"Tutorial: Classification","title":"Classification","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"To predict the class of unseen data, use the classify function.","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"classify(::TrainedMPS, ::AbstractMatrix, ::MPSOptions)","category":"page"},{"location":"tutorial/#MPSTime.classify-Tuple{TrainedMPS, AbstractMatrix, MPSOptions}","page":"Tutorial: Classification","title":"MPSTime.classify","text":"classify(mps::TrainedMPS, X_test::AbstractMatrix, opts::AbstractMPSOptions)) -> (predictions::Vector)\n\nUse the mps to predict the class of the rows of X_test by computing the maximum overlap.\n\nExample\n\njulia> W, info, test_states = fitMPS( X_train, y_train, opts);\njulia> preds  = classify(W, X_test, opts); # make some predictions\njulia> mean(preds .== y_test)\n0.9504373177842566\n\n\n\n\n\n","category":"method"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"For example, for the noisy trendy sine from earlier:","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"julia> predictions = classify(mps, X_test, opts);\njulia> using Statistics\njulia> mean(predictions .== y_test)\n0.925","category":"page"},{"location":"tutorial/#Imputation","page":"Tutorial: Classification","title":"Imputation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"See Imputation","category":"page"},{"location":"tutorial/#Training-with-a-custom-basis","page":"Tutorial: Classification","title":"Training with a custom basis","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"To train with a custom basis, first, declare a custom basis with function_basis, and pass it in as the last argument to fitMPS. For this to work, the encoding hyperparameter must be set to :Custom in MPSOptions","category":"page"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"encoding = function_basis(...)\nfitMPS(X_train, y_train, X_test, y_test, MPSOptions(; encoding=:Custom), encoding)","category":"page"},{"location":"tutorial/#Docstrings","page":"Tutorial: Classification","title":"Docstrings","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial: Classification","title":"Tutorial: Classification","text":"fitMPS(::Matrix, ::Vector, ::Matrix, ::Vector, ::MPSOptions, ::Nothing)\nsweep_summary(info)\nget_training_summary(mps::TrainedMPS, test_states::EncodedTimeSeriesSet)\nprint_opts","category":"page"},{"location":"tutorial/#MPSTime.fitMPS-Tuple{Matrix, Vector, Matrix, Vector, MPSOptions, Nothing}","page":"Tutorial: Classification","title":"MPSTime.fitMPS","text":"fitMPS(X_train::AbstractMatrix, \n       y_train::AbstractVector=zeros(Int, size(X_train, 1)), \n       X_test::AbstractMatrix=zeros(0,0), \n       y_test::AbstractVector=zeros(Int, 0), \n       opts::AbstractMPSOptions=MPSOptions(),\n       custom_encoding::Union{Encoding, Nothing}=nothing) -> (MPS::TrainedMPS, training_info::Dict, encoded_test_states::EncodedTimeSeriesSet)\n\nTrain an MPS on the data X_train using the hyperparameters opts, see MPSOptions. The number of classes are determined by the entries of y_train. Fo\n\nReturns a trained MPS, a dictionary containing training info, and the encoded test states. X_test and y_test are used only to print performance evaluations, and may be empty.  The return value encoded_test_states will be sorted by class, so predictions shouldn't be compared directly to y_test. The custom_encoding argument allows the use of user defined custom encodings, see function_basis. This requires that encoding=:Custom is specified in MPSOptions\n\nSee also: Encoding\n\nExample\n\nSee ??fitMPS to for a more verbose example\n\njulia> opts = MPSOptions(; d=5, chi_max=30, encoding=:Legendre, eta=0.05);\njulia> print_opts(opts) # Prints options as a table\n       ...\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\nGenerating initial weight MPS with bond dimension χ_init = 4\n        using random state 1234.\nInitialising train states.\nUsing 1 iterations per update.\nTraining KL Div. 28.213032851945012 | Training acc. 0.31343283582089554.\nUsing optimiser CustomGD with the \"TSGO\" algorithm\nStarting backward sweeep: [1/5]\n        ...\n\nStarting forward sweep: [5/5]\nFinished sweep 5. Time for sweep: 0.76s\nTraining KL Div. -12.577920427063361 | Training acc. 1.0.\n\nMPS normalised!\n\nTraining KL Div. -12.57792042706337 | Training acc. 1.0.\nTest KL Div. -9.815236609211746 | Testing acc. 0.9504373177842566.\n\nTest conf: [497 16; 35 481].\n\njulia> \n\n\nExtended help\n\njulia> Using JLD2 # load some data\njulia> dloc = \"test/Data/italypower/datasets/ItalyPowerDemandOrig.jld2\"\njulia> f = jldopen(dloc, \"r\") \n           X_train = read(f, \"X_train\")\n           y_train = read(f, \"y_train\")\n           X_test = read(f, \"X_test\")\n           y_test = read(f, \"y_test\")\n       close(f);\njulia> opts = MPSOptions(; d=5, chi_max=30, encoding=:Legendre, eta=0.05);\njulia> print_opts(opts) # Prints options as a table\n       ...\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\nGenerating initial weight MPS with bond dimension χ_init = 4\n        using random state 1234.\nInitialising train states.\nUsing 1 iterations per update.\nTraining KL Div. 28.213032851945012 | Training acc. 0.31343283582089554.\nUsing optimiser CustomGD with the \"TSGO\" algorithm\nStarting backward sweeep: [1/5]\n        ...\n\nStarting forward sweep: [5/5]\nFinished sweep 5. Time for sweep: 0.76s\nTraining KL Div. -12.577920427063361 | Training acc. 1.0.\n\nMPS normalised!\n\nTraining KL Div. -12.57792042706337 | Training acc. 1.0.\nTest KL Div. -9.815236609211746 | Testing acc. 0.9504373177842566.\n\nTest conf: [497 16; 35 481].\n\njulia> get_training_summary(W, test_states; print_stats=true);\n         Overlap Matrix\n┌──────┬───────────┬───────────┐\n│      │   |ψ0⟩    │   |ψ1⟩    │\n├──────┼───────────┼───────────┤\n│ ⟨ψ0| │ 5.074e-01 │ 1.463e-02 │\n├──────┼───────────┼───────────┤\n│ ⟨ψ1| │ 1.463e-02 │ 4.926e-01 │\n└──────┴───────────┴───────────┘\n          Confusion Matrix\n┌──────────┬───────────┬───────────┐\n│          │ Pred. |0⟩ │ Pred. |1⟩ │\n├──────────┼───────────┼───────────┤\n│ True |0⟩ │       497 │        16 │\n├──────────┼───────────┼───────────┤\n│ True |1⟩ │        35 │       481 │\n└──────────┴───────────┴───────────┘\n┌───────────────────┬───────────┬──────────┬──────────┬─────────────┬──────────┬───────────┐\n│ test_balanced_acc │ train_acc │ test_acc │ f1_score │ specificity │   recall │ precision │\n│           Float64 │   Float64 │  Float64 │  Float64 │     Float64 │  Float64 │   Float64 │\n├───────────────────┼───────────┼──────────┼──────────┼─────────────┼──────────┼───────────┤\n│          0.950491 │       1.0 │ 0.950437 │ 0.950425 │    0.950491 │ 0.950491 │  0.951009 │\n└───────────────────┴───────────┴──────────┴──────────┴─────────────┴──────────┴───────────┘\n\njulia> sweep_summary(info)\n┌────────────────┬──────────┬───────────────┬───────────────┬───────────────┬───────────────┬───────────────┬────────────┬──────────┐\n│                │ Initial  │ After Sweep 1 │ After Sweep 2 │ After Sweep 3 │ After Sweep 4 │ After Sweep 5 │ After Norm │   Mean   │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│ Train Accuracy │ 0.313433 │      1.0      │      1.0      │      1.0      │      1.0      │      1.0      │    1.0     │   1.0    │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│  Test Accuracy │ 0.409135 │   0.947522    │   0.951409    │   0.948494    │   0.948494    │   0.950437    │  0.950437  │ 0.949271 │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│  Train KL Div. │  28.213  │   -11.7855    │    -12.391    │   -12.4831    │   -12.5466    │   -12.5779    │  -12.5779  │ -12.3568 │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│   Test KL Div. │ 27.7435  │   -9.12893    │   -9.73479    │   -9.79248    │    -9.8158    │   -9.81524    │  -9.81524  │ -9.65745 │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│     Time taken │   0.0    │   0.658366    │    0.75551    │   0.719035    │   0.718444    │    1.16256    │    NaN     │ 0.802783 │\n└────────────────┴──────────┴───────────────┴───────────────┴───────────────┴───────────────┴───────────────┴────────────┴──────────┘\n\n\n\n\n\n\n","category":"method"},{"location":"tutorial/#MPSTime.sweep_summary-Tuple{Any}","page":"Tutorial: Classification","title":"MPSTime.sweep_summary","text":"sweep_summary(info; io::IO=stdin)\n\nPrint a pretty summary of what happened in every sweep\n\n\n\n\n\n","category":"method"},{"location":"tutorial/#MPSTime.get_training_summary-Tuple{TrainedMPS, EncodedTimeSeriesSet}","page":"Tutorial: Classification","title":"MPSTime.get_training_summary","text":"get_training_summary(mps::TrainedMPS, \n                     test_states::EncodedTimeSeriesSet;  \n                     print_stats::Bool=false, \n                     io::IO=stdin) -> stats::Dict\n\nPrint a summary of the training process of mps, with performane evaluated on test_states.\n\n\n\n\n\n","category":"method"},{"location":"tutorial/#MPSTime.print_opts","page":"Tutorial: Classification","title":"MPSTime.print_opts","text":"print_opts(opts::AbstractMPSOptions; io::IO=stdin)\n\nPrint the MPSOptions struct in a table.\n\n\n\n\n\n","category":"function"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"We cite the following works in the development of MPSTime.jl:","category":"page"},{"location":"references/","page":"References","title":"References","text":"M. Fishman, S. R. White and E. M. Stoudenmire. The ITensor Software Library for Tensor Network Calculations. SciPost Phys. Codebases, 4 (2022).\n\n\n\nM. Fishman, S. R. White and E. M. Stoudenmire. Codebase release 0.3 for ITensor. SciPost Phys. Codebases, 4-r0.3 (2022).\n\n\n\nE. M. Stoudenmire and D. J. Schwab. Supervised Learning with Quantum-Inspired Tensor Networks (2017), arXiv:1605.05775 [stat.ML].\n\n\n\nB. D. Fulcher and N. S. Jones, hctsa: A Computational Framework for Automated Time-Series Phenotyping Using Massive Feature Extraction. Cell Systems 5, 527-531.e3 (2017).\n\n\n\n","category":"page"},{"location":"synthdatagen/#Synthetic-Data-Generation","page":"Synthetic Data Generation","title":"Synthetic Data Generation","text":"","category":"section"},{"location":"encodings/#Encodings","page":"Encodings","title":"Encodings","text":"","category":"section"},{"location":"encodings/#Overview","page":"Encodings","title":"Overview","text":"","category":"section"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"To use MPS methods on time-series data, the continuous time-series amplitudes must be mapped to MPS compatible vectors using an encoding. There are a number of encodings built into this library, and they can be specified by the encoding keyword in MPSOptions.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Encoding","category":"page"},{"location":"encodings/#MPSTime.Encoding","page":"Encodings","title":"MPSTime.Encoding","text":"Encoding\n\nAbstract supertype of all encodings. To specify an encoding for MPS training, set the encoding keyword when calling MPSOptions.\n\nExample\n\njulia> opts = MPSOptions(; encoding=:Legendre);\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\n\nEncodings\n\n:Legendre: The first d L2-normalised Legendre Polynomials. Real valued, and supports passing projected_basis=true to MPSOptions.\n:Fourier: Complex valued Fourier coefficients. Supports passing projected_basis=true to MPSOptions.\n\n    Phi(x d) = left1 + 0i e^i pi x e^-i pi x e^2i pi x e^-2i pi x ldots right  sqrtd \n\n:Stoudenmire: The original complex valued \"Spin-1/2\" encoding from Stoudenmire & Schwab, 2017 arXiv. Only supports d = 2\n\n    Phi(x) = left e^3 i pi x  2 cos(fracpi2 x)  e^-3 i pi x  2 sin(fracpi2 x)right\n\n:Sahand_Legendre_Time_Dependent:  (:SLTD) A custom, real-valued encoding constructed as a data-driven adaptation of the Legendre Polynomials. At each time point, t, the training data is used to construct a probability density function that describes the distribution of the time-series amplitude x_t. This is the first basis function. \nb_1(x t) = textpdf_x_t(x_t). This is computed with KernelDensity.jl:\n\njulia> Using KernelDensity\njulia> xs_samps = range(-1,1, max(200,size(X_train,2)))\njulia> b1(xs,t) = pdf(kde(X_train[t,:]), xs_samps)\n\nThe second basis function is the first order polynomial that is L2-orthogonal to this pdf on the interval [-1,1]. \n\nb_2(xt) = a_1 x + a_0 text where  int_-1^1 b_1(xt) b_2^*(x t) textrmd x = 0  lvertlvert b_2(x t) rvertrvert_L2 = 1\n\nThe third basis function is the second order polynomial that is L2-orthogonal to the first two basis functions on [-1,1], etc.\n\n-:Custom: For use with user-defined custom bases. See function_basis\n\n\n\n\n\n","category":"type"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Encodings can be visualsed with the plot_encoding function.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(:legendre, 4)","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"For data driven bases, the data histograms can be plotted alongside for reference:","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(:sahand_legendre_time_dependent, 4, X_train; tis=[1,20]); # X_train is taken from the noisy trendy sine demo in the Imputation section","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/#Using-a-SplitBasis-encoding","page":"Encodings","title":"Using a SplitBasis encoding","text":"","category":"section"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"One way to increase the encoding dimension is to repeat a basis many times across the encoding domain in 'bins'. In theory, this can be advantageous when data is concentrated in narrow regions in the encoding domain, as very fine bins can be used to reduce encoding error in well-populated regions, while computational effort can be saved with wide bins in sparsely-population regions. To this end, we provide the \"Split\" bases.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"The uniform-split encoding, which simply bins data up as a proof of concept:","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(uniform_split(:legendre), 8, X_train; tis=[1,20], aux_basis_dim=4);","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"And the histogram-split encoding, which narrows the bins in frequently occuring regions.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(histogram_split(:legendre), 8, X_train; tis=[1,20], aux_basis_dim=4);","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Every data-independent encoding can be histogram split and uniform split, including other split bases:","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(histogram_split(uniform_split(:legendre)), 16, X_train; tis=[1,20], aux_basis_dim=8, size=(1600,900));","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/#Custom-encodings","page":"Encodings","title":"Custom encodings","text":"","category":"section"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Custom encodings can be declared using function_basis.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"function_basis","category":"page"},{"location":"encodings/#MPSTime.function_basis","page":"Encodings","title":"MPSTime.function_basis","text":"function_basis(basis::Function, is_complex::Bool, range::Tuple{<:Real,<:Real}, <args>; name::String=\"Custom\")\n\nConstructs a time-(in)dependent encoding from the function basis, which is either real or complex, and has support on the interval range.\n\nFor a time independent basis, the input function must have the signature :\n\nbasis(x::Float64, d::Integer, init_args...)\n\nand return a d-dimensional numerical Vector.  A vector x_1 x_2 x_3  x_N will be encoded as b(x_1) b(x_2) b(x_3) b(x_N)\n\nTo use a time dependent basis, set is_time_dependent to true. The input function must have the signature \n\nbasis(x::Float64, d::Integer, ti::Int, init_args...)\n\nand return a d-dimensional numerical Vector. A vector x_1 x_2 x_3  x_N will be encoded as  b_1(x_1) b_2(x_2) b_3(x_3) b_N(x_N)\n\nOptional Arguments\n\nis_time_dependent::Bool=false: Whether the basis is time dependent \nis_data_driven::Bool=false: Whether functional form of the basis depends on the training data\ninit::Function=no_init: The initialiser function for the basis. This is used to compute arguments for the function that are not known in advance,\n\nfor example, the polynomial coefficients for the Sahand-Legendre basis. This function should have the form\n\ninit_args = opts.encoding.init(X_normalised::AbstractMatrix, y::AbstractVector; opts::MPSTime.Options=opts)\n\nX_normalised will be preprocessed (with sigmoid transform and MinMax transform pre-applied), with Time series as columns\n\nExample\n\nThe Legendre Polynomial Basis:\n\njulia> Using LegendrePolynomials\njulia> function legendre_encode(x::Float64, d::Int)\n    # default legendre encoding: choose the first n-1 legendre polynomials\n\n    leg_basis = [Pl(x, i; norm = Val(:normalized)) for i in 0:(d-1)] \n    \n    return leg_basis\njulia> custom_basis = function_basis(legendre_encode, false, (-1., 1.))\n\n\n\n\n\n","category":"function"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"To use a custom encoding, you must manually pass it into fitMPS.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"encoding = function_basis(...)\nfitMPS(X_train, y_train, X_test, y_test, MPSOptions(; encoding=:Custom), encoding)","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"plot_encoding(::Symbol, ::Integer)","category":"page"},{"location":"encodings/#MPSTime.plot_encoding-Tuple{Symbol, Integer}","page":"Encodings","title":"MPSTime.plot_encoding","text":"plot_encoding(E::Union(Symbol, MPSTime.Encoding), \n              d::Integer, \n              X_train::Matrix{Float64}=zeros(0,0), \n              y_train::Vector{Any}=[];\n              <keyword arguments>) -> encoding::Vector, plot::Plots.Plot\n\nPlot the first d terms of the encoding E across its entire domain.\n\nX_train and y_train are only needed if E is data driven or time dependent, or plot_hist is true.\n\nKeyword Arguments\n\nplot_hist::Bool=E.isdatadriven: Whether to plot the histogram of the traing data at several time points. Useful for understanding the behviour of data-driven bases.\ntis::Vector{<:Integer} = Int[]: Time(s) to plot the Encoding at.\nds::Vector{<:Integer} = collect(1:d): Enables plotting of a subset of a d-dimensional Encoding, e.g. ds=[1,3,5] plots the first, third and fifth basis functions.\nnum_xvals::Integer=500: \nsize::Tuple=(1200, 800): \npadding::Real=6.: \n\nUsed for data-driven Encodings\n\nsigmoid_transform::Bool=false: Whether to apply a robust sigmoid transform to the training data, see MPSOptions\nminmax::Bool=true: Whether to apply a minmax normalsation to the training data after the sigmoid, see MPSOptions\ndata_bounds::Tuple{<:Real, <:Real}=(0.,1.): Whether to apply a robust sigmoid transform to the X data, see MPSOptions\nproject_basis::Bool=false: Whether to project the basis onto the data. Supported only by :legendre and :Fourier basis when E has type Symbol\naux_basis_dim::Integer=2: Dimension of each auxilliary basis. Only relevent when E is a MPSTime.SplitBasis. \n\nMisc\n\nkwargs: Passed to Plots.Plot()\n\n\n\n\n\n","category":"method"},{"location":"imputation/#Imputation_top","page":"Imputation","title":"Imputation","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"MPSTime supports univriate time-series imputation with three key missingness patterns:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Individual missing points (e.g., values missing at t = 10 20 80)\nSingle contiguous blocks (e.g., values missing from t = 25-70)\nMultiple contiguous blocks (e.g., values missing from t = 5-10, t = 25-50 and t = 80-90)","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"MPSTime can also handle any combination of these patterns. For instance, you might need to impute a single contiguous block from t = 10-30, plus individual missing points at t = 50 and t=80.","category":"page"},{"location":"imputation/#Setup","page":"Imputation","title":"Setup","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"The first step is to train an MPS (see Tutorial).  Here, we'll train an MPS in an unsupervised manner (no class labels) using a noisy trendy sinusoid.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"# Fix rng seed\nusing Random\nrng = Xoshiro(1)\n\n# dataset size\nntimepoints = 100\nntrain_instances = 300\nntest_instances = 200\n\n# generate the train and test datasets\nX_train = trendy_sine(ntimepoints, ntrain_instances, 0.1, rng);\nX_test = trendy_sine(ntimepoints, ntest_instances , 0.1, rng);\n\n# hyper parameters and training\nopts = MPSOptions(d=10, chi_max=40, sigmoid_transform=false);\nmps, info, test_states= fitMPS(X_train, opts);","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Next, we initialize an imputation problem. This does a lot of necessary precomputation:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"julia> imp = init_imputation_problem(mps, X_test);\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n                         Summary:\n\n - Dataset has 300 training samples and 200 testing samples.\nSlicing MPS into individual states...\n - 1 class(es) were detected.\n - Time independent encoding - Legendre - detected.\n - d = 10, chi_max = 40\nRe-encoding the training data to get the encoding arguments...\n\n Created 1 ImputationProblem struct(s) containing class-wise mps and test samples.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"A summary of the imputation problem setup is printed to verify the model parameters and dataset information. For multi-class data, you can pass y_test to init_imputation_problem in order to exploit the labels / class information while doing imputation.","category":"page"},{"location":"imputation/#Imputing-missing-values","page":"Imputation","title":"Imputing missing values","text":"","category":"section"},{"location":"imputation/#Single-block-imputation","page":"Imputation","title":"Single-block imputation","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Now, decide how you want to impute the missing data. The necessary options are:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"class::Integer: The class of the time-series instance we are going to impute, leave as zero for \"unlabelled\" data (i.e., all data belong to the same class).\nimpute_sites: The MPS sites (time points) that are missing (inclusive).\ninstance_idx: The time-series instance from the chosen class in the test set.\nmethod: The imputation method to use. Can be trajectories (ITS), median, mode, mean, etc...","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"In this example, we will consider a single block of contiguous missing values, from t = 10 through to t = 90 inclusive (i.e., 81% missing data). We will use the median to impute the missing values, as well as computing a 1-Nearest Neighbor Imputation (1-NNI) baseline for comparison:   ","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"class = 0\nimpute_sites = collect(10:90) # impute time pts. 10-90 inclusive\ninstance_idx = 59 # time series instance in test set\nmethod = :median\n\nimputed_ts, pred_err, target_ts, stats, plots = MPS_impute(\n    imp,\n    class, \n    instance_idx, \n    impute_sites, \n    method; \n    NN_baseline=true, # whether to also do a baseline imputation using 1-NNI\n    plot_fits=true, # whether to plot the fits\n)","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Several outputs are returned from MPS_impute:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"imputed_ts: The imputed time-series instance, containing the original data points and the predicted values.\npred_err: The prediction error for each imputed value, given a known ground-truth.\ntarget_ts: The original time-series instance containing missing values.\nstats: A collection of statistical metrics (e.g., MAE) evaluating imputation performance with respect to a ground truth. Includes baseline performance when NN_baseline=true.\nplots: Stores plot object(s) in an array for visualization when plot_fits=true.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"We can inspect the imputation performance in a summary table:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"julia> using PrettyTables\njulia> pretty_table(stats[1]; header=[\"Metric\", \"Value\"], header_crayon= crayon\"yellow bold\", tf = tf_unicode_rounded);\n╭────────┬───────────╮\n│ Metric │     Value │\n├────────┼───────────┤\n│    MAE │ 0.0817192 │\n│ NN_MAE │  0.127104 │\n╰────────┴───────────╯","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"To plot the imputed time series, we can call the plot function as follows: ","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"julia> using Plots\njulia> plot(plots...)","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: ) The solid orange line depicts the \"ground-truth\" (observed) time-series values, the dotted blue line is the MPS-imputed data points and the dotted red line is the 1-NNI baseline. The blue shading indicates the uncertainty due to encoding error.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"There are a lot of other options, and many more impution methods to choose from! See MPS_impute for more details.","category":"page"},{"location":"imputation/#Multi-block-imputation","page":"Imputation","title":"Multi-block imputation","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Building on the previous example of single-block imputation, MPSTime can also be used to impute missing values in multiple blocks of contiguous points.  For example, consider missing points between t = 10-25, t = 40-60 and t = 75-90:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"class = 0\nimpute_sites = vcat(collect(10:25), collect(40:60), collect(65:90))\ninstance_idx = 32\nmethod = :median\n\nimputed_ts, pred_err, target_ts, stats, plots = MPS_impute(\n    imp,\n    class, \n    instance_idx, \n    impute_sites, \n    method; \n    NN_baseline=true, # whether to also do a baseline imputation using 1-NNI\n    plot_fits=true, # whether to plot the fits\n)","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/#Plotting-Trajectories","page":"Imputation","title":"Plotting Trajectories","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"To plot trajectories, use method=:ITS. Here, we'll plot 10 randomly selected trajectories by setting the num_trajectories keyword. ","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"class = 0\nimpute_sites = collect(10:90)\ninstance_idx = 59\nmethod = :ITS\n\nimputed_ts, pred_err, target_ts, stats, plots = MPS_impute(\n    imp,\n    class, \n    instance_idx, \n    impute_sites, \n    method; \n    NN_baseline=false, # whether to also do a baseline imputation using 1-NN\n    plot_fits=true, # whether to plot the fits\n    num_trajectories=10, # number of trajectories to plot\n    rejection_threshold=2.5 # limits how unlikely we allow the random trajectories to be.\n    # there are more options! see [`MPS_impute`](@ref)\n)\n\nplot(plots...)","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/#Plotting-cumulative-distribution-functions","page":"Imputation","title":"Plotting cumulative distribution functions","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"It can be interesting to inspect the probability distribution being sampled from at each missing time point. To enable this, we provide the get_cdfs function, which works very similarlary to MPS_impute, only it returns the CDF at each missing time point.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"cdfs, ts, pred_err, target = get_cdfs(\n    imp, \n    class, \n    instance_idx, \n    impute_sites\n    );\n\nxvals = imp.x_guess_range.xvals[1:10:end]\n\nplot(xvals, cdfs[1][1:10:end]; legend=:none)\np = last([plot!(xvals, cdfs[i][1:10:end]) for i in eachindex(cdfs)])\nylabel!(\"cdf(x)\")\nxlabel!(\"x_t\")\ntitle!(\"CDF at each time point.\")","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/#Docstrings","page":"Imputation","title":"Docstrings","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"init_imputation_problem(::TrainedMPS, ::Matrix)\nMPS_impute\nget_cdfs","category":"page"},{"location":"imputation/#MPSTime.init_imputation_problem-Tuple{TrainedMPS, Matrix}","page":"Imputation","title":"MPSTime.init_imputation_problem","text":"init_imputation_problem(W::TrainedMPS, X_test::AbstractMatrix, y_test::AbstractArray=zeros(Int, size(X_test,1)); <keyword arguments>) -> imp::ImputationProblem\n\nInitialise an imputation problem using a trained MPS and relevent test data.\n\nThis involves a lot of pre-computation, which can be quite time intensive for data-driven bases. For unclassed/unsupervised data y_test may be omitted.\n\nKeyword Arguments\n\nguess_range::Union{Nothing, Tuple{<:Real,<:Real}}=nothing: The range of values that guesses are allowed to take. This range is applied to normalised, encoding-adjusted time-series data. To allow any guess, leave as nothing, or set to encoding.range (e.g. [(-1., 1.) for the legendre encoding]).\ndx::Float64 = 1e-4: The spacing between possible guesses in normalised, encoding-adjusted units. When imputing missing data with an MPS method, the imputed values will be selected from    range(guess_range...; step=dx)\nverbosity::Integer=1: The verbosity of the initialisation process. Useful for debugging, or to completely suppress output.\n\n\n\n\n\n","category":"method"},{"location":"imputation/#MPSTime.MPS_impute","page":"Imputation","title":"MPSTime.MPS_impute","text":"MPS_impute(imp::ImputationProblem, \n           class::Any, \n           instance::Integer, \n           missing_sites::AbstractVector{<:Integer}, \n           method::Symbol=:median; \n           <keyword arguments>) -> (imputed_instance::Vector, errors::Vector, target::Vector, stats::Dict, p::Vector{Plots.Plot})\n\nImpute the missing_sites using an MPS-based approach, selecting the trajectory from the conditioned distribution with method\n\nSee init_imputation_problem for constructing an ImputationProbleminstance out of a trained MPS. Theinstance` number is relative to the class, so class 1, instance 2 would be distinct from class 2, instance 2. \n\nImputation Methods\n\n:median: For each missing value, compute the probability density function of the possible outcomes from the MPS, and choose the median. This method is the most robust to outliers. Keywords:\nget_wmad::Bool=true: Whether to return an 'error' vector that computes the Weighted Median Absolute Deviation (WMAD) of each imputed value.\n:mean: For each missing value, compute the probability density function of the possible outcomes from the MPS, and choose the expected value. Keywords:\nget_std::Bool=true: Whether to return an 'error' vector that computes standard deviation of each imputed value.\n:mode: For each missing value, choose the most likely outcome predicted by the MPS. Keywords:\nmax_jump::Union{Number,Nothing}=nothing: The largest jump allowed between two adjacent imputations. Leave as nothing to allow any jump. Helpful to suppress 'spikes' caused by poor support near the encoding domain edges.\n:ITS: For each missing value, choose a value at random with probability weighted by the probability density function of the possible outcomes. Keywords:\nrseed::Integer=1: Random seed for producing the trajectories.\n`num_trajectories::Integer=1: Number of trajectories to compute.\nrejection_threshold::Union{Float64, Symbol}=:none: Number of WMADs allowed between adjacent points. Setting this low helps suppress rapidly varying trajectories that occur by bad luck. \nmax_trials::Integer=10: Number of attempts allowed to make guesses conform to rejection_threshold before giving up.\n:kNearestNeighbour: Select the k nearest neighbours in the training set using Euclidean distance to the known data. Keyword:\nk: Number of nearest neighbours to return. See kNN_impute\n\nKeyword Arguments\n\nimpute_order::Symbol=:forwards: Whether to impute the missing values :forwards (left to right) or :backwards (right to left)\nNN_baseline::Bool=true: Whether to also impute the missing data with a k-Nearest Neighbour baseline.\nn_baselines::Integer=1: How many nearest neighbour baselines to compute.\nplot_fits::Bool=true: Whether to make a plot showing the target timeseries, the missing values, and the imputed region. If false, then p will be an empty vector. The plot will show the NN_baseline (if it was computed), as well as every trajectory if using the :ITS method.\nget_metrics::Bool=true: Whether to compute imputation metrics, if false, then stats, will be empty.\nfull_metrics::Bool=false: Whether to compute every metric (MAPE, SMAPE, MAE, MSE, RMSE) or just MAE\nprint_metric_table::Bool=false: Whether to print the stats as a table.\ninvert_transform::Bool=true:, # Whether to undo the sigmoid transform/minmax normalisation before returning the imputed points. If this is false, imputed_instance, errors, target timeseries, stats, and plot y-axis will all be scaled by the data preprocessing / normalisation and fit to the encoding domain.\nkwargs...: Extra keywords passed to the imputation method. See the Imputation Methods section.\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.get_cdfs","page":"Imputation","title":"MPSTime.get_cdfs","text":"get_cdfs(imp::ImputationProblem, \n           class::Any, \n           instance::Integer, \n           missing_sites::AbstractVector{<:Integer}, \n           method::Symbol=:median; \n           <keyword arguments>) -> (cdfs::Vector{Vector}, ts::Vector, pred_err::Vector, target_timeseries_full::Vector)\n\nImpute the missing_sites using an MPS-based approach, selecting the trajectory from the conditioned distribution with method, and returns the cumulative distribution function used to infer each missing value. \n\nSee MPS_impute for a list of imputation methods and keyword arguments (does not support plotting, stats, or kNN baselines). See init_imputation_problem for constructing an ImputationProblem instance. The instance number is relative to the class, so class 1, instance 2 would be distinct from class 2, instance 2.\n\n\n\n\n\n","category":"function"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Internal imputation methods:","category":"page"},{"location":"imputation/#Internal-imputation-methods","page":"Imputation","title":"Internal imputation methods","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"MPSTime.impute_median\nMPSTime.impute_ITS\nMPSTime.kNN_impute","category":"page"},{"location":"imputation/#MPSTime.impute_median","page":"Imputation","title":"MPSTime.impute_median","text":"impute missing data points using the median of the conditional distribution (single site rdm ρ).\n\nArguments\n\nclass_mps::MPS: \nopts::Options: MPS parameters.\nenc_args::AbstractVector\nx_guess_range::EncodedDataRange\ntimeseries::AbstractVector{<:Number}: The input time series data that will be imputed.\ntimeseries_enc::MPS: The encoded version of the time series represented as a product state. \nimputation_sites::Vector{Int}: Indices in the time series where imputation is to be performed.\nget_wmad::Bool: Whether to compute the weighted median absolute deviation (WMAD) during imputation (default is false).\n\nReturns\n\nA tuple containing:\n\nmedian_values::Vector{Float64}: The imputed median values at the specified imputation sites.\nwmad_value::Union{Nothing, Float64}: The weighted median absolute deviation if get_wmad is true; otherwise, nothing.\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.impute_ITS","page":"Imputation","title":"MPSTime.impute_ITS","text":"Impute a SINGLE trajectory using inverse transform sampling (ITS).\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.kNN_impute","page":"Imputation","title":"MPSTime.kNN_impute","text":"kNN_impute(imp::ImputationProblem, \n           class::Any, instance::Integer, \n           missing_sites::AbstractVector{<:Integer}; \n           k::Integer=1) -> [neighbour1::Vector, neighbour2::Vector, ...]\n\nImpute missing_sites using the k nearest neighbours in the test set, based on Euclidean distance.\n\nSee init_imputation_problem for constructing an ImputationProblem instance. The instance number is relative to the class, so class 1, instance 2 would be distinct from class 2, instance 2.\n\n\n\n\n\n","category":"function"},{"location":"#MPSTime.jl","page":"Introduction","title":"MPSTime.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"A Julia package for time-series machine learning (ML) using Matrix-Product States (MPS) built on the ITensors.jl framework [1, 2].","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"#Overview","page":"Introduction","title":"Overview","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"MPSTime is a Julia package for learning the joint probability distribution of time series directly from data using matrix product state (MPS) methods inspired by quantum many-body physics.  It provides a unified formalism for:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Time-series classification (inferring the class of unseen time-series).\nUnivariate time-series imputation across fixed-length time series.\nSynthetic data generation (coming soon).","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Info\nMPSTime is currently under active development. Many features are in an experimental stage and may undergo significant changes, refinements, or removals.","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This is not yet a registered Julia package, but it will be soon (TM)! In the meantime, you can install it directly from github:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> ]\npkg> add https://github.com/jmoo2880/MPSTime.jl.git","category":"page"},{"location":"#Usage","page":"Introduction","title":"Usage","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"See the Tutorial section and other sidebars for a basic usage examples. We're continually adding more features and documentation as we go.","category":"page"},{"location":"#Citation","page":"Introduction","title":"Citation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"If you use MPSTime in your research, please cite the MPSTime Paper:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Coming soon (TM).","category":"page"},{"location":"docstrings/#Docstrings","page":"Docstrings","title":"Docstrings","text":"","category":"section"},{"location":"docstrings/#Useful-Structs","page":"Docstrings","title":"Useful Structs","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"MPSTime.Encoding","category":"page"},{"location":"docstrings/#MPSTime.Encoding-docstrings","page":"Docstrings","title":"MPSTime.Encoding","text":"Encoding\n\nAbstract supertype of all encodings. To specify an encoding for MPS training, set the encoding keyword when calling MPSOptions.\n\nExample\n\njulia> opts = MPSOptions(; encoding=:Legendre);\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\n\nEncodings\n\n:Legendre: The first d L2-normalised Legendre Polynomials. Real valued, and supports passing projected_basis=true to MPSOptions.\n:Fourier: Complex valued Fourier coefficients. Supports passing projected_basis=true to MPSOptions.\n\n    Phi(x d) = left1 + 0i e^i pi x e^-i pi x e^2i pi x e^-2i pi x ldots right  sqrtd \n\n:Stoudenmire: The original complex valued \"Spin-1/2\" encoding from Stoudenmire & Schwab, 2017 arXiv. Only supports d = 2\n\n    Phi(x) = left e^3 i pi x  2 cos(fracpi2 x)  e^-3 i pi x  2 sin(fracpi2 x)right\n\n:Sahand_Legendre_Time_Dependent:  (:SLTD) A custom, real-valued encoding constructed as a data-driven adaptation of the Legendre Polynomials. At each time point, t, the training data is used to construct a probability density function that describes the distribution of the time-series amplitude x_t. This is the first basis function. \nb_1(x t) = textpdf_x_t(x_t). This is computed with KernelDensity.jl:\n\njulia> Using KernelDensity\njulia> xs_samps = range(-1,1, max(200,size(X_train,2)))\njulia> b1(xs,t) = pdf(kde(X_train[t,:]), xs_samps)\n\nThe second basis function is the first order polynomial that is L2-orthogonal to this pdf on the interval [-1,1]. \n\nb_2(xt) = a_1 x + a_0 text where  int_-1^1 b_1(xt) b_2^*(x t) textrmd x = 0  lvertlvert b_2(x t) rvertrvert_L2 = 1\n\nThe third basis function is the second order polynomial that is L2-orthogonal to the first two basis functions on [-1,1], etc.\n\n-:Custom: For use with user-defined custom bases. See function_basis\n\n\n\n\n\n","category":"type"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"MPSTime.EncodedTimeSeriesSet\nTrainedMPS","category":"page"},{"location":"docstrings/#MPSTime.EncodedTimeSeriesSet","page":"Docstrings","title":"MPSTime.EncodedTimeSeriesSet","text":"EncodedTimeSeriesSet\n\nHolds an encoded time-series dataset, as well as a copy of the original data and its class distribution.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MPSTime.TrainedMPS","page":"Docstrings","title":"MPSTime.TrainedMPS","text":"TrainedMPS\n\nContainer for a trained MPS and its associated Options and training data.\n\nFields\n\nmps::MPS: A trained Matrix Product state.\nopts::MPSOptions: User defined MPSOptions used to create the MPS.\nopts_concrete::MPSTime.Options: Internal Options struct. Necessary to preserve custom encodings passed to fitMPS.\ntrain_data::EncodedTimeSeriesSet: Stores both the raw and encoded data used to train the mps.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#Hyperparameters","page":"Docstrings","title":"Hyperparameters","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"MPSTime.AbstractMPSOptions","category":"page"},{"location":"docstrings/#MPSTime.AbstractMPSOptions","page":"Docstrings","title":"MPSTime.AbstractMPSOptions","text":"AbstractMPSOptions\n\nAbstract supertype of \"MPSOptions\", a collection of concrete types which is used to specify options for training, and \"Options\", which is used internally and contains references to internal objects\n\n\n\n\n\n","category":"type"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"MPSTime.MPSOptions","category":"page"},{"location":"docstrings/#MPSTime.MPSOptions-docstrings","page":"Docstrings","title":"MPSTime.MPSOptions","text":"MPSOptions(; <Keyword Arguments>)\n\nSet the hyperparameters and other options for fitMPS. \n\nFields:\n\nLogging\n\nverbosity::Int=1: How much debug/progress info to print to the terminal while optimising the MPS. Higher numbers mean more output\nlog_level::Int=3: How much statistical output. 0 for nothing, >0 to print losses, accuracies, and confusion matrix at each step (noticeable) computational overhead) #TODO implement finer grain control\ntrack_cost::Bool=false: Whether to print the cost at each Bond tensor site to the terminal while training, mostly useful for debugging new cost functions or optimisers (HUGE computational overhead)\n\nMPS Training Hyperparameters\n\nnsweeps::Int=5: Number of MPS optimisation sweeps to perform (Both forwards and Backwards)\nchi_max::Int=25: Maximum bond dimension allowed within the MPS during the SVD step\neta::Float64=0.01: The learning rate. For gradient descent methods, this is the step size. For Optim and OptimKit this serves as the initial step size guess input into the linesearch\nd::Int=5: The dimension of the feature map or \"Encoding\". This is the true maximum dimension of the feature vectors. For a splitting encoding, d = numsplits * auxbasis_dim\ncutoff::Float64=1E-10: Size based cutoff for the number of singular values in the SVD (See Itensors SVD documentation)\ndtype::DataType=Float64 or ComplexF64 depending on encoding: The datatype of the elements of the MPS. Supports the arbitrary precsion types such as BigFloat and Complex{BigFloat}\nexit_early::Bool=false: Stops training if training accuracy is 1 at the end of any sweep.\n\nEncoding Options\n\nencoding::Symbol=:Legendre: The encoding to use, including :Stoudenmire, :Fourier, :Legendre, :SLTD, :Custom, etc. see Encoding docs for a complete list. Can be just a time (in)dependent orthonormal basis, or a time (in)dependent basis mapped onto a number of \"splits\" which distribute tighter basis functions where the sites of a timeseries are more likely to be measured.  \nprojected_basis::Bool=false: Whether toproject a basis onto the training data at each time. Normally, when specifying a basis of dimension d, the first d lowest order terms are used. When project=true, the training data is used to construct a pdf of the possible timeseries amplitudes at each time point. The first d largest terms of this pdf expanded in a series are used to select the basis terms.\naux_basis_dim::Int=2: Unused for standard encodings. If the encoding is a SplitBasis, serves as the auxilliary dimension of a basis mapped onto the split encoding, so that the number of histogram bins = d / auxbasisdim. \nencode_classes_separately::Bool=false: Only relevant for data driven bases. If true, then data is split up by class before being encoded. Functionally, this causes the encoding method to vary depending on the class\n\nData Preprocessing and MPS initialisation\n\nsigmoid_transform::Bool: Whether to apply a sigmoid transform to the data before minmaxing. This has the form\n\nboldsymbolX = left(1 + exp-fracboldsymbolX-m_boldsymbolXr_boldsymbolX  135right)^-1\n\nwhere boldsymbolX is the un-normalized time-series data matrix, m_boldsymbolX is the median of boldsymbolX and r_boldsymbolXis its interquartile range.\n\nminmax::Bool: Whether to apply a minmax norm to [0,1] before encoding. This has the form\n\nboldsymbolX =  fracboldsymbolX - x_textminx_textmax - x_textmin\n\nwhere boldsymbolX is the scaled robust-sigmoid transformed data matrix, x_textmin and x_textmax are the minimum and maximum of boldsymbolX.\n\ndata_bounds::Tuple{Float64, Float64} = (0.,1.): The region to bound the data to if minmax=true. This is separate from the encoding domain. All encodings expect data to be scaled scaled between 0 and 1. Setting the data bounds a bit away from [0,1] can help when your basis has poor support near its boundaries.\ninit_rng::Int: Random seed used to generate the initial MPS\nchi_init::Int: Initial bond dimension of the random MPS\n\nLoss Functions and Optimisation Methods\n\nloss_grad::Symbol=:KLD: The type of cost function to use for training the MPS, typically Mean Squared Error (:MSE) or KL Divergence (:MSE), but can also be a weighted sum of the two (:Mixed)\nbbopt::Symbol=:TSGO: Which local Optimiser to use, builtin options are symbol gradient descent (:GD), or gradient descent with a TSGO rule (:TSGO). Other options are Conjugate Gradient descent using either the Optim or OptimKit packages (:Optim or :OptimKit respectively). The CGD methods work well for MSE based loss functions, but seem to perform poorly for KLD base loss functions.\nrescale::Tuple{Bool,Bool}=(false,true): Has the form rescale = (before::Bool, after::Bool). Where to enforce the normalisation of the MPS during training, either calling normalise!(Bond Tensor) before or after BT is updated. Note that for an MPS that starts in canonical form, rescale = (true,true) will train identically to rescale = (false, true) but may be less performant.\nupdate_iters::Int=1: Maximum number of optimiser iterations to perform for each bond tensor optimisation. E.G. The number of steps of (Conjugate) Gradient Descent used by TSGO, Optim or OptimKit\ntrain_classes_separately::Bool=false: Whether the the trainer optimises the total MPS loss over all classes or whether it considers each class as a separate problem. Should make very little diffence\n\nDebug\n\nreturn_encoding_meta_info::Bool=false: Debug flag: Whether to return the normalised data as well as the histogram bins for the splitbasis types\n\n\n\n\n\nConvert the internal Options type into a serialisable MPSOptions.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"The internal Options type and the functions that help convert between","category":"page"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"MPSTime.Options\nMPSTime.safe_options\nMPSTime.model_encoding\nMPSTime.symbolic_encoding\nMPSTime.model_loss_func\nMPSTime.model_bbopt","category":"page"},{"location":"docstrings/#MPSTime.Options","page":"Docstrings","title":"MPSTime.Options","text":"Options(; <Keyword Arguments>)\n\nThe internal options struct. Fields have the same meaning as MPSOptions, but contains objects instead of symbols, e.g. Encoding=Basis(\"Legendre\") instead of :Legendre\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MPSTime.safe_options","page":"Docstrings","title":"MPSTime.safe_options","text":"safe_options(opts::AbstractMPSOptions)\n\nTakes any AbstractMPSOptions type, and returns an instantiated Options type.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.model_encoding","page":"Docstrings","title":"MPSTime.model_encoding","text":"model_encoding(symb::Symbol, project::Bool=false)\n\nConstruct an Encoding object from symb. Not case sensitive. See Encodings documentation for the full list of options. Will use the specified project options if the encoding supports projecting. The inverse of symbolic_encoding.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.symbolic_encoding","page":"Docstrings","title":"MPSTime.symbolic_encoding","text":"symbolic_encoding(E::Encoding)\n\nConstruct a symbolic name from an Encoding object. The inverse of model_encoding\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.model_loss_func","page":"Docstrings","title":"MPSTime.model_loss_func","text":"model_loss_func(symb::Symbol)\n\nSelect a loss function (::Function) from the symb. Not case sensitive. The inverse of symboliclossfunc\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.model_bbopt","page":"Docstrings","title":"MPSTime.model_bbopt","text":"model_bbopt(symb::Symbol)\n\nConstuct a BBOpt object from symb. Not case sensitive.\n\n\n\n\n\n","category":"function"}]
}
