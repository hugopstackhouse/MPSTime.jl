var documenterSearchIndex = {"docs":
[{"location":"tools/#Tools","page":"Tools","title":"Tools","text":"","category":"section"},{"location":"tools/#Entanglement-Entropy","page":"Tools","title":"Entanglement Entropy","text":"","category":"section"},{"location":"tools/#Overview","page":"Tools","title":"Overview","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"In quantum many-body physics, the entanglement entropy (EE) determines the extent to which two partitions of the collective quantum system are entangled. More simply, the EE can be thought of as quantifying the information shared between subsystem A and subsystem B within a many-body system. In practice, the EE is computed as the von Neumman entropy of the reduced density matrix for any of the two subsystems (A or B).  An EE of zero implies that there is no entanglement between the subsystems.","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"We provide functions for two types of EE: (i) single-site entanglement entropy (SEE), and (ii) bipartite entanglement entropy (BEE):","category":"page"},{"location":"tools/#(1)-Single-site-entanglement-entropy-(SEE)","page":"Tools","title":"(1) Single-site entanglement entropy (SEE)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"The single-site entanglement entropy (SEE) quantifies the degree of entanglement between a single site (time-point) in the MPS and all other sites (time points). Given a particular site in the MPS, i, the SEE is then specified by the von Neumann entropy of the reduced density matrix [3, 4]:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"S_textrmSEE = -mathrmtr rho_i log rho_i","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"where rho_i is the reduced density matrix (rdm) at site i, obtained by tracing over all sites except for the i-th site:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"rho_i = Tr_i ketpsibrapsi","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"and ketpsi is the MPS. Using the 1D spin-chain as an illustrative example, the SEE between a single site (dark blue) and the rest of the system (light blue) can be depicted as:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#(2)-Bipartite-entanglement-entropy-(BEE)","page":"Tools","title":"(2) Bipartite entanglement entropy (BEE)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"The bipartite entanglement entropy (BEE) quantifies the quantum entanglement between two complementary subsystems of a matrix product state (MPS).  For an MPS with N sites, we can create a bipartition by splitting the system at any bond l, resulting in region A (sites 1 to l) and region B (sites l+1 to N). The BEE can be expressed using the singular values of the Shmidt decomposition of either of the two subsystems:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"ketpsi = sum_i alpha_i ketu_i_A\notimes ketv_i_B","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"where alpha_i are the Schmidt coefficients (singular values) satisfying sum_i alpha_i^2 = 1, ketu_i_A and ketv_i_B are orthonormal states in subsystem A and B, respectively. The BEE is then given by the von Neumann entropy [4]:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"S_textrmBEE = -sum_i alpha_i^2 log alpha_i^2","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"The BEE can be represented schematically using the 1D spin chain analogy where the red dotted line denotes the bipartition, the light blue particles represent subsystem A and the dark blue represent subsystem B:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Bipartite-Entanglement-Entropy-(BEE)","page":"Tools","title":"Bipartite Entanglement Entropy (BEE)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"Given a trained MPS (for either classification or imputation), we can compute the bipartite entanglement entropy (BEE) using the bipartite_spectrum function:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# train the MPS as usual\nmps, _, _ = fitMPS(...);\nbees = bipartite_spectrum(mps);","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"A vector is returned where each entry contains the BEE spectrum for the class-specific MPS.  For example, in the case of a two class problem, we obtain the individual BEE spectrums for the class 0 MPS and the class 1 MPS.  For an unsupervised problem with only a single class, there is only a single BEE spectrum. ","category":"page"},{"location":"tools/#Example","page":"Tools","title":"Example","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"To illustrate how we might use the BEE in a typical analysis, consider an example involving real world time series from the ItalyPowerDemand (IPD) UCR dataset.  There are two classes corresponding to the power demand during: (i) the winter months; (ii) the summer months.  For this example, we will train an MPS to classify between summer and winter time-series data:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# load in the training data\nusing JLD2\nipd_load = jldopen(\"ipd_original.jld2\", \"r\");\n    X_train = read(ipd_load, \"X_train\")\n    y_train = read(ipd_load, \"y_train\")\n    X_test = read(ipd_load, \"X_test\")\n    y_test = read(ipd_load, \"y_test\")\nclose(ipd_load)\nopts = MPSOptions(d=10, chi_max=40, nsweeps=10; init_rng=4567)\nmps, _, _ = fitMPS(X_train, y_train, X_test, y_test, opts)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"Let's take a look at the training dataset for this problem:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: ) Using the trained MPS, we can then inspect the BEE for the class 0 (winter) and class 1 (summer) MPS individually:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"bees = bipartite_spectrum(mps);\nbee0, bee1 = bees\nb1 = bar(bee0, title=\"Winter\", label=\"\", c=palette(:tab10)[1], xlabel=\"site\", ylabel=\"entanglement entropy\");\nb2 = bar(bee1, title=\"Summer\", label=\"\", c=palette(:tab10)[2], xlabel=\"site\", ylabel=\"entanglement entropy\");\np = plot(b1, b2)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Single-Site-Entanglement-Entropy-(SEE)","page":"Tools","title":"Single-Site Entanglement Entropy (SEE)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"Given a trained MPS, we can also compute the single-site entanglement entropy (SEE) using the single_site_spectrum function:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# train MPS as usual\nmps, _, _ = fitMPS(...);\nsees = MPSTime.single_site_spectrum(mps);","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"As with the BEE, a vector is returned where each entry contains the SEE spectrum for the class-specific MPS. ","category":"page"},{"location":"tools/#Example-2","page":"Tools","title":"Example","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"Continuing our example from the BEE with the ItalyPowerDemand (IPD) dataset, we will now compute the single-site entanglement entropy (SEE) spectrum:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"sees = single_site_spectrum(mps);\nsee0, see1 = sees\nb1 = bar(see0, title=\"Winter\", label=\"\", c=palette(:tab10)[1], xlabel=\"site\", ylabel=\"SEE\");\nb2 = bar(see1, title=\"Summer\", label=\"\", c=palette(:tab10)[2], xlabel=\"site\", ylabel=\"SEE\");\np = plot(b1, b2)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Single-Site-Entanglement-Entropy-Variation","page":"Tools","title":"Single-Site Entanglement Entropy Variation","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"Another quantity we can compute is the single-site entanglement entropy (SEE) variation. In effect, the SEE variation captures the change in SEE at any given MPS site, conditional upon having measured the preceding sites. Given a trained MPS, the SEE variation can be computed:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# see_variation expects a data matrix, so we need to index as follows to feed in a single instance\nsee_variation = see_variation(mps, X_test[1:1, :])\n# if there is more than one class (e.g., classification)\nsee_variation_c0 = see_variation(mps, X_test[1:1, :], 0)\nsee_variation_c1 = see_variation(mps, X_test[1:1, :], 1)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"It can be useful to visualize the SEE variation as a barplot.  Here we will plot the SEE of the unmeasured MPS, after measuring 5 sites (i.e., 5 time pts.), 20 sites, and 50 sites: ","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"cpal = palette(:tab10)\nsee_variation = see_variation(mps, X_test[1:1, :])\nb = bar(see_variation[1, 1, :], c=cpal[1], label=\"Unmeasured\", xlabel=\"site\", ylabel=\"SEE\")\nbar!(see_variation[1, 6, :], c=cpal[2], label=\"5 Sites Measured\")\nbar!(see_variation[1, 21, :], c=cpal[3], label=\"20 Sites Measured\")\nbar!(see_variation[1, 51, :], c=cpal[4], label=\"50 Sites Measured\")","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Missing-Data-Simulation","page":"Tools","title":"Missing Data Simulation","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"In the time-series imputation literature, time-series data can be categorised into one of three types based on the underlying process responsible for the missing data: (i) missing completely at random (MCAR); (ii) missing at random (MAR); or, (iii) missing not at random (MNAR). A review of the various mechanisms in the univariate setting can be found in [5].","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"MPSTime provides implementations of all three mechanisms, adapted from the more typical multivariate setting to the case of univariate time-series data. To generate synthetic missing data, the original (uncorrupted) univariate time-series instance is passed into a function which assigns a NaN value to time points determined by the missing data mechanism of choice. ","category":"page"},{"location":"tools/#Missing-Completely-at-Random-(MCAR)","page":"Tools","title":"Missing Completely at Random (MCAR)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"To simulate missing completely at random (MCAR) data, the locations (time points) of missing points are sampled from a Bernoulli distribution where the probability of a \"successful trial\" (i.e., missing data point) is the same for all time points. Let's generate a random time-series instance and simulate 50% data missingness using an MCAR mechanism:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"using MPSTime\nusing Random\nRandom.seed!(42)\npm = 0.5 # 50% data missing\nX_clean = rand(100) # your data as a vector\nX_corrupted, X_missing_inds = mcar(X_clean, pm)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"The mcar function will return two values: a copy of the time-series with NaN values at missing positions (X_corrupted), and the indices of the missing values (X_missing_inds). Let's plot the corrupted data:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"using Plots\np1 = plot(X_clean, xlabel=\"time\", ylabel=\"x\", label=\"\", title=\"Original data\");\np2 = plot(X_corrupted, xlabel=\"time\", ylabel=\"x\", label=\"\", title=\"MCAR $(pm*100)% missing data\");\nplot(p1, p2, size=(1200, 300));","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"For reproducibility, we can optionally pass in a random seed to the mcar function:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"seed = 42; # random seed \nX_corrupted, X_missing_inds = mcar(X_clean, pm; state=seed)","category":"page"},{"location":"tools/#Missing-at-Random-(MAR)","page":"Tools","title":"Missing at Random (MAR)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"Following from the example above with randomly generated data, we can simulate a missing at random (MAR) mechanism using the mar function. Currently, MPSTime supports block missing patterns whereby a starting time point is randomly selected, and all subsequent observations within a specified block length are set to NaN:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# using the same data, X_clean, from above...\npm = 0.5\nX_corrupted, X_missing_inds = mar(X_clean, pm)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"Plotting the corrupted data:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Missing-Not-At-Random-(MNAR)","page":"Tools","title":"Missing Not At Random (MNAR)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"To simulate missing not at random (MNAR) data, use the mnar function. There are two possible options for the MNAR mechanims: (i) LowestMNAR (default option) and (ii) HighestMNAR. The LowestMNAR mechanism sets the lowest N values of the time-series to NaN where N is determined by the target percentage data missing.  Conversely, HighestMNAR sets the highest N value to NaN:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# using the same data, X_clean, from above...\npm = 0.5\nX_corrupted_low, X_missing_inds_low = mnar(X_clean, pm) # default setting uses LowestMNAR\nX_corrupted_high, X_missing_inds_high = mnar(X_clean, pm, MPSTime.HighestMNAR()) # use the HighestMNAR mechanism","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"Plotting corrupted time-series from the LowestMNAR mechanism:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Docstrings","page":"Tools","title":"Docstrings","text":"","category":"section"},{"location":"tools/#MPSTime.bipartite_spectrum","page":"Tools","title":"MPSTime.bipartite_spectrum","text":"bipartite_spectrum(mps::TrainedMPS; logfn::Function=log) -> Vector{Vector{Float64}}\n\nCompute the bipartite entanglement entropy (BEE) of a trained MPS across each bond. Given a single unlabeled MPS the BEE is defined as:\n\nsum_i alpha_i^2 log(alpha_i^2)\n\nwhere alpha_i are the eigenvalues obtained from the shmidt decomposition. \n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.single_site_spectrum","page":"Tools","title":"MPSTime.single_site_spectrum","text":"single_site_spectrum(mps::TrainedMPS) -> Vector{Vector{Float64}}\n\nCompute the single-site entanglement entropy (SEE) spectrum of a trained MPS.\n\nThe single-site entanglement entropy (SEE) quantifies the entanglement between each site in the MPS and all other sites. It is computed as:\n\ntextSEE = -texttr(rho log(rho))\n\nwhere rho is the single-site reduced density matrix (RDM).\n\nArguments\n\nmps::TrainedMPS: A trained Matrix Product State (MPS).\n\nReturns\n\nA vector of vectors, where the outer vector corresponds to each label in the expanded MPS, and the inner vectors contain the SEE values for the respective sites.\n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.see_variation","page":"Tools","title":"MPSTime.see_variation","text":"see_variation(mps::TrainedMPS, measure_series::Matrix, class::Int=0) -> Array{Float64, 3}\n\nCompute the variation in single-site entanglement entropy for each site N in the MPS,  having measured sites 1 to N-1.\n\nArguments\n\nmps::TrainedMPS: A trained Matrix Product State (MPS).\nmeasure_series::Matrix: A matrix of time-series instances to measure (unscaled).\nclass::Int: Class MPS to compute SEE variation if there are multiple classes (MPSs).\n\nReturns\n\nAn array of dimension 3, where the first dimension is the time series instance, the second dimension is the probe site (site at which to measure SEE) and the third dimenion is the number of previous sites measured.  \n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.mcar","page":"Tools","title":"MPSTime.mcar","text":"mcar(\n    X::AbstractVector, \n    fraction_missing::Float64, \n    mechanism::MCARMechanism=BernoulliMCAR(); \n    rng::AbstractRNG=Random.GLOBAL_RNG, \n    verbose::Bool=false\n) -> Tuple{Vector{Float64}, Vector{Int64}}\n\nGenerate missing data using a Missing Completely At Random (MCAR) mechanism, where the probability  of missingness is independent of both observed and unobserved values. Available mechanisms:\n\nBernoulliMCAR(): Missing values are generated by sampling from a Bernoulli distribution with   probability fraction_missing.\n\nArguments\n\nX::AbstractVector: Input time series data.\nfraction_missing::Float64: Target fraction of missing values, must be between 0 and 1 (default: 0.5).\nmechanism::MCARMechanism: Mechanism used to generate missing values (default: BernoulliMCAR()).\nrng::AbstractRNG: Random number generator for reproducibility (default: GLOBAL_RNG).\nverbose::Bool: If true, prints comparison of target vs actual percentage of missing values.\n\nReturns\n\nX_corrupted::Vector: Copy of input vector with NaN values at missing positions.\nmissing_idxs::Vector{Int64}: Indices of missing values.\n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.mar","page":"Tools","title":"MPSTime.mar","text":"mar(\n    X::AbstractVector, \n    fraction_missing::Float64, \n    mechanism::MARMechanism=BlockMissingMAR();\n    rng::AbstractRNG=Random.GLOBAL_RNG, \n    verbose::Bool=false\n) -> Tuple{Vector{Float64}, Vector{Int64}}\n\nGenerate missing data using a Missing At Random (MAR) mechanism, where the probability of  missingness depends only on observed values or known information. Available mechanisms:\n\nBlockMissingMAR(): Generates a contiguous block of missing values with random start position.\n\nThe missingness of each point after the first depends on the position of previous missing values.\n\nArguments\n\nX::AbstractVector: Input time series data.\nfraction_missing::Float64: Target fraction of missing values, must be between 0 and 1 (default: 0.5).\nmechanism::MARMechanism: Mechanism used to generate missing values (default: BlockMissingMAR())\nrng::AbstractRNG: Random number generator for reproducibility (default: GLOBAL_RNG).\nverbose::Bool: If true, prints comparison of target vs actual percentage of missing values.\n\nReturns\n\nX_corrupted::Vector{Float64}: Copy of input vector with NaN values at missing positions.\nmissing_idxs::Vector{Int64}: Indices of missing values.\n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.mnar","page":"Tools","title":"MPSTime.mnar","text":"mnar(\n    X::AbstractVector, \n    fraction_missing::Float64, \n    mechanism::MNARMechanism=LowestMNAR();\n    verbose::Bool=false\n) -> Tuple{Vector{Float64}, Vector{Int64}}\n\nGenerate missing data using a Missing Not At Random (MNAR) mechanism, where the probability  of missingness depends on the unobserved values themselves. Available mechanisms:\n\nLowestMNAR(): Introduces missing values at positions with the lowest values in the time series.   Implementation as in Twala 2019. \nHighestMNAR(): Introduces missing values at positions with the highest values in the time series.   Implementation as in Xia et al. 2017.\n\nArguments\n\nX::AbstractVector: Input time series data.\nfraction_missing::Float64: Target fraction of missing values, must be between 0 and 1 (default: 0.5).\nmechanism::MNARMechanism: Mechanism used to generate missing values (default: LowestMNAR()).\nverbose::Bool: If true, prints comparison of target vs actual percentage of missing values.\n\nReturns\n\nX_corrupted::Vector{Float64}: Copy of input vector with NaN values at missing positions.\nmissing_idxs::Vector{Int64}: Indices of missing values.\n\n\n\n\n\n","category":"function"},{"location":"tools/#Internal-Methods","page":"Tools","title":"Internal Methods","text":"","category":"section"},{"location":"tools/#MPSTime.von_neumann_entropy","page":"Tools","title":"MPSTime.von_neumann_entropy","text":"von_neumann_entropy(mps::MPS; logfn::Function=log) -> Vector{Float64}\n\nCompute the von Neumann entanglement entropy for each site in a Matrix Product State (MPS).\n\nThe von Neumann entropy quantifies the entanglement at each bond of the MPS by computing the entropy of the singular value spectrum obtained from a singular value decomposition (SVD). The entropy is computed as:\n\nS = -sum_i p_i log(p_i)\n\nwhere p_i are the squared singular values normalized to sum to 1.\n\nArguments\n\nmps::MPS: The Matrix Product State (MPS) whose entanglement entropy is to be computed.\nlogfn::Function: (Optional) The logarithm function to use (log, log2, or log10). Defaults to the natural logarithm (log).\n\nReturns\n\nA vector of Float64 values where the i-th element represents the von Neumann entropy at site i of the MPS.\n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.one_site_rdm","page":"Tools","title":"MPSTime.one_site_rdm","text":"one_site_rdm(mps::MPS, site::Int) -> Matrix\n\nCompute the single-site reduced density matrix (RDM) of the  MPS at a given site.  If the RDM is not positive semidefinite, clamp the negative eigenvalues (if within the tolerance) and reconstruct the rdm.\n\n\n\n\n\n","category":"function"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"We cite the following works in the development of MPSTime.jl:","category":"page"},{"location":"references/","page":"References","title":"References","text":"M. Fishman, S. R. White and E. M. Stoudenmire. The ITensor Software Library for Tensor Network Calculations. SciPost Phys. Codebases, 4 (2022).\n\n\n\nM. Fishman, S. R. White and E. M. Stoudenmire. Codebase release 0.3 for ITensor. SciPost Phys. Codebases, 4-r0.3 (2022).\n\n\n\nS.-J. Ran, Z.-Z. Sun, S.-M. Fei, G. Su and M. Lewenstein. Tensor network compressed sensing with unsupervised machine learning. Physical Review Research 2 (2020).\n\n\n\nY. Liu, W.-J. Li, X. Zhang, M. Lewenstein, G. Su and S.-J. Ran. Entanglement-Based Feature Extraction by Tensor Network Machine Learning. Frontiers in Applied Mathematics and Statistics 7 (2021).\n\n\n\nM. S. Santos, R. C. Pereira, A. F. Costa, J. P. Soares, J. A. Santos and P. H. Abreu. Generating Synthetic Missing Data: A Review by Missing Mechanism. IEEE Access 7, 11651–11667 (2019).\n\n\n\nE. M. Stoudenmire and D. J. Schwab. Supervised Learning with Quantum-Inspired Tensor Networks (2017), arXiv:1605.05775 [stat.ML].\n\n\n\nB. D. Fulcher and N. S. Jones, hctsa: A Computational Framework for Automated Time-Series Phenotyping Using Massive Feature Extraction. Cell Systems 5, 527-531.e3 (2017).\n\n\n\nH. A. Dau, A. Bagnall, K. Kamgar, C.-C. M. Yeh, Y. Zhu, S. Gharghabi, C. A. Ratanamahatana and E. Keogh. The UCR time series archive. IEEE/CAA Journal of Automatica Sinica 6, 1293–1305 (2019).\n\n\n\n","category":"page"},{"location":"hyperparameters/#hyperparameters_top","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"","category":"section"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"This tutorial for MPSTime will take you through tuning the hyperparameters of the fitMPS algorithm to maximise either imputation or classification performance.","category":"page"},{"location":"hyperparameters/#Setup","page":"Hyperparameter Tuning","title":"Setup","text":"","category":"section"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"For this tutorial, we'll be solving a classification hyperoptimisation problem and an imputation hyperoptimisation problem use the same noisy trendy sinusoid dataset from the Classification and Imputation sections.","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"using MPSTime, Random\nrng = Xoshiro(1); # fix rng seed\nntimepoints = 100; # specify number of samples per instance\nntrain_instances = 300; # specify num training instances\nntest_instances = 200; # specify num test instances\nX_train = vcat(\n    trendy_sine(ntimepoints, ntrain_instances ÷ 2; sigma=0.1, slope=[-3,0,3], period=(12,15), rng=rng)[1],\n    trendy_sine(ntimepoints, ntrain_instances ÷ 2; sigma=0.1, slope=[-3,0,3], period=(16,19), rng=rng)[1]\n);\ny_train = vcat(\n    fill(1, ntrain_instances ÷ 2),\n    fill(2, ntrain_instances ÷ 2)\n);\nX_test = vcat(\n    trendy_sine(ntimepoints, ntest_instances ÷ 2; sigma=0.2, slope=[-3,0,3], period=(12,15), rng=rng)[1],\n    trendy_sine(ntimepoints, ntest_instances ÷ 2; sigma=0.2, slope=[-3,0,3], period=(16,19), rng=rng)[1]\n);\ny_test = vcat(\n    fill(1, ntest_instances ÷ 2),\n    fill(2, ntest_instances ÷ 2)\n);","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"info: Info\nGiven how computationally intensive hyperparameter tuning can be, if you're running these examples yourself it's a good idea to take advantage of the multiprocessing built into the MPSTime library (see the Distributed Computing section). ","category":"page"},{"location":"hyperparameters/#Hyperoptimising-classification","page":"Hyperparameter Tuning","title":"Hyperoptimising classification","text":"","category":"section"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"The hyperparameter tuning algorithms supported by MPSTime supports every numerical hyperparameter that may be specified by MPSOptions. For this problem, we'll generate a small search space over the three most important hyperparameters: the maximum MPS bond dimension chi_max, the physical dimension d, and the learning rate eta. Every other hyperparamter will be left at its default value.","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"The variables to optimise, along with their upper and lower bounds are specified with the syntax params = (<variable_name_1>=(<lower_bound_1>, <upper_bound_1>), ...), e.g.","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"params = (\n    eta=(1e-3, 1e-1), \n    d=(2,8), \n    chi_max=(20,40)\n) ","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"When solving real-world problems, it's a good idea to explore a larger d and chi_max search space, but for this example it will serve well enough.","category":"page"},{"location":"hyperparameters/#K-fold-cross-validation-with-tune()","page":"Hyperparameter Tuning","title":"K-fold cross validation with tune()","text":"","category":"section"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"To optimise the hyperparameters on your dataset, simply call tune:","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"julia> nfolds = 5;\njulia> best_params, cache = tune(\n    X_train, \n    y_train, \n    nfolds,\n    params,\n    MPSRandomSearch(); \n    objective=MisclassificationRate(), \n    maxiters=20, # for demonstration purposes only, typically this should be much larger\n    logspace_eta=true\n);\niter 1, cvfold 1: training MPS with (chi_max = 37, d = 8, eta = 0.023357214690901226)...\niter 1, cvfold 2: training MPS with (chi_max = 37, d = 8, eta = 0.023357214690901226)...\niter 1, cvfold 3: training MPS with (chi_max = 37, d = 8, eta = 0.023357214690901226)...\niter 1, cvfold 4: training MPS with (chi_max = 37, d = 8, eta = 0.023357214690901226)...\niter 1, cvfold 5: training MPS with (chi_max = 37, d = 8, eta = 0.023357214690901226)...\niter 1, cvfold 1: finished. MPS (chi_max = 37, d = 8, eta = 0.023357214690901226) finished in 128.25s (train=126.38s, loss=1.87s)\niter 1, cvfold 2: finished. MPS (chi_max = 37, d = 8, eta = 0.023357214690901226) finished in 129.0s (train=127.11s, loss=1.89s)\niter 1, cvfold 3: finished. MPS (chi_max = 37, d = 8, eta = 0.023357214690901226) finished in 128.57s (train=126.74s, loss=1.84s)\niter 1, cvfold 4: finished. MPS (chi_max = 37, d = 8, eta = 0.023357214690901226) finished in 128.78s (train=126.96s, loss=1.82s)\niter 1, cvfold 5: finished. MPS (chi_max = 37, d = 8, eta = 0.023357214690901226) finished in 127.77s (train=125.94s, loss=1.83s)\niter 1, t=139.39: Mean CV Loss: 0.040000000000000015\niter 2, cvfold 1: training MPS with (chi_max = 39, d = 6, eta = 0.0379269019073225)...\n  \n[...]\n\niter 20, cvfold 4: finished. MPS (chi_max = 21, d = 2, eta = 0.018329807108324356) finished in 13.52s (train=13.45s, loss=0.08s)\niter 20, cvfold 5: finished. MPS (chi_max = 21, d = 2, eta = 0.018329807108324356) finished in 14.98s (train=14.47s, loss=0.5s)\niter 20, t=843.55: Mean CV Loss: 0.08\n\njulia> best_params\n(chi_max = 31,\n d = 4,\n eta = 0.07847599703514611,)\n\njulia> cache[values(best_params)] # retrieve loss from the cache\n0.0033333333333333435 # equivalent to 99.67% accuracy","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"which returns best params: a named tuple containing the optimised hyperparameters, and cache: a dictionary that saves the mean loss of every tested hyperparameter combination.","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"The arguments used here are:","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"X_train: timeseries data in a matrix, time series are rows.\ny_train: Vector of time series class labels.\nnfolds: Number of cross validiation folds to use. Folding type can be specified with the foldmethod keyword.\nparams: Hyperparameters to tune and their upper and lower bounds, see previous section.\nMPSRandomSearch(): The tuning algorithm to use, see the tuning algorithm section.\nobjective=MisclassificationRate(): Optimise the raw misclassification rate (1 - accuracy). The other options are BalancedMisclassificationRate (optimises balanced accuracy), or ImputationLoss() (see imputation hyperoptimisation).\nmaxiters=20: The maximum number of solver iterations. Here we use a very small number for demonstration reasons.\nlogspace_eta=true: A useful option that tells the tuning algorithm to sample the eta search space logarithmically.","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"There are many more customisation options for tune, see the docstring and extended help for more information / advanced usecases.","category":"page"},{"location":"hyperparameters/#Evaluating-model-performance-with-evaluate()","page":"Hyperparameter Tuning","title":"Evaluating model performance with evaluate()","text":"","category":"section"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"If you want to estimate the performance of MPSTime on a dataset, you can call the evaluate function, which resamples your data into train/test splits using a provided resampling strategy (default is k-fold cross validation), tunes each split on the \"training\" set, and evaluates the test set. It can be called with the following syntax:","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"julia> nresamples = 3; # number of outer \"resampling\" folds - usually 30\n\njulia> Xs = vcat(X_train, X_test);\n\njulia> ys = vcat(y_train, y_test);\n\njulia> results = evaluate(\n    Xs,\n    ys,\n    nresamples,\n    params,\n    MPSRandomSearch(); \n    n_cvfolds=nfolds, # the number of folds used by tune()\n    objective=MisclassificationRate(),\n    tuning_maxiters=20\n);\nBeginning fold 1:\nFold 1: iter 1, cvfold 1: training MPS with (chi_max = 40, d = 8, eta = 0.09478947368421053)...\nFold 1: iter 1, cvfold 2: training MPS with (chi_max = 40, d = 8, eta = 0.09478947368421053)...\nFold 1: iter 1, cvfold 3: training MPS with (chi_max = 40, d = 8, eta = 0.09478947368421053)...\nFold 1: iter 1, cvfold 4: training MPS with (chi_max = 40, d = 8, eta = 0.09478947368421053)...\nFold 1: iter 1, cvfold 5: training MPS with (chi_max = 40, d = 8, eta = 0.09478947368421053)...\n\n[...]\n\nFold 3: iter 20, cvfold 4: finished. MPS (chi_max = 28, d = 2, eta = 0.06873684210526317) finished in 19.26s (train=19.14s, loss=0.12s)\nFold 3: iter 20, cvfold 5: finished. MPS (chi_max = 28, d = 2, eta = 0.06873684210526317) finished in 19.86s (train=19.78s, loss=0.08s)\nFold 3: iter 20, t=875.4: Mean CV Loss: 0.011985526910900046\nfold 3: t=2899.6: training MPS with (chi_max = 39, d = 3, eta = 0.016631578947368424)...  done\n\njulia> results[1]\nDict{String, Any} with 13 entries:\n  \"time\"           => 1055.61\n  \"objective\"      => \"MisclassificationRate()\"\n  \"train_inds\"     => [340, 445, 262, 132, 89, 379, 225, 59, 224, 57  …  495, 484, 355, 322, 284, 363, …\n  \"optimiser\"      => \"MPSRandomSearch(:LatinHypercube)\"\n  \"fold\"           => 1\n  \"test_inds\"      => [133, 477, 112, 148, 13, 453, 342, 83, 252, 455  …  151, 483, 74, 380, 61, 297, 1…\n  \"tuning_windows\" => nothing\n  \"eval_windows\"   => nothing\n  \"cache\"          => Dict((39, 6, 0.0166316)=>0.0149706, (33, 7, 0.0426842)=>0.0300769, (36, 7, 0.0635…\n  \"tuning_pms\"     => nothing\n  \"loss\"           => [0.00598802]\n  \"eval_pms\"       => nothing\n  \"opts\"           => MPSOptions(-5, 10, 32, 0.0531053, 4, :Legendre_No_Norm, false, 2, 1.0e-10, 1, Flo…\n\njulia> losses = getindex.(results, \"loss\")\n3-element Vector{Vector{Float64}}:\n [0.005988023952095856]\n [0.005988023952095856]\n [0.0060240963855421326]\n","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"Which outputs a results dictionary, containing the losses on each resample fold, as well as a lot of other useful information. See the docstring for more detail as well as a plethora of customistation options.","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"A very common extension of evaluate is to customise the resampling strategy. The simplest way to do this is to pass a vector of (training_indices, testing_indices) to the foldmethod keyword. For example, to use scikit-learn's StratifiedShuffleSplit to generate the train/test splits:","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"julia> using PyCall\n\njulia> nresamples = 3;\n\njulia> py\"\"\"\n    from sklearn.model_selection import StratifiedShuffleSplit # requires a python environment with sklearn installed\n    sp = StratifiedShuffleSplit(n_splits=$nresamples, test_size=$(length(y_test)), random_state=1)\n    folds_py = sp.split($Xs, $ys)\n    \"\"\"\n\njulia> folds = [(tr_inds .+ 1, te_inds .+ 1) for (tr_inds, te_inds) in py\"folds_py\"] # shift indices up by 1\n3-element Vector{Tuple{Vector{Int64}, Vector{Int64}}}:\n ([197, 472, 462, 108, 133, 258, 179, 57, 149, 373  …  279, 94, 234, 473, 319, 378, 387, 92, 359, 35], [354, 313, 137, 239, 316, 479, 274, 145, 134, 485  …  110, 417, 346, 141, 165, 50, 77, 23, 347, 130])\n ([399, 105, 322, 289, 281, 187, 131, 18, 56, 231  …  463, 38, 491, 288, 408, 430, 330, 185, 481, 353], [345, 469, 396, 10, 96, 452, 245, 76, 367, 84  …  202, 153, 94, 446, 372, 152, 79, 387, 51, 301])\n\n[...]\n\njulia> results = evaluate(\n    Xs,\n    ys,\n    nresamples,\n    params,\n    MPSRandomSearch(); \n    objective=MisclassificationRate(),\n    tuning_maxiters=20,\n    foldmethod=folds\n);\n[...]","category":"page"},{"location":"hyperparameters/#imputation_hyper","page":"Hyperparameter Tuning","title":"Hyperoptimising imputation","text":"","category":"section"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"The tune and evaluate methods may both be used to minimise imputation loss, with a small amount of extra setup. Setting objective=ImputationLoss() will optimise an MPS for imputation performance by minimising the mean absolute error (MAE) between predicted and unseen data. To accomplish this, MPSTime takes data from the test (or validation) set, corrupts a portion of it, and then predicts what the corrupted data should be based on the uncorrupted values. There are two methods for how the test (or validation) data can be corrupted.","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"Setting the windows (or eval_windows) keyword in tune (or evaluate, respectively) to a vector of 'windows'. Each window is a vector of missing/corrupted data indices, for example","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"windows = [[1,3,7],[4,5,6]]","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"will take each timeseries in the test set, and create two 'corrupted' test series, missing the 1st, 3rd, and 7th; and the 4th, 5th, and 6th values respectively.","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"Setting the pms (or eval_pms) keyword in tune (or evaluate, respectively) to a vector of 'percentage missings'. This generates corrupted time series by removing randomly selected contiguous blocks that make up a specified percentage of the data. For example, ","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"pms = [0.05, 0.05, 0.6, 0.95]","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"will generate four corrupted time series from each element of the test (or validation) set. Two will have missing blocks that make up 5% of their length, and one each will have blocks with 60% and 95% missing. The imputation tuning loss is the average MAE of imputing every window on every element of the test (or validation) set.","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"Example: Calling tune with percentages missing Tune the MPS on an imputation problem with randomly selected 5%, 15%, 25%, ... , 95% long missing blocks.","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"params = (\n    d=(8,12), \n    chi_max=(30,50)\n)\n\nbest_params, cache = tune(\n    X_train, \n    y_train, \n    nfolds,\n    params,\n    MPSRandomSearch(); \n    objective=ImputationLoss(), \n    pms=collect(0.05:0.1:0.95),\n    maxiters=20,\n    logspace_eta=true\n)","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"iter 1, cvfold 1: training MPS with (chi_max = 47, d = 12)...\niter 1, cvfold 2: training MPS with (chi_max = 47, d = 12)...\niter 1, cvfold 3: training MPS with (chi_max = 47, d = 12)...\niter 1, cvfold 4: training MPS with (chi_max = 47, d = 12)...\niter 1, cvfold 5: training MPS with (chi_max = 47, d = 12)...\niter 1, cvfold 1: finished. MPS (chi_max = 47, d = 12) finished in 515.58s (train=402.83s, loss=112.75s)\niter 1, cvfold 2: finished. MPS (chi_max = 47, d = 12) finished in 511.0s (train=399.52s, loss=111.48s)\niter 1, cvfold 3: finished. MPS (chi_max = 47, d = 12) finished in 516.33s (train=406.92s, loss=109.41s)\niter 1, cvfold 4: finished. MPS (chi_max = 47, d = 12) finished in 519.06s (train=405.13s, loss=113.93s)\niter 1, cvfold 5: finished. MPS (chi_max = 47, d = 12) finished in 522.12s (train=406.87s, loss=115.25s)\niter 1, t=534.28: Mean CV Loss: 0.44716868140737487\niter 2, cvfold 1: training MPS with (chi_max = 50, d = 11)...\n\n[...]\n\niter 20, cvfold 4: finished. MPS (chi_max = 31, d = 8) finished in 112.81s (train=66.33s, loss=46.48s)\niter 20, cvfold 5: finished. MPS (chi_max = 31, d = 8) finished in 113.9s (train=66.32s, loss=47.58s)\niter 20, t=5223.05: Mean CV Loss: 0.4755302875557089","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"julia> best_params\n(chi_max = 48,\n d = 9,)\n\njulia> cache[values(best_params)]\n0.39402101779354365","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"Example: Using evaluate with the Missing Completely At Random tool Tune the MPS on an imputation problem by completely randomly corrupting 5%, 15%, 25%, ... , or 95% of each test (or validation) time series. See the Missing Completely at Random tool.","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"inds = collect(1:size(Xs,2))\nrng = Xoshiro(42)\npms = 0.05:0.1:0.95\nmcar_windows = [mcar(inds, pm; rng=rng)[2] for pm in pms]\nresults = evaluate(\n    Xs,\n    ys,\n    nresamples,\n    params,\n    MPSRandomSearch(); \n    objective=ImputationLoss(),\n    eval_windows=mcar_windows,\n    tuning_maxiters=20,\n)","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"Fold 1: iter 1, cvfold 1: training MPS with (chi_max = 50, d = 12)...\n     \n[...]\n\nFold 3: iter 20, cvfold 5: finished. MPS (chi_max = 35, d = 8) finished in 635.05s (train=255.05s, loss=380.0s)\nFold 3: iter 20, t=20328.2: Mean CV Loss: 0.24440015696620948\nfold 3: t=42210.43: training MPS with (chi_max = 48, d = 12)...  done","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"julia> losses = getindex.(results, \"opts\");\n\njulia> mean.(losses)\n3-element Vector{Float64}:\n 0.21739511880650658\n 0.21129840885487566\n 0.21441462843171047\n\njulia> getindex.(results, \"opts\") # print out the MPSOptions objects\n3-element Vector{MPSOptions}:\n MPSOptions(-5, 10, 47, 0.01, 11, :Legendre_No_Norm, false, 2, 1.0e-10, 1, Float64, :KLD, :TSGO, false,\n(false, true), false, false, false, true, false, false, 1234, 4, -1, (0.0, 1.0), false, \"divide_and_conquer\")\n MPSOptions(-5, 10, 50, 0.01, 12, :Legendre_No_Norm, false, 2, 1.0e-10, 1, Float64, :KLD, :TSGO, false,\n(false, true), false, false, false, true, false, false, 1234, 4, -1, (0.0, 1.0), false, \"divide_and_conquer\")\n MPSOptions(-5, 10, 48, 0.01, 12, :Legendre_No_Norm, false, 2, 1.0e-10, 1, Float64, :KLD, :TSGO, false,\n(false, true), false, false, false, true, false, false, 1234, 4, -1, (0.0, 1.0), false, \"divide_and_conquer\")\n\njulia> results[1]\nDict{String, Any} with 13 entries:\n  \"time\"           => 6152.08\n  \"objective\"      => \"ImputationLoss()\"\n  \"train_inds\"     => [340, 445, 262, 132, 89, 379, 225, 59, 224, 57  …  495, 484, 355, 322, 284, 363, …\n  \"optimiser\"      => \"MPSRandomSearch(:LatinHypercube)\"\n  \"fold\"           => 1\n  \"test_inds\"      => [133, 477, 112, 148, 13, 453, 342, 83, 252, 455  …  151, 483, 74, 380, 61, 297, 1…\n  \"tuning_windows\" => [[35, 97], [10, 36, 38, 49, 60, 62, 65, 72, 81, 82, 88, 92, 93, 97], [9, 16, 24, …\n  \"eval_windows\"   => [[35, 97], [10, 36, 38, 49, 60, 62, 65, 72, 81, 82, 88, 92, 93, 97], [9, 16, 24, …\n  \"cache\"          => Dict((49, 10)=>0.217511, (41, 10)=>0.233205, (45, 8)=>0.231089, (31, 8)=>0.259456…\n  \"tuning_pms\"     => nothing\n  \"loss\"           => [0.192093, 0.193801, 0.183081, 0.189859, 0.194329, 0.19409, 0.211727, 0.235688, 0…\n  \"eval_pms\"       => nothing\n  \"opts\"           => MPSOptions(-5, 10, 47, 0.01, 11, :Legendre_No_Norm, false, 2, 1.0e-10, 1, Float64…\n\n","category":"page"},{"location":"hyperparameters/#Minimising-a-custom-loss-function","page":"Hyperparameter Tuning","title":"Minimising a custom loss function","text":"","category":"section"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"Custom objectives can be used by implementing a custom loss value type (CustomLoss <: MPSTime.TuningLoss) and extending the definition of MPSTime.eval_loss with the signature","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"eval_loss(\n    CustomLoss(), \n    mps::TrainedMPS, \n    X_validation::AbstractMatrix, \n    y_validation::AbstractVector, \n    windows; \n    p_fold=nothing, \n    distribute::Bool=false\n) -> Union{Float64, Vector{Float64}}","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"As a simple example, we could implement a custom misclassification rate with the following:","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"julia> import MPSTime: TuningLoss, eval_loss;\njulia> struct CustomMisclassificationRate <: TuningLoss end;\n\njulia> function eval_loss(\n        ::CustomMisclassificationRate, \n        mps::TrainedMPS, \n        X_val::AbstractMatrix, \n        y_val::AbstractVector, \n        windows; \n        p_fold=nothing,\n        distribute::Bool=false\n    )\n    return [1. - mean(classify(mps, X_val) .== y_val)] # misclassification rate, vector for type stability\nend;\n\njulia> results = evaluate(\n    Xs,\n    ys,\n    nfolds,\n    params,\n    MPSRandomSearch(); \n    objective=CustomMisclassificationRate(),\n    tuning_maxiters=20\n)\n[...]","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"Since imputation type losses have multiple windows, the general output type of the eval_loss function is a vector. Because of this, the tune() function always optimises mean(eval_loss(...))","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"The p_fold variable is a tuple containing information used for logging. The distribute flag is enabled by evaluate when distribute_final_eval is true. As an example of implementing both of these, here's MPSTime's implementation of eval_loss(::ImputationLoss, ...):","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"details: `eval_loss` source code\nfunction eval_loss(\n    ::ImputationLoss, \n    mps::TrainedMPS, \n    X_val::AbstractMatrix, \n    y_val::AbstractVector, \n    windows::Union{Nothing, AbstractVector}=nothing;\n    p_fold::Union{Nothing, Tuple}=nothing,\n    distribute::Bool=false,\n    method::Symbol=:median\n)\n    \n    if ~isnothing(p_fold)\n        verbosity, pre_string, tstart, fold, nfolds = p_fold\n        logging = verbosity >= 2\n        foldstr = isnothing(fold) ? \"\" : \"cvfold $fold:\"\n    else\n        logging = false\n    end\n    imp = init_imputation_problem(mps, X_val, y_val, verbosity=-5);\n    numval = size(X_val, 1)\n\n    # conversion from instance index to something MPS_impute understands. \n    cmap = countmap(y_val) # from StatsBase\n    classes = vcat([fill(k,v) for (k,v) in pairs(cmap)]...)\n    class_ind = vcat([1:v for v in values(cmap)]...)\n\n    if distribute\n        loss_by_window = @sync @distributed (+) for inst in 1:numval\n            logging && print(pre_string, \"$foldstr Evaluating instance $inst/$numval...\")\n            t = time()\n            ws = Vector{Float64}(undef, length(windows))\n            for (iw, impute_sites) in enumerate(windows)\n                stats = MPS_impute(imp, classes[inst], class_ind[inst], impute_sites, method; NN_baseline=false, plot_fits=false)[4]\n                ws[iw] = stats[1][:MAE]\n            end\n            logging && println(\"done ($(MPSTime.rtime(t))s)\") # rtime just nicely prints time elapsed since $t in seconds\n            ws\n        end\n        loss_by_window /= numval\n    else\n        instance_scores = Matrix{Float64}(undef, numval, length(windows)) # score for each instance across all % missing\n        for inst in 1:numval\n            logging && print(pre_string, \"$foldstr Evaluating instance $inst/$numval...\")\n            t = time()\n            for (iw, impute_sites) in enumerate(windows)\n                stats = MPS_impute(imp, classes[inst], class_ind[inst], impute_sites, method; NN_baseline=false, plot_fits=false)[4]\n                instance_scores[inst, iw] = stats[1][:MAE]\n            end\n            logging && println(\"done ($(MPSTime.rtime(t))s)\") # rtime just nicely prints time elapsed since $t in seconds\n        end\n        loss_by_window = mean(instance_scores; dims=1)[:]\n    end\n    \n\n    return loss_by_window\nend\n","category":"page"},{"location":"hyperparameters/#hyper_algs","page":"Hyperparameter Tuning","title":"Choice of tuning algorithm","text":"","category":"section"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"The hyperparameter tuning algorithm used by tune (or evaluate) can be specified with the optimiser argument. This supports the default builtin MPSRandomSearch methods, as well as (in theory) any solver that is supported by the Optimization.jl interface. Note that many of these solvers struggle with discrete search spaces, such as tuning the integer valued chi_max and d. Some of them require initial conditions (set provide_x0=true), and some require no initial conditions (set provide_x0=false), so your mileage may vary. By default, tune() handles optimisers attempting to evaluate discrete hyperparameters at a non-integer value by rounding and using its own cache to avoid rounding based cache misses. This is effective, but has the downside of causing maxiters to be inaccurate (as repeated hyperparameter evaluations caused by rounding result in a 'skipped' iteration).","category":"page"},{"location":"hyperparameters/#MPSTime.MPSRandomSearch","page":"Hyperparameter Tuning","title":"MPSTime.MPSRandomSearch","text":"    MPSRandomSearch(sampling::Symbol=:LatinHypercube)\n\nValue type used to specify a random search algorithm for hyperparameter tuning an MPS. \n\nSampling Specifies the method used to determine the search space. The supported sampling methods are \n\n:LatinHypercube: An implementation of LatinHypercubeSampling.jl's random (pseudo-) Latin Hypercubesearch space generator. Supports both discrete and continuous hyperparameters.\n:UniformRandom: Generate a search space by randomly sampling from the interval [lower bound, upper bound] for each hyperparameter. Supports both discrete andcontinuous hyperparameters.\nExhaustive: Perform an exhaustive gridsearch of all hyperparameters within the lower and upper bounds. Only supports discrete hyperparameters. Not actually a random search.  \n\n\n\n\n\n","category":"type"},{"location":"hyperparameters/#distributed_computing","page":"Hyperparameter Tuning","title":"Distributed computing","text":"","category":"section"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"Both tune tune and evaluate support several different parallel processing paradigms for different use cases, compatible with processors added via Distributed.jl's addprocs function.  For example, to distribute each fold of the classification style evaluation above, run:","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"using Distributed\nnfolds = 30;\n\ne = copy(ENV);\ne[\"OMP_NUM_THREADS\"] = \"1\"; # attempt to prevent threading\ne[\"JULIA_NUM_THREADS\"] = \"1\"; # attempt to prevent threading\n\naddprocs(nfolds; env=e);\n@everywhere using MPSTime\n\nXs = vcat(X_train, X_test);\nys = vcat(y_train, y_test);\n\nresults = evaluate(\n    Xs,\n    ys,\n    nfolds,\n    params,\n    MPSRandomSearch(); \n    objective=MisclassificationRate(),\n    tuning_maxiters=20,\n    distribute_folds=true\n)\n\n[...]\n","category":"page"},{"location":"hyperparameters/","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning","text":"See the respective docstrings for more information.","category":"page"},{"location":"hyperparameters/#Docstrings","page":"Hyperparameter Tuning","title":"Docstrings","text":"","category":"section"},{"location":"hyperparameters/#Hyperparameter-tuning","page":"Hyperparameter Tuning","title":"Hyperparameter tuning","text":"","category":"section"},{"location":"hyperparameters/#MPSTime.tune","page":"Hyperparameter Tuning","title":"MPSTime.tune","text":"function tune(\n    Xs::AbstractMatrix, \n    [ys::AbstractVector], \n    nfolds::Integer,\n    parameters::NamedTuple,\n    optimiser=MPSRandomSearch(:LatinHypercubeSampling);\n    <Keyword Arguments>) -> best_parameters::NamedTuple, cache::Dictionary\n\nPerform nfolds-fold hyperparameter tuning of an MPS on the timeseries data Xs, optionally specifying the data classes ys. Returns a NamedTuple containing the optimal hyperparameters, and a cache Dictionary that saves the loss of every tested hyperparameter combination.\n\nparameters specifies the hyperparameters to tune and (optionally) a search space. Currently, every numeric field of MPSOptions is supported. parameters  are specified as named tuples, with the key being the name of the hyperparameter (See Example below). There are a couple of options for specifying the bounds:\n\nThe preferred option is Tuple of lower/upper bounds: \"params=(eta=(upper_bound,lower_bound), ...)\", which allows the optimiser to choose any value in the interval [upperbound, lowerbound]. You can also pass an empty tuple: \"params=(eta=(), ...)\" which allows the parameter to take any non-negative value. \nAs a Vector of possible values, e.g. params = (d=[1,3,6,7,8], ...). For convenience, you can also use the Tuple syntax \"params=(d=(start,step,stop), ...)\", which is equivalent to \"params = (d=start:step:stop, ...)\"\n\nNote that if you use the first method, the upper and lower bounds are passed to the hyperparameter tuning algorithm, but may not be strictly enforced depending on your choice of algorithm.  Alternatively, use the enforce_bounds=false keyword argument to disable bounds checking completely (for compatible optimisers).\n\nExample:\n\nTo tune a classification problem by searching the hyperparameter space η ∈ [0.001, 0.1], d ∈ {5,6,7}, and χmax ∈ {20,21,...,25}:\n\njulia> params = (\n    eta=(1e-3, 1e-1), \n    d=(5,7), \n    chi_max=(20,25)\n); # configure the search space\n\njulia> nfolds = 5;\n\njulia> best_params, cache = tune(\n    X_train, # Training data as a matrix, rows are time series\n    y_train, # Vector of time series class labels\n    nfolds, # Number of cross validation folds\n    params,\n    MPSRandomSearch(); # Tuning algorithm\n    objective=MisclassificationRate(), # Type of loss to use\n    maxiters=20, # Maximum number of tuning iterations\n    logspace_eta=true # When true, the eta search space [0.001, 0.1] is sampled logarithmically\n)\n[...]\n\njulia> best_opts = MPSOptions(best_params); # convert to an MPSOptions object\n[...]\n\nOther problem classes are available, see the MPSTime documentation.\n\nHyperparameter Tuning Methods\n\nThe hyperparameter tuning algorithm can be specified with the optimiser argument. This supports the default builtin MPSRandomSearch methods, as well as (in theory) any solver that is supported by the Optimization.jl interface. Note that many of these solvers struggle with discrete search spaces, such as tuning the integer valued chi_max and d, or tuning an eta specified with a vector. Some of them require initial conditions (set provide_x0=true),  and some require no initial conditions (set provide_x0=false), so your mileage may vary. By default, tune() handles optimisers attempting to evaluate  discrete hyperparameters at a non-integer value by rounding, and using its own cache to avoid rounding based cache misses.\n\nThere are a lot of keyword arguments... Extended help is avaliable with ??tune\n\nSee also: evaluate\n\nExtended Help\n\nKeyword Arguments\n\nHyperparameter Options\n\nopts0::AbstractMPSOptions=MPSOptions(; verbosity=-5, log_level=-1, sigmoid_transform=objective isa ClassificationLoss): Default hyperparamaters to pass to fitMPS. Hyperparameter candidates are generated by modifying opts0 with the values in the search space specified by parameters.\nenforce_bounds::Bool=true: Whether to pass the constraints given in params to the optimisation algorithm.\nlogspace_eta::Bool=false: Whether to treat the eta parameterspace as logarithmic. E.g. setting parameters=(eta=(10^-3,10^-1) ) and logspace_eta=true will sample each eta_candidate from the log10 search space [-3.,-1.], and then pass eta = 10^(eta_candidate) to MPSOptions. \ninput_supertype::Type=Float64: A numeric type that can represent the types of each hyperparameter being tuned as well as their upper and lower bounds. Typically, Float64 is sufficient, but it can be set to Int for purely discrete optimisation problems etc. This is necessary for mixed integer / Float  hyperparameter tuning because certain solvers in Optimization.jl require variables in the search space to all be the same type.\n\nLoss and Windowing\n\nobjective::TuningLoss=ImputationLoss(): The objective of the hyperparameter optimisation. This comes in two categories:\n\nImputation Loss\n\nobvjective=ImputationLoss() Uses the mean of the mean absolute error to measure the performance of an MPS on imputing unseen data. First, it generates 'corrupted' time series data by applying missing data windows to the validation set, using one of the following options:\n\npms::Union{Nothing, AbstractVector}=nothing: Stands for 'percentage missings'. Will remove a randomly selected contiguous blocks from each time series in the validation set, according to the percentages missing listed in the pms vector. For example, pms=[0.05, 0.05, 0.6, 0.95] will generate four windows, two with 5% missing, and one each with 60% and 95% missing\nwindows::Union{Nothing, AbstractVector, Dict}=nothing: Manually input missing windows. Expects a vector of missing windows, or a dictionary where values(windows) is a vector of missing windows.\n\nThe tuning loss is the average of computing the mean absolute error of imputing every window on every element of the validation set.\n\nClassification Loss\n\nClassification type problems can be hyperparameter tuned to minimise either MisclassificationRate() (1 - classification accuracy), or BalancedMisclassificationRate() (1 - balanced accuracy).\n\nCustom Loss Functions\n\nCustom losses can be used by implementing a custom loss value type (CustomLoss <: TuningLoss) and extending the definition of MPSTime.eval_loss with the signature\n\neval_loss(\n    CustomLoss(), \n    mps::TrainedMPS, \n    X_validation::AbstractMatrix, \n    y_validation::AbstractVector, \n    windows; \n    p_fold=nothing, \n    distribute::Bool=false\n) -> Union{Float64, Vector{Float64}\n\nif eval_loss returns a vector, then tune() will optimise mean(eval_loss(...)). For concrete examples, see the documentation\n\nTuning algorithm\n\nabstol::Float64=1e-3: Passed directly to Optimization.jl: Absolute tolerance in changes to the objective (loss) function\nmaxiters::Integer=250: Maximum number of iterations allowed when solving\nprovide_x0::Bool=true: Whether to provide initial conditions to the solve, ignored by MPSRandomSearch. The initial condition will be opts0, unless it contains a hyperparameter outside the range specified by parameters, in which case the lower bound of that hyperparameter will be used.\nrng::Union{Integer, AbstractRNG}=1: An Integer or RNG object used to seed any randomness in imputation window or search space generation.\nkwargs...: Any further keyword arguments to passed through to Optimization.jl through the Optimization.solve function\n\nFolds and Cross validation\n\nfoldmethod::Union{Function, AbstractVector}=make_stratified_cvfolds: The method used to generate the train/validation folds from Xs. Can either be an nfolds-long Vector of [train_indices::Vector, validation_indices::Vector] pairs, or a function that produces them, with the signature foldmethod(Xs,ys, nfolds; rng::AbstractRNG) To clarify, the tune function determines the train/validation splits for the ith fold in the following way:\n\njulia> folds::Vector = foldmethod isa Function ? foldmethod(Xs,ys, nfolds; rng=rng) : foldmethod;\n\njulia> train_inds, validation_inds = folds[i];\n\njulia> X_train, y_train = Xs[train_inds, :], ys[train_inds];\n\njulia> X_validation, y_validation = Xs[validation_inds, :], ys[validation_inds];\n\nLogging\n\nverbosity::Integer=1: Controls how explicit the logging is. 0 for none, 5 for maximimum. This is separate to the verbosity in MPSOptions.\npre_string::String=\"\": Prints this string on the same line before logging messages are printed. Useful for logging when calling tune() multiple times in parallel.\n\nDistributed Computing\n\nParallel processing is available using processors added via Distributed.jl's addprocs function.\n\ndistribute_iters::Bool=false: When using an MPSRandomSearch, distribute the search grid across all available processors. For thread safety, using distribute_iters disables caching.\ndistribute_folds::Bool=false: Distribute each fold to its own processor. Scales up to at most nfolds processors. Not very compatible with distribute_iters.\nworkers::AbstractVector{Int}=distribute_folds ? workers() : Int[]: Workers that may be used to distribute folds, does not affect distribute_iters. This can be used to run multiple instances of tune() on different sets of workers.\ndisable_nondistributed_threading::Bool=false: Attempts to disable threading using BLAS.set_num_threads(1) and Strided.disable_threads() (May not work if using the MKL.jl linear algebra backend).\n\n\n\n\n\n","category":"function"},{"location":"hyperparameters/#MPSTime.evaluate","page":"Hyperparameter Tuning","title":"MPSTime.evaluate","text":"function evaluate(\n    Xs::AbstractMatrix, \n    [ys::AbstractVector], \n    nfolds::Integer,\n    tuning_parameters::NamedTuple,\n    tuning_optimiser=MPSRandomSearch();\n    <Keyword Arguments>) -> results::Vector{Dictionary}\n\nEvaluate the performance of MPSTime by hyperparameter tuning on nfolds resampled folds of the timeseries dataset Xs with classes ys.\n\ntuning_parameters controls the hyperparamters to tune over, and tuning_optimiser specifies the hyperparameter tuning algorithm. These are passed directly to tune, refer to its documentation for details. \n\nExample\n\nTo evaluate a classification problem by searching the hyperparameter space η ∈ [0.001, 0.1], d ∈ {5,6,7}, and χmax ∈ {20,21,...,25},\n\njulia> params = (\n    eta=(1e-3, 1e-1), \n    d=(5,7), \n    chi_max=(20,25)\n);\njulia> nfolds = 30;\n\njulia> results = evaluate(\n    X_train, # training data as a matrix, rows are time series\n    y_train, # Vector of time series class labels\n    nfolds, # number of resample folds\n    params, # search space\n    MPSRandomSearch(); # Hyperparameter search method, see Extended help\n    objective=MisclassificationRate(), # Type of lsso to use\n    maxiters=20, # Maximum number of tuning iterations\n    logspace_eta=true # the eta search space [0.001, 0.1] is sampled logarithmically\n)\n[...]\n\nReturn value\n\nA length nfolds vector of dictionaries that contain detailed informatation about each fold. Each dictionary has the following keys:\n\n\"fold\"=>Integer: The fold index.\n\"objective\"=>String: The objective (loss function) this fold was trained on.\n\"train_inds\"=>Vector: The indices (rows of Xs) this fold was tuned/trained on.\n\"test_inds\"=>Vector: The indices (rows of Xs) this fold was tested on.\n\"optimiser\"=>String: Name of the optimiser used to hyperparameter tune each fold.\n\"tuning_windows\"=>Vector: The windows used to hyperparameter tune this fold.\n\"tuning_pms\"=>Vector: The 'Percentages Missing' used to hyperparameter tune this fold (possibly used to generate tuning_windows).\n\"eval_windows\"=>Vector: The windows used to evaluate the test loss.\n\"eval_pms\"=>eval_pms: The 'Percentages Missing' used to evaluate the test loss (possibly used to generate eval_windows).\n\"time\"=>Vector: Total time to tune and test this fold in seconds.\n\"opts\"=>MPSOptions: Optimal options for this fold as determined by tune(). Used to compute the test loss.\n\"cache\"=>Dict: Cache of the validation losses of every set of hyperparameters evaluated on this fold. Disabled if distribute_iters is true.\n\"loss\"=>Union{Vector{Float64}, Float64}. The test loss of this fold. If objective is an ImputationLoss(), this is a vector with each entry corresponding to a window in results[fold][\"eval_windows\"].\n\nThere are a lot of keyword arguments... Extended help is avaliable with ??evaluate\n\nExtended Help\n\nKeyword Arguments\n\nLoss and Windowing\n\nobjective::TuningLoss=ImputationLoss(): The loss used to evaluate and tune the MPS. If its an ImputationLoss, then either pms or windows must be specified for each of evaluation and tuning. See the tune extended documentation for more details.\neval_pms::Union{Nothing, AbstractVector}=nothing: 'Percentage MissingS' used to evaluate the test loss.\neval_windows::Union{Nothing, AbstractVector, Dict}=nothing: Windows used to evaluate the test loss.\ntuning_pms::Union{Nothing, AbstractVector}=eval_pms: 'Percentage MissingS' passed to tune, and used to compute validation loss.\ntuning_windows::Union{Nothing, AbstractVector, Dict}=eval_windows: Windows passed to tune, and used to compute validation loss.\nrng::Union{Integer, AbstractRNG}=1: An integer or RNG object used to seed any randomness in imputation window or search space generation. Random.seed!(fold)` is called prior to tuning each fold, so that any optimization algorithms that are random but don't take rng objects should still be deterministic.\ntuning_rng::AbstractVector{<:Union{Integer, AbstractRNG}=collect(1:nfolds): Passed through to tune. An integer or RNG object used to seed any randomness in tuning imputation window generation or hyperparameter searching.\nopts0::AbstractMPSOptions=MPSOptions(; verbosity=-5, log_level=-1, sigmoid_transform=(objective isa ClassificationLoss)): Options that are modified by the best options returned by tune. Used to train the MPS which evalutates the test loss. \ntuning_opts0::AbstractMPSOptions=opts0: Initial guess passed as opts0 to tune that sets the values of the non-tuned hyperparameters. Should generally always be the same as opts0, but can be specified separately in case you wish to make the final mps train with more verbosity etc. \n\nResampling and Cross Validation\n\nfoldmethod::Union{Function, Vector}=make_stratified_cvfolds: Can either be an nfolds-long Vector of [train_indices::Vector, test_indices::Vector] pairs, or a function that produces them, with the signature foldmethod(Xs,ys, nfolds; rng::AbstractRNG) To clarify, the tune function determines the train/test splits for the ith fold in the following way:\n\njulia> folds::Vector = foldmethod isa Function ? foldmethod(Xs,ys, nfolds; rng=rng) : foldmethod;\n\njulia> train_inds, test_inds = folds[i];\n\njulia> X_train, y_train = Xs[train_inds, :], ys[train_inds];\n\njulia> X_test, y_test = Xs[test_inds, :], ys[test_inds];\n\nfoldmethod defaults to nfold-fold cross validation.\n\ntuning_foldmethod::Union{Function, Vector}=make_stratified_cvfolds: Same as above, although it is passed to tune and used to split the training set into hyperparameter train/validation sets. The fold number specified by the n_cvfolds keyword.\nfold_inds::Vector{<:Integer}=collect(1:nfolds): A vector of the fold indices to evaluate. This can be used to split large training runs into batches, or to resume a halted benchmark.\n\nDistributed Computing\n\nSeveral parallel processing paradigms are availble for different use cases, implented using processors added via Distributed.jl's addprocs` function.\n\ndistribute_iters::Bool=false: When using an MPSRandomSearch, for each fold, distribute the search grid across all available processors. For thread safety, using distributed_iters disables caching.\ndistribute_folds::Bool=false: Allocate one processor to each fold.\ndistribute_cvfolds::Bool=false: Equivalent to passing distribute_folds to tune. Allocates a processor to each hyperparameter train/val split.\ndistribute_final_eval::Bool=false: Allocate a processor to each test timeseries when computing the test loss. Useful when the test set is very large. The only option compatible with the others. \n\nSaving and Resuming\n\nIf write is enabled, evaluate will automatically resume if it finds a partially complete run. \n\ndanger: Only the filename is checked when comparing save data, so it is possible to accidentally merge incompatible evaluations or overwrite complete ones if they are named the same thing!\n\n\nwrite::Bool=false: Whether to write output to files. If true, it will save temporary files, saving each completed fold inside \"$writedir/$(simname)_temp/\", and the final result to \"$writedir/$(simname).jld2\".\nwritedir::String=\"evals\": The directory to save data to.\nsimname::String=\"$(objective)_$(tuning_optimiser)_f=$(nfolds)_cv$(n_cvfolds)_iters=$(tuning_maxiters)\": The simulation name. Used to determine save location.\ndelete_tmps::Bool=length(fold_inds)==nfolds: Whether to delete the temp directory at the end.\n\nLogging\n\nverbosity::Integer=1: Controls how explicit the logging is. 0 for none, 5 for maximimum. This is separate to the verbosity in MPSOptions.\n\nHyperparameter Tuning Options\n\nThese options are passed directly to their corresponding keywords in tune\n\nn_cvfolds::Integer=5: Corresponds to nfolds in tune, number of train/val splits.\nlogspace_eta::Bool=false: Whether to treat the eta parameterspace as logarithmic. E.g. setting parameters=(eta=(10^-3,10^-1) ) and logspace_eta=true will sample each eta_candidate from the log10 search space [-3.,-1.], and then pass eta = 10^(eta_candidate) to MPSOptions. \ninput_supertype::Type=Float64: A numeric type that can represent the types of each hyperparameter being tuned as well as their upper and lower bounds. Typically, Float64 is sufficient, but it can be set to Int for purely discrete optimisation problems etc. This is necessary for mixed Integer / Float hyperparameter tuning because certain solvers in Optimization.jl require variables in the search space to all be the same type.\ntuning_abstol::Float64=1e-3: Passed directly to Optimization.jl: Absolute tolerance in changes to the objective (loss) function. \ntuning_maxiters::Integer=250: Maximum number of iterations allowed when solving.\nprovide_x0::Bool=true: Whether to provide initial conditions to the solve, ignored by MPSRandomSearch. The initial condition will be opts0, unless it contains a hyperparameter outside the range specified by parameters, in which case the lower bound of that hyperparameter will be used.\n\nFurther keyword arguments to evaluate are passed through to tune, and then Optimization.jl through the Optimization.solve function.\n\n\n\n\n\n","category":"function"},{"location":"hyperparameters/#Hyperparameter-tuning-utilities","page":"Hyperparameter Tuning","title":"Hyperparameter tuning utilities","text":"","category":"section"},{"location":"hyperparameters/#MPSTime.make_stratified_cvfolds","page":"Hyperparameter Tuning","title":"MPSTime.make_stratified_cvfolds","text":"make_stratified_cvfolds(\n    Xs::AbstractMatrix, \n    ys::AbstractVector, \n    nfolds::Integer; \n    rng=Union{Integer, AbstractRNG}, \n    shuffle::Bool=true\n) -> folds::Vector{Vector{Vector{Int}}}\n\nCreates nfold-fold stratified cross validation train/validation splits for hyperparameter tuning, with the form:\n\njulia> train_indices_fold_i, validation_indices_fold_i = folds[i];\n\n\nUses MLJs StratifiedCV method. \n\n\n\n\n\n","category":"function"},{"location":"hyperparameters/#MPSTime.eval_loss","page":"Hyperparameter Tuning","title":"MPSTime.eval_loss","text":"eval_loss(\n    ::TuningLoss, \n    mps::TrainedMPS, \n    X_val::AbstractMatrix, \n    y_val::AbstractVector, \n    windows::Union{Nothing, AbstractVector}=nothing;\n    p_fold::Union{Nothing, Tuple}=nothing,\n    distribute::Bool=false,\n    )\n\nEvaluate the TuningLoss of mps on the validation time-series dataset specified by X_val, y_val.\n\np_fold is to allow verbose logging during runs of tune and evaluate. When computing an imputation loss, windows are used to compute imputation losses, as specified in tune, and distribute will distribute the loss calculation across each time-series instance.\n\n\n\n\n\n","category":"function"},{"location":"hyperparameters/#MPSTime.MPSRandomSearch-hyperparameters","page":"Hyperparameter Tuning","title":"MPSTime.MPSRandomSearch","text":"    MPSRandomSearch(sampling::Symbol=:LatinHypercube)\n\nValue type used to specify a random search algorithm for hyperparameter tuning an MPS. \n\nSampling Specifies the method used to determine the search space. The supported sampling methods are \n\n:LatinHypercube: An implementation of LatinHypercubeSampling.jl's random (pseudo-) Latin Hypercubesearch space generator. Supports both discrete and continuous hyperparameters.\n:UniformRandom: Generate a search space by randomly sampling from the interval [lower bound, upper bound] for each hyperparameter. Supports both discrete andcontinuous hyperparameters.\nExhaustive: Perform an exhaustive gridsearch of all hyperparameters within the lower and upper bounds. Only supports discrete hyperparameters. Not actually a random search.  \n\n\n\n\n\n","category":"type"},{"location":"synthdatagen/#Synthetic-Data-Generation","page":"Synthetic Data Generation","title":"Synthetic Data Generation","text":"","category":"section"},{"location":"synthdatagen/","page":"Synthetic Data Generation","title":"Synthetic Data Generation","text":"info: Info\nThis section is still under development. Come back soon!","category":"page"},{"location":"classification/#Classification_top","page":"Classification","title":"Classification","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"This tutorial for MPSTime will take you through the basic steps needed to fit an MPS to a time-series dataset.","category":"page"},{"location":"classification/#nts_demo","page":"Classification","title":"Demo dataset","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"First, import or generate your data. Here, we generate a two class \"noisy trendy sine\" dataset for the sake of demonstration, but if you have a dataset in mind, you can skip to the next section. Our demonstration dataset consists of a sine function with a randomised phase, plus a linear trend, plus some normally distributed noise. Each T-length time series in class c at time t is given by:","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"x^c_t = sinleft(frac2pitaut + psiright) + fracmtT + sigma_c n_t","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"where tau is the period, m is the slope of a linear trend, psi in 0 2pi) is a uniformly random phase offset, sigma_c is the noise scale, and n_t sim mathcalN(01) are  normally distributed random variables. ","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"For the demonstration dataset, the two classes will be generated with different distributions of periods. The class one time series x^1 have tau in12 15, and the class two time series x^2 will havetau in16 19. We'll use sigma_c = 02, and the slope m will be randomly selected from -303.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"We'll set up this dataset using the trendy_sine function from MPSTime.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"using MPSTime, Random\nrng = Xoshiro(1); # fix rng seed\nntimepoints = 100; # specify number of samples per instance\nntrain_instances = 300; # specify num training instances\nntest_instances = 200; # specify num test instances\nX_train = vcat(\n    trendy_sine(ntimepoints, ntrain_instances ÷ 2; sigma=0.1, slope=[-3,0,3], period=(12,15), rng=rng)[1],\n    trendy_sine(ntimepoints, ntrain_instances ÷ 2; sigma=0.1, slope=[-3,0,3], period=(16,19), rng=rng)[1]\n);\ny_train = vcat(\n    fill(1, ntrain_instances ÷ 2),\n    fill(2, ntrain_instances ÷ 2)\n);\nX_test = vcat(\n    trendy_sine(ntimepoints, ntest_instances ÷ 2; sigma=0.2, slope=[-3,0,3], period=(12,15), rng=rng)[1],\n    trendy_sine(ntimepoints, ntest_instances ÷ 2; sigma=0.2, slope=[-3,0,3], period=(16,19), rng=rng)[1]\n);\ny_test = vcat(\n    fill(1, ntest_instances ÷ 2),\n    fill(2, ntest_instances ÷ 2)\n);\nnothing # hide","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"using Plots\np1 = plot(X_train[1:30,:]'; colour=\"blue\", alpha=0.5, legend=:none, title=\"Class 1\");\np2 = plot(X_train[end-30:end,:]'; colour=\"blue\", alpha=0.5, legend=:none, title=\"Class 2\");\nplot(p1,p2)\nsavefig(\"./figs_generated/classification_classes.svg\") # hide\nnothing # hide","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"(Image: )","category":"page"},{"location":"classification/#classification_training","page":"Classification","title":"Training an MPS","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"warning: Floating Point Error\nDepending on the dataset, the results of fitMPS can be noticeably affected by what machine it is running on. If you're trying to replicate these tutorials, expect a classification uncertainty of 1-2% (the noisy trendy sine can be something of an extreme case). You can resolve this by either using higher precision computing (pass dtype=BigFloat or Complex{BigFloat} to MPSOptions), or use the evaluate function to resample  your data and average the result. This is generally not significant for scientific computing applications as for real word datasets, the floating point error of up to a few percent is much less than the resampling error caused by choosing different train/test splits.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"To train an MPS on your dataset, first, set up the hyperparameters (see Hyperparameters):","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"opts = MPSOptions(); # calling this with no arguments gives default hyperparameters\nprint_opts(opts; long=false); # pretty print the options table","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"and then pass the data and hyperparameters to the fitMPS function:","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"julia> mps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, opts);","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"details: output collapsed\nmps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, opts);","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"fitMPS doesn't use X_test or y_test for anything except printing performance evaluations, so it is safe to leave them blank. For unsupervised learning, input a dataset with only one class, or only pass X_train ( y_train has a default value of zeros(Int, size(X_train, 1)) ).","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"The mps::TrainedMPS can be passed directly to classify for classification, or init_imputation_problem to set up an imputation problem. info provides a short training summary, which can be pretty-printed with the sweep_summary function.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"You can use also test_states to print a summary of the MPS performance on the test set.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"get_training_summary(mps, test_states; print_stats=true);   \n","category":"page"},{"location":"classification/#c_hparams","page":"Classification","title":"Hyperparameters","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"There are number of hyperparameters and data preprocessing opttrendy_sine ions that can be specified using MPSOptions(; key=value)","category":"page"},{"location":"classification/#MPSTime.MPSOptions","page":"Classification","title":"MPSTime.MPSOptions","text":"MPSOptions(; <Keyword Arguments>)\n\nSet the hyperparameters and other options for fitMPS. \n\nFields:\n\nLogging\n\nverbosity::Int=1: How much debug/progress info to print to the terminal while optimising the MPS. Higher numbers mean more output\nlog_level::Int=3: How much statistical output. 0 for nothing, >0 to print losses, accuracies, and confusion matrix at each step. Noticeable computational overhead \ntrack_cost::Bool=false: Whether to print the cost at each Bond tensor site to the terminal while training, mostly useful for debugging new cost functions or optimisers (HUGE computational overhead)\n\nMPS Training Hyperparameters\n\nnsweeps::Int=5: Number of MPS optimisation sweeps to perform (One sweep is both forwards and Backwards)\nchi_max::Int=25: Maximum bond dimension allowed within the MPS during the SVD step\neta::Float64=0.01: The learning rate. For gradient descent methods, this is the step size. For Optim and OptimKit this serves as the initial step size guess input into the linesearch\nd::Int=5: The dimension of the feature map or \"Encoding\". This is the true maximum dimension of the feature vectors. For a splitting encoding, d = numsplits * auxbasis_dim\ncutoff::Float64=1E-10: Size based cutoff for the number of singular values in the SVD (See Itensors SVD documentation)\ndtype::DataType=Float64 or ComplexF64 depending on encoding: The datatype of the elements of the MPS. Supports the arbitrary precsion types such as BigFloat and Complex{BigFloat}\nexit_early::Bool=false: Stops training if training accuracy is 1 at the end of any sweep.\n\nEncoding Options\n\nencoding::Symbol=:Legendre: The encoding to use, including :Stoudenmire, :Fourier, :Legendre, :SLTD, :Custom, etc. see Encoding docs for a complete list. Can be just a time (in)dependent orthonormal basis, or a time (in)dependent basis mapped onto a number of \"splits\" which distribute tighter basis functions where the sites of a timeseries are more likely to be measured.  \nprojected_basis::Bool=false: Whether to project a basis onto the training data at each time. Normally, when specifying a basis of dimension d, the first d lowest order terms are used. When project=true, the training data is used to construct a pdf of the possible timeseries amplitudes at each time point. The first d largest terms of this pdf expanded in a series are used to select the basis terms.\naux_basis_dim::Int=2: Unused for standard encodings. If the encoding is a SplitBasis, serves as the auxilliary dimension of a basis mapped onto the split encoding, so that the number of histogram bins = d / auxbasisdim. \nencode_classes_separately::Bool=false: Only relevant for data driven bases. If true, then data is split up by class before being encoded. Functionally, this causes the encoding method to vary depending on the class\n\nData Preprocessing and MPS initialisation\n\nsigmoid_transform::Bool: Whether to apply a sigmoid transform to the data before minmaxing. This has the form\n\nboldsymbolX = left(1 + exp-fracboldsymbolX-m_boldsymbolXr_boldsymbolX  135right)^-1\n\nwhere boldsymbolX is the un-normalized time-series data matrix, m_boldsymbolX is the median of boldsymbolX and r_boldsymbolXis its interquartile range.\n\nminmax::Bool: Whether to apply a minmax norm to [0,1] before encoding. This has the form\n\nboldsymbolX =  fracboldsymbolX - x_textminx_textmax - x_textmin\n\nwhere boldsymbolX is the scaled robust-sigmoid transformed data matrix, x_textmin and x_textmax are the minimum and maximum of boldsymbolX.\n\ndata_bounds::Tuple{Float64, Float64} = (0.,1.): The region to bound the data to if minmax=true. This is separate from the encoding domain. All encodings expect data to be scaled scaled between 0 and 1. Setting the data bounds a bit away from [0,1] can help when your basis has poor support near its boundaries.\ninit_rng::Int: Random seed used to generate the initial MPS\nchi_init::Int: Initial bond dimension of the random MPS\n\nLoss Functions and Optimisation Methods\n\nloss_grad::Symbol=:KLD: The type of cost function to use for training the MPS, typically Mean Squared Error (:MSE) or KL Divergence (:KLD), but can also be a weighted sum of the two (:Mixed) if uselegacyITensor is enabled.\nbbopt::Symbol=:TSGO: Which local Optimiser to use, builtin options are symbol gradient descent (:GD), or gradient descent with a TSGO rule (:TSGO). If use_legacy_ITensor` is enabled, can be a Conjugate Gradient descent optimisation rule using either the Optim or OptimKit package (:Optim or :OptimKit respectively). The CGD methods work well for MSE based loss functions, but seem to perform poorly for KLD base loss functions.\nrescale::Tuple{Bool,Bool}=(false,true): Has the form rescale = (before::Bool, after::Bool). Where to enforce the normalisation of the MPS during training, either calling normalise!(Bond Tensor) before or after BT is updated. Note that for an MPS that starts in canonical form, rescale = (true,true) will train identically to rescale = (false, true) but may be less performant.\nupdate_iters::Int=1: Maximum number of optimiser iterations to perform for each bond tensor optimisation. E.G. The number of steps of (Conjugate) Gradient Descent used by TSGO, Optim or OptimKit\ntrain_classes_separately::Bool=false: Whether the the trainer optimises the total MPS loss over all classes or whether it considers each class as a separate problem. Should make very little diffence\nuse_legacy_ITensor::Bool=false: Whether to use the old, slow (but possibly easier to understand) ITensor Implementation\nsvd_alg::String=\"divide_and_conquer\": SVD Algorithm to pass to ITensor\n\n`\n\nDebug\n\nreturn_encoding_meta_info::Bool=false: Debug flag: Whether to return the normalised data as well as the histogram bins for the splitbasis types\n\n\n\n\n\nMPSOptions(params::NamedTuple) -> MPSOptions\n\nConvert the named tuple params with format (:option1=value1, :option2=value2,...) to an MPSOptions object.\n\n\n\n\n\n","category":"type"},{"location":"classification/#Classification","page":"Classification","title":"Classification","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"To predict the class of unseen data, use the classify function.","category":"page"},{"location":"classification/#MPSTime.classify-Tuple{TrainedMPS, AbstractMatrix}","page":"Classification","title":"MPSTime.classify","text":"classify(mps::TrainedMPS, X_test::AbstractMatrix)) -> (predictions::Vector)\n\nUse the mps to predict the class of the rows of X_test by computing the maximum overlap.\n\nExample\n\njulia> W, info, test_states = fitMPS( X_train, y_train);\n\njulia> preds  = classify(W, X_test); # make some predictions\n\njulia> mean(preds .== y_test)\n0.9504373177842566\n\n\n\n\n\n","category":"method"},{"location":"classification/","page":"Classification","title":"Classification","text":"For example, for the noisy trendy sine from earlier:","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"predictions = classify(mps, X_test);\nusing StatsBase\nmean(predictions .== y_test)","category":"page"},{"location":"classification/#Training-with-a-custom-basis","page":"Classification","title":"Training with a custom basis","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"To train with a custom basis, first, declare a custom basis with function_basis, and pass it in as the last argument to fitMPS. For this to work, the encoding hyperparameter must be set to :Custom in MPSOptions","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"using LegendrePolynomials\nfunction legendre_encode(x::Float64, d::Int)\n    # default legendre encoding: choose the first n-1 legendre polynomials\n\n    leg_basis = [Pl(x, i; norm = Val(:normalized)) for i in 0:(d-1)] \n    \n    return leg_basis\nend\ncustom_basis = function_basis(legendre_encode, false, (-1., 1.))","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"julia> mps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, MPSOptions(; encoding=:Custom), custom_basis);","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"details: output collapsed\nmps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, MPSOptions(; encoding=:Custom), custom_basis);","category":"page"},{"location":"classification/#Docstrings","page":"Classification","title":"Docstrings","text":"","category":"section"},{"location":"classification/#MPSTime.trendy_sine","page":"Classification","title":"MPSTime.trendy_sine","text":"trendy_sine(T::Int, n::Int; period=nothing, slope=nothing, phase=nothing, sigma=0.0, \n    rng=Random.GLOBAL_RNG, return_metadata=true) -> Tuple{Matrix{Float64}, Dict{Symbol, Any}}\n\nGenerate n time series of length T, each composed of a sine wave with an optional linear trend and Gaussian noise defined by the equation:\n\nx_t = sinleft(frac2pitaut + psiright) + fracm tT + sigma n_t\n\nwith period tau, time point t, linear trend slope m, phase offset psi, noise scale sigma and n_t sim mathcalN(01)\n\nArguments\n\nT::Int: Length of each time series\nn::Int: Number of time series instances to generate\n\nKeyword Arguments\n\nperiod: Period of the sinusoid, τ\nnothing: Random values between 1.0 and 50.0 (default)\nFloat64: Fixed period for all time series\nTuple: Bounds for uniform random values, e.g., (1.0, 20.0) → τ ~ U(1.0, 20.0)\nVector: Sample from discrete uniform distribution, e.g., τ ∈ 10, 20, 30\nslope: Linear trend gradient, m\nnothing: Random values bewteen -5.0 and 5.0 (default)\nFloat64: Fixed slope for all time series\nTuple: Bounds for uniform random values, e.g., (-3.0, 3.0) → m ~ U(-3.0, 3.0)\nVector: Sample from discrete uniform distribution, e.g., m ∈ -3.0, 0.0, 3.0\nphase: Phase offset, ψ\nnothing: Random values between 0 and 2π (default)\nFloat64: Fixed phase for all time series\nTuple: Bounds for uniform random values, e.g., (0.0, π) → ψ ~ U(0.0, π)\nVector: Sample from discrete uniform distribution\nsigma::Real: Standard deviation of Gaussian noise, σ (default: 0.0)\nrng::AbstractRNG: Random number generator for reproducibility (default: Random.GLOBAL_RNG)\nreturn_metadata::Bool: Return generation parameters (default: true)\n\nReturns\n\nMatrix{Float64} of shape (n, T)\nDictionary of generation parameters (:period, :slope, :phase, :sigma, :T, :n)\n\n\n\n\n\n","category":"function"},{"location":"classification/#MPSTime.fitMPS-Tuple{Matrix, Vector, Matrix, Vector, MPSOptions, Nothing}","page":"Classification","title":"MPSTime.fitMPS","text":"fitMPS(X_train::AbstractMatrix, \n       y_train::AbstractVector=zeros(Int, size(X_train, 1)), \n       X_test::AbstractMatrix=zeros(0,0), \n       y_test::AbstractVector=zeros(Int, 0), \n       opts::AbstractMPSOptions=MPSOptions(),\n       custom_encoding::Union{Encoding, Nothing}=nothing) -> (MPS::TrainedMPS, training_info::Dict, encoded_test_states::EncodedTimeSeriesSet)\n\nTrain an MPS on the data X_train using the hyperparameters opts, see MPSOptions. The number of classes are determined by the entries of y_train.\n\nReturns a trained MPS, a dictionary containing training info, and the encoded test states. X_test and y_test are used only to print performance evaluations, and may be empty. The custom_encoding argument allows the use of user defined custom encodings, see function_basis. This requires that encoding=:Custom is specified in MPSOptions\n\nNOTE: the return value encoded_test_states will be sorted by class, so predictions shouldn't be compared directly to y_test. \n\nSee also: Encoding\n\nExample\n\nSee ??fitMPS to for a more verbose example\n\njulia> opts = MPSOptions(; d=5, chi_max=30, encoding=:Legendre, eta=0.05);\n\njulia> print_opts(opts) # Prints options as a table\n[...]\n\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\nGenerating initial weight MPS with bond dimension χ_init = 4\n        using random state 1234.\nInitialising train states.\nUsing 1 iterations per update.\nTraining KL Div. 28.213032851945012 | Training acc. 0.31343283582089554.\nUsing optimiser CustomGD with the \"TSGO\" algorithm\nStarting backward sweeep: [1/5]\n[...]\n\nStarting forward sweep: [5/5]\nFinished sweep 5. Time for sweep: 0.76s\nTraining KL Div. -12.577920427063361 | Training acc. 1.0.\n\nMPS normalised!\n\nTraining KL Div. -12.57792042706337 | Training acc. 1.0.\nTest KL Div. -9.815236609211746 | Testing acc. 0.9504373177842566.\n\nTest conf: [497 16; 35 481].\n\n\nExtended help\n\njulia> Using JLD2 # load some data\n\njulia> dloc = \"your_data_path/data.jld2\";\n\njulia> f = jldopen(dloc, \"r\") \n           X_train = read(f, \"X_train\")\n           y_train = read(f, \"y_train\")\n           X_test = read(f, \"X_test\")\n           y_test = read(f, \"y_test\")\n       close(f);\n\njulia> opts = MPSOptions(; d=5, chi_max=30, encoding=:Legendre, eta=0.05);\n\njulia> print_opts(opts) # Prints options as a table\n[...]\n\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\nGenerating initial weight MPS with bond dimension χ_init = 4\n        using random state 1234.\nInitialising train states.\nUsing 1 iterations per update.\nTraining KL Div. 28.213032851945012 | Training acc. 0.31343283582089554.\nUsing optimiser CustomGD with the \"TSGO\" algorithm\nStarting backward sweeep: [1/5]\n[...]\n\nStarting forward sweep: [5/5]\nFinished sweep 5. Time for sweep: 0.76s\nTraining KL Div. -12.577920427063361 | Training acc. 1.0.\n\nMPS normalised!\n\nTraining KL Div. -12.57792042706337 | Training acc. 1.0.\nTest KL Div. -9.815236609211746 | Testing acc. 0.9504373177842566.\n\nTest conf: [497 16; 35 481].\n\njulia> get_training_summary(W, test_states; print_stats=true);\n         Overlap Matrix\n┌──────┬───────────┬───────────┐\n│      │   |ψ0⟩    │   |ψ1⟩    │\n├──────┼───────────┼───────────┤\n│ ⟨ψ0| │ 5.074e-01 │ 1.463e-02 │\n├──────┼───────────┼───────────┤\n│ ⟨ψ1| │ 1.463e-02 │ 4.926e-01 │\n└──────┴───────────┴───────────┘\n          Confusion Matrix\n┌──────────┬───────────┬───────────┐\n│          │ Pred. |0⟩ │ Pred. |1⟩ │\n├──────────┼───────────┼───────────┤\n│ True |0⟩ │       497 │        16 │\n├──────────┼───────────┼───────────┤\n│ True |1⟩ │        35 │       481 │\n└──────────┴───────────┴───────────┘\n┌───────────────────┬───────────┬──────────┬──────────┬─────────────┬──────────┬───────────┐\n│ test_balanced_acc │ train_acc │ test_acc │ f1_score │ specificity │   recall │ precision │\n│           Float64 │   Float64 │  Float64 │  Float64 │     Float64 │  Float64 │   Float64 │\n├───────────────────┼───────────┼──────────┼──────────┼─────────────┼──────────┼───────────┤\n│          0.950491 │       1.0 │ 0.950437 │ 0.950425 │    0.950491 │ 0.950491 │  0.951009 │\n└───────────────────┴───────────┴──────────┴──────────┴─────────────┴──────────┴───────────┘\n\njulia> sweep_summary(info)\n┌────────────────┬──────────┬───────────────┬───────────────┬───────────────┬───────────────┬───────────────┬────────────┬──────────┐\n│                │ Initial  │ After Sweep 1 │ After Sweep 2 │ After Sweep 3 │ After Sweep 4 │ After Sweep 5 │ After Norm │   Mean   │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│ Train Accuracy │ 0.313433 │      1.0      │      1.0      │      1.0      │      1.0      │      1.0      │    1.0     │   1.0    │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│  Test Accuracy │ 0.409135 │   0.947522    │   0.951409    │   0.948494    │   0.948494    │   0.950437    │  0.950437  │ 0.949271 │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│  Train KL Div. │  28.213  │   -11.7855    │    -12.391    │   -12.4831    │   -12.5466    │   -12.5779    │  -12.5779  │ -12.3568 │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│   Test KL Div. │ 27.7435  │   -9.12893    │   -9.73479    │   -9.79248    │    -9.8158    │   -9.81524    │  -9.81524  │ -9.65745 │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│     Time taken │   0.0    │   0.658366    │    0.75551    │   0.719035    │   0.718444    │    1.16256    │    NaN     │ 0.802783 │\n└────────────────┴──────────┴───────────────┴───────────────┴───────────────┴───────────────┴───────────────┴────────────┴──────────┘\n\n\n\n\n\n\n","category":"method"},{"location":"classification/#MPSTime.sweep_summary-Tuple{Union{Nothing, IO}, Any}","page":"Classification","title":"MPSTime.sweep_summary","text":"sweep_summary([io::IO], info)\n\nPrint a pretty summary of what happened in every sweep\n\n\n\n\n\n","category":"method"},{"location":"classification/#MPSTime.get_training_summary-Tuple{IO, TrainedMPS, EncodedTimeSeriesSet}","page":"Classification","title":"MPSTime.get_training_summary","text":"get_training_summary(\n    [io::IO],\n    mps::TrainedMPS, \n    test_states::EncodedTimeSeriesSet;  \n    print_stats::Bool=false\n    ) -> stats::Dict\n\nPrint a summary of the training process of mps, with performane evaluated on test_states.\n\n\n\n\n\n","category":"method"},{"location":"classification/#MPSTime.print_opts","page":"Classification","title":"MPSTime.print_opts","text":"print_opts([io::IO], opts::AbstractMPSOptions; long::Bool=false)\n\nPrint the MPSOptions struct in a table. Summarises (long=false) by default.\n\n\n\n\n\n","category":"function"},{"location":"encodings/#Encodings","page":"Encodings","title":"Encodings","text":"","category":"section"},{"location":"encodings/#Overview","page":"Encodings","title":"Overview","text":"","category":"section"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"To use MPS methods on time-series data, the continuous time-series amplitudes must be mapped to MPS compatible vectors using an encoding. There are a number of encodings built into this library, and they can be specified by the encoding keyword in MPSOptions.","category":"page"},{"location":"encodings/#MPSTime.Encoding","page":"Encodings","title":"MPSTime.Encoding","text":"Encoding\n\nAbstract supertype of all encodings. To specify an encoding for MPS training, set the encoding keyword when calling MPSOptions.\n\nExample\n\njulia> opts = MPSOptions(; encoding=:Legendre);\n\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\n\nEncodings\n\n:Legendre: The first d L2-normalised Legendre Polynomials. Real valued, and supports passing projected_basis=true to MPSOptions.\n:Fourier: Complex valued Fourier coefficients. Supports passing projected_basis=true to MPSOptions.\n\n    Phi(x d) = left1 + 0i e^i pi x e^-i pi x e^2i pi x e^-2i pi x ldots right  sqrtd \n\n:Stoudenmire: The original complex valued \"Spin-1/2\" encoding from Stoudenmire & Schwab, 2017 arXiv. Only supports d = 2\n\n    Phi(x) = left e^3 i pi x  2 cos(fracpi2 x)  e^-3 i pi x  2 sin(fracpi2 x)right\n\n:Sahand_Legendre_Time_Dependent:  (:SLTD) A custom, real-valued encoding constructed as a data-driven adaptation of the Legendre Polynomials. At each time point, t, the training data is used to construct a probability density function that describes the distribution of the time-series amplitude x_t. This is the first basis function. \nb_1(x t) = textpdf_x_t(x_t). This is computed with KernelDensity.jl:\n\njulia> Using KernelDensity\n\njulia> xs_samps = range(-1,1, max(200,size(X_train,2)));\n\njulia> b1(xs,t) = pdf(kde(X_train[t,:]), xs_samps);\n\nThe second basis function is the first order polynomial that is L2-orthogonal to this pdf on the interval [-1,1]. \n\nb_2(xt) = a_1 x + a_0 text where  int_-1^1 b_1(xt) b_2^*(x t) textrmd x = 0  lvertlvert b_2(x t) rvertrvert_L2 = 1\n\nThe third basis function is the second order polynomial that is L2-orthogonal to the first two basis functions on [-1,1], etc.\n\n-:Custom: For use with user-defined custom bases. See function_basis\n\n\n\n\n\n","category":"type"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Encodings can be visualized with the plot_encoding function.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"using MPSTime, Plots\nbasis, p = plot_encoding(:legendre, 4)\nsavefig(p[1], \"./figs_generated/encodings/leg.svg\") # hide\nnothing # hide","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"For data-driven bases, the data histograms can be plotted alongside for reference:","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"# Generate the  Noisy Trendy Sine dataset from the imputation section\nusing Random \nrng = Xoshiro(1) # fix rng seed\nntimepoints = 100 # specify number of samples per instance.\nntrain_instances = 600 # specify num training instances\nntest_instances = 300 # specify num test instances\nX_train = trendy_sine(ntimepoints, ntrain_instances; sigma=0.2, slope=3, period=15, rng=rng)[1]\nX_test = trendy_sine(ntimepoints, ntest_instances; sigma=0.2, slope=3, period=15, rng=rng)[1]\n\n\n# Plot a data-driven basis\nbasis, p = plot_encoding(:sahand_legendre_time_dependent, 4, X_train; tis=[1,20]);\n\nsavefig(p[1], \"./figs_generated/encodings/SLTD.svg\") # hide\nnothing # hide","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/#Using-a-SplitBasis-encoding","page":"Encodings","title":"Using a SplitBasis encoding","text":"","category":"section"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"One way to increase the encoding dimension is to repeat a basis many times across the encoding domain in 'bins'. In theory, this can be advantageous when data is concentrated in narrow regions in the encoding domain, as very fine bins can be used to reduce encoding error in well-populated regions, while computational effort can be saved with wide bins in sparsely-population regions. To this end, we provide the \"Split\" bases.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"The uniform-split encoding, which simply bins data up as a proof of concept:","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(uniform_split(:legendre), 8, X_train; tis=[1,20], aux_basis_dim=4)\nsavefig(p[1], \"./figs_generated/encodings/usplit.svg\") # hide\nnothing #hide","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"And the histogram-split encoding, which narrows the bins in frequently occurring regions.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(histogram_split(:legendre), 8, X_train; tis=[1,20], aux_basis_dim=4)\nsavefig(p[1], \"./figs_generated/encodings/hsplit.svg\") # hide\nnothing # hide","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Every data-independent encoding can be histogram split and uniform split, including other split bases:","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(\n    histogram_split(uniform_split(:legendre)), \n    16, \n    X_train;\n     tis=[1,20], \n     aux_basis_dim=8, \n     size=(1600,900)\n)\nsavefig(p[1], \"./figs_generated/encodings/husplit.svg\") # hide\nnothing # hide","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/#Custom-encodings","page":"Encodings","title":"Custom encodings","text":"","category":"section"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Custom encodings can be declared using function_basis.","category":"page"},{"location":"encodings/#MPSTime.function_basis","page":"Encodings","title":"MPSTime.function_basis","text":"function_basis(basis::Function, is_complex::Bool, range::Tuple{<:Real,<:Real}, <args>; name::String=\"Custom\")\n\nConstructs a time-(in)dependent encoding from the function basis, which is either real or complex, and has support on the interval range.\n\nFor a time independent basis, the input function must have the signature :\n\nbasis(x::Float64, d::Integer, init_args...)\n\nand return a d-dimensional numerical Vector.  A vector x_1 x_2 x_3  x_N will be encoded as b(x_1) b(x_2) b(x_3) b(x_N)\n\nTo use a time dependent basis, set is_time_dependent to true. The input function must have the signature \n\nbasis(x::Float64, d::Integer, ti::Int, init_args...)\n\nand return a d-dimensional numerical Vector. A vector x_1 x_2 x_3  x_N will be encoded as  b_1(x_1) b_2(x_2) b_3(x_3) b_N(x_N)\n\nOptional Arguments\n\nis_time_dependent::Bool=false: Whether the basis is time dependent \nis_data_driven::Bool=false: Whether functional form of the basis depends on the training data\ninit::Function=no_init: The initialiser function for the basis. This is used to compute arguments for the function that are not known in advance,\n\nfor example, the polynomial coefficients for the Sahand-Legendre basis. This function should have the form\n\ninit_args = opts.encoding.init(X_normalised::AbstractMatrix, y::AbstractVector; opts::MPSTime.Options=opts)\n\nX_normalised will be preprocessed (with sigmoid transform and MinMax transform pre-applied), with Time series as columns\n\nExample\n\nThe Legendre Polynomial Basis:\n\nusing MPSTime, LegendrePolynomials\nfunction legendre_encode(x::Float64, d::Int)\n    # default legendre encoding: choose the first n-1 legendre polynomials\n\n    leg_basis = [Pl(x, i; norm = Val(:normalized)) for i in 0:(d-1)] \n    \n    return leg_basis\nend\ncustom_basis = function_basis(legendre_encode, false, (-1., 1.))\n\n\n\n\n\n\n","category":"function"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"To use a custom encoding, you must manually pass it into fitMPS.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"\n# Declare a 'custom basis'\nusing LegendrePolynomials\nfunction legendre_encode(x::Float64, d::Int)\n    # default legendre encoding: choose the first n-1 legendre polynomials\n\n    leg_basis = [Pl(x, i; norm = Val(:normalized)) for i in 0:(d-1)] \n    \n    return leg_basis\nend\ncustom_basis = function_basis(legendre_encode, false, (-1., 1.))","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"julia> mps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, MPSOptions(; encoding=:Custom), custom_basis);","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"details: output collapsed\nmps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, MPSOptions(; encoding=:Custom), custom_basis);","category":"page"},{"location":"encodings/#MPSTime.plot_encoding-Tuple{Symbol, Integer}","page":"Encodings","title":"MPSTime.plot_encoding","text":"plot_encoding(E::Union(Symbol, MPSTime.Encoding), \n              d::Integer, \n              X_train::Matrix{Float64}=zeros(0,0), \n              y_train::Vector{Any}=[];\n              <keyword arguments>) -> encoding::Vector, plots::Vector{Plots.plot}\n\nPlot the first d terms of the encoding E across its entire domain, X_train and y_train are only needed if E is data driven or time dependent, or plot_hist is true.\n\nReturn Values\n\nencoding::Vector: The value of the encoding computed at each time point.\nplots::Vector{Plots.plot} A vector of plots, containing either a summary figure, or a summary figure followed by every subplot if return_subplots=true.\n\nKeyword Arguments\n\nplot_hist::Bool=E.isdatadriven: Whether to plot the histogram of the traing data at several time points. Useful for understanding the behviour of data-driven bases.\ntis::Vector{<:Integer} = Int[]: Time(s) to plot the Encoding at.\nds::Vector{<:Integer} = collect(1:d): Enables plotting of a subset of a d-dimensional Encoding, e.g. ds=[1,3,5] plots the first, third and fifth basis functions.\nnum_xvals::Integer=500: Number of points to compute each basis function at.\nsize::Tuple=(1200, 800): Size of the summaryt figure in points (passed directly to plot).\npadding::Real=6.: Size of the padding between the plots and the margins in the summary figure (in mm).\n\nUsed for data-driven Encodings\n\nsigmoid_transform::Bool=false: Whether to apply a robust sigmoid transform to the training data, see MPSOptions.\nminmax::Bool=true: Whether to apply a minmax normalsation to the training data after the sigmoid, see MPSOptions.\ndata_bounds::Tuple{<:Real, <:Real}=(0.,1.): Whether to apply a robust sigmoid transform to the X data, see MPSOptions.\nproject_basis::Bool=false: Whether to project the basis onto the data. Supported only by :Legendre and :Fourier basis when E has type Symbol.\naux_basis_dim::Integer=2: Dimension of each auxilliary basis. Only relevent when E is a MPSTime.SplitBasis. \nreturn_subplots::Bool=false: Wheter to return all of the subplots alongside the summary figure.W\nkwargs: Passed to Plots.Plot()\n\n\n\n\n\n","category":"method"},{"location":"imputation/#Imputation_top","page":"Imputation","title":"Imputation","text":"","category":"section"},{"location":"imputation/#Overview","page":"Imputation","title":"Overview","text":"","category":"section"},{"location":"imputation/#Imputation-Scenarios","page":"Imputation","title":"Imputation Scenarios","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"MPSTime supports univariate time-series imputation with three key patterns of missing data:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Individual missing points (e.g., values missing at t = 10 20 80)\nSingle contiguous blocks (e.g., values missing from t = 25-70)\nMultiple contiguous blocks (e.g., values missing from t = 5-10, t = 25-50 and t = 80-90)","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"MPSTime can also handle any combination of these patterns. For instance, you might need to impute a single contiguous block from t = 10-30, plus individual missing points at t = 50 and t=80.","category":"page"},{"location":"imputation/#Setup","page":"Imputation","title":"Setup","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"The first step is to train an MPS. Here, we'll train an MPS in an unsupervised manner (no class labels) using an adapted version of the noisy trendy sinusoid dataset from the Classification tutorial.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"using MPSTime, Random \nrng = Xoshiro(1); # fix rng seed\nntimepoints = 100; # specify number of samples per instance.\nntrain_instances = 300; # specify num training instances\nntest_instances = 200; # specify num test instances\nX_train = trendy_sine(ntimepoints, ntrain_instances; sigma=0.2, slope=3, period=15, rng=rng)[1];\nX_test = trendy_sine(ntimepoints, ntest_instances; sigma=0.2, slope=3, period=15, rng=rng)[1];\n# hyper parameters and training\nopts = MPSOptions(\n    d=12, \n    chi_max=40, \n    sigmoid_transform=false # disabling preprocessing generally improves imputation performance.\n) \nmps, info, test_states = fitMPS(X_train, opts);","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Next, we initialize an imputation problem. This does a lot of necessary pre-computation:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"imp = init_imputation_problem(mps, X_test)\nnothing # hide\n","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"A summary of the imputation problem setup is printed to verify the model parameters and dataset information. For multi-class data, you can pass y_test to init_imputation_problem in order to exploit the labels / class information while doing imputation.","category":"page"},{"location":"imputation/#Imputing-missing-values","page":"Imputation","title":"Imputing missing values","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"warning: Floating Point Error\nDepending on the dataset, the results of fitMPS can be noticeably affected by what machine it is running on. If you're trying to replicate these tutorials, expect an average uncertainty of 1-2%. In rare cases, it can go up to 5%-10% for single imputation windows in particularly large datasets. We strongly recommend either using higher precision computing (pass dtype=BigFloat or Complex{BigFloat} to MPSOptions), or the evaluate function to resample your data and average the result. This is generally not significant for scientific computing applications as for real word datasets, the floating point error of up to a few percent is much less than the resampling error caused by choosing different train/test splits.","category":"page"},{"location":"imputation/#Single-block-Imputation","page":"Imputation","title":"Single-block Imputation","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Now, decide how you want to impute the missing data. The necessary options are:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"class::Integer: The class of the time-series instance we are going to impute, leave as zero for \"unlabelled\" data (i.e., all data belong to the same class).\nimpute_sites: The MPS sites (time points) that are missing (inclusive).\ninstance_idx: The time-series instance from the chosen class in the test set.\nmethod: The imputation method to use. Can be trajectories (ITS), median, mode, mean, etc...","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"In this example, we will consider a single block of contiguous missing values, simulated from a missing-at-random mechanism (MAR). We will use the median to impute the missing values, as well as computing a 1-Nearest Neighbor Imputation (1-NNI) benchmark for comparison:   ","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"rng = Xoshiro(42) # Fix RNG\nclass = 0\npm = 0.8 # 80% missing data\ninstance_idx = 1 # pick a time series instance in test set\n_, impute_sites = mar(X_test[instance_idx, :], pm; rng=rng) # simulate MAR mechanism\nmethod = :median\n\nimputed_ts, pred_err, target_ts, stats, plots = MPS_impute(\n    imp,\n    class, \n    instance_idx, \n    impute_sites, \n    method; \n    NN_baseline=true, # whether to also do a baseline imputation using the (first) Nearest Neighbour benchmark\n    plot_fits=true, # whether to plot the fits\n    get_wmad=true # when method=:median, this uses the Weighted Median Absolute Deviation (WMAD) to compute the prediction error.\n);\n\n# Pretty-print the stats\nusing PrettyTables\npretty_table(stats[1]; header=[\"Metric\", \"Value\"], header_crayon=crayon\"yellow bold\", tf=tf_unicode_rounded);","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Several outputs are returned from MPS_impute:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"imputed_ts: The imputed time-series instance, containing the original data points and the predicted values.\npred_err: The prediction error for each imputed value, given a known ground-truth.\ntarget_ts: The original time-series instance containing missing values.\nstats: A collection of statistical metrics (MAE and MAPE) evaluating imputation performance with respect to a ground truth. Includes benchmark performance when NN_baseline=true.\nplots: Stores plot object(s) in an array for visualization when plot_fits=true.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"The MAE and MAPE in the 'stats' table are the Mean Absolute Error and Mean Absolute Percentage Error for the MPS prediction, while the NN_ prefix corresponds to the same errors for the 1-Nearest Neighbours benchmark. In this case, MAPE is an unreliable measure of error as the data goes through zero. ","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"To plot the imputed time series, we can call the plot function as follows: ","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"using Plots\nplots[1]\nsavefig(\"figs_generated/imputation/median_impute_1.svg\") # hide\nnothing # hide","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"The solid orange line depicts the \"ground-truth\" (observed) time-series values, the dotted blue line is the MPS-imputed data points and the dotted red line is the 1-NN benchmark. The blue shading indicates the uncertainty due to encoding error.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"There are a lot of other options, and many more imputation methods to choose from! See MPS_impute for more details.","category":"page"},{"location":"imputation/#Multi-block-Imputation","page":"Imputation","title":"Multi-block Imputation","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Building on the previous example of single-block imputation, MPSTime can also be used to impute missing values in multiple blocks of contiguous points.  For example, consider missing points between t = 10-25, t = 40-60 and t = 75-90:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"class = 0\nimpute_sites = vcat(collect(10:25), collect(40:60), collect(65:90))\ninstance_idx = 32\nmethod = :median\n\nimputed_ts, pred_err, target_ts, stats, plots = MPS_impute(\n    imp,\n    class, \n    instance_idx, \n    impute_sites, \n    method; \n    NN_baseline=true, # whether to also do a baseline imputation using the (first) Nearest Neighbour benchmark\n    plot_fits=true, # whether to plot the fits\n    get_wmad=true,\n);\npretty_table(stats[1]; header=[\"Metric\", \"Value\"], header_crayon=crayon\"yellow bold\", tf=tf_unicode_rounded);","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"plots[1]\nsavefig(\"figs_generated/imputation/median_impute_nblocks.svg\") # hide\nnothing # hide","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/#Individual-Point-Imputation","page":"Imputation","title":"Individual Point Imputation","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"To impute individual points rather than ranges of consecutive points (blocks), we can simply pass their respective time points into the imputation function as a vector:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"impute_sites = [10]; # only impute t = 10\nimpute_sites = [10, 25, 50]; # impute multiple individual points","category":"page"},{"location":"imputation/#Plotting-Trajectories","page":"Imputation","title":"Plotting Trajectories","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"To plot individual trajectories from the conditional distribution, use method=:ITS.  Here, we'll plot 10 randomly selected trajectories for the missing points by setting the num_trajectories keyword: ","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"class = 0\nimpute_sites = collect(10:90)\ninstance_idx = 59\nmethod = :ITS\n\nimputed_ts, pred_err, target_ts, stats, plots = MPS_impute(\n    imp,\n    class, \n    instance_idx, \n    impute_sites, \n    method; \n    NN_baseline=false, # whether to also do a baseline imputation using 1-NN\n    plot_fits=true, # whether to plot the fits\n    num_trajectories=10, # number of trajectories to plot\n    rejection_threshold=2.5 # limits how unlikely we allow the random trajectories to be.\n    # there are more options! see [`MPS_impute`](@ref)\n);\n\nstats","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"plots[1]\nsavefig(\"./figs_generated/imputation/ITS_impute.svg\") # hide\nnothing #hide","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/#Plotting-cumulative-distribution-functions","page":"Imputation","title":"Plotting cumulative distribution functions","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"It can be interesting to inspect the probability distribution being sampled from at each missing time point.  To enable this, we provide the get_cdfs function, which works very similarly to MPS_impute, only it returns the CDF at each missing time point in the encoding domain. Currently only supports method=:median.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"using Plots\ncdfs, ts, pred_err, target = get_cdfs(\n    imp,\n    class, \n    instance_idx, \n    impute_sites, \n    # method; # Only supports method=:median\n)\nnothing # hide","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"xvals = imp.x_guess_range.xvals[1:10:end]\nplot(xvals, cdfs[1][1:10:end]; legend=:none)\np = last([plot!(xvals, cdfs[i][1:10:end]) for i in eachindex(cdfs)])\nylabel!(\"cdf(x)\")\nxlabel!(\"x_t\")\ntitle!(\"CDF at each time point.\")\nsavefig(\"./figs_generated/imputation/cdfs.svg\") # hide\nnothing # hide","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/#Docstrings","page":"Imputation","title":"Docstrings","text":"","category":"section"},{"location":"imputation/#MPSTime.init_imputation_problem-Tuple{TrainedMPS, Matrix}","page":"Imputation","title":"MPSTime.init_imputation_problem","text":"init_imputation_problem(W::TrainedMPS, X_test::AbstractMatrix, y_test::AbstractArray=zeros(Int, size(X_test,1)), [custom_encoding::MPSTime.Encoding]; <keyword arguments>) -> imp::ImputationProblem\ninit_imputation_problem(W::TrainedMPS, X_test::AbstractMatrix, [custom_encoding::MPSTime.Encoding]); <keyword arguments>) -> imp::ImputationProblem\n\nInitialise an imputation problem using a trained MPS and relevent test data.\n\nThis involves a lot of pre-computation, which can be quite time intensive for data-driven bases. For unclassed/unsupervised data y_test may be omitted.  If the MPS was trained with a custom encoding, then this encoding must be passed to init_imputation_problem.\n\nKeyword Arguments\n\nguess_range::Union{Nothing, Tuple{<:Real,<:Real}}=nothing: The range of values that guesses are allowed to take. This range is applied to normalised, encoding-adjusted time-series data. To allow any guess, leave as nothing, or set to encoding.range (e.g. [(-1., 1.) for the legendre encoding]).\ndx::Float64 = 1e-4: The spacing between possible guesses in normalised, encoding-adjusted units. When imputing missing data with an MPS method, the imputed values will be selected from    range(guess_range...; step=dx)\nverbosity::Integer=1: The verbosity of the initialisation process. Useful for debugging, set to -1 to completely suppress output.\ntest_encoding::Bool=true: Whether to double check the encoding and scaling options are correct. This is strongly recommended but has a slight performance cost, so may be disabled.\nstatic_xvecs::Bool=true: Whether to store encoded xvalues as StaticVectors. Usually improved performance\n\n\n\n\n\n","category":"method"},{"location":"imputation/#MPSTime.MPS_impute","page":"Imputation","title":"MPSTime.MPS_impute","text":"MPS_impute(\n    imp::ImputationProblem, \n    [class::Any,] \n    instance::Integer, \n    missing_sites::AbstractVector{<:Integer}, \n    [method::Symbol=:median]; \n    <keyword arguments>\n) -> (imputed_instances::Vector, errors::Vector, targets::Vector, stats::Dict, plots::Vector{Plots.Plot})\n\nImpute the missing_sites using an MPS-based approach, selecting the trajectory from the conditioned distribution with method. \n\nThe imputed time series, the imputation errors, the original time series, statistics about goodness of fit, and any plots generated by MPS_impute  are returned by imputed_instances, errors, targets, stats, and plots respectively. These will always be length one vectors,  unless the ITS or kNearestNeighbour method are used, which may cause MPS_impute to return multiple potential imputations with the associated errors, stats, and plots.  Goodness of fit is always measured with respect to unscaled\n\nSee init_imputation_problem for constructing an ImputationProblem instance out of a trained MPS. The instance number is relative to the class, so class 1, instance 2 would be distinct from class 2, instance 2.\n\nImputation Methods\n\n:median: For each missing value, compute the probability density function of the possible outcomes from the MPS, and choose the median. This method is the most robust to outliers. Keywords:\nget_wmad::Bool=true: Whether to return an 'error' vector that computes the Weighted Median Absolute Deviation (WMAD) of each imputed value.\n:mean: For each missing value, compute the probability density function of the possible outcomes from the MPS, and choose the expected value. Keywords:\nget_std::Bool=true: Whether to return an 'error' vector that computes standard deviation of each imputed value.\n:mode: For each missing value, choose the most likely outcome predicted by the MPS. Keywords:\nmax_jump::Union{Number,Nothing}=nothing: The largest jump allowed between two adjacent imputations. Leave as nothing to allow any jump. Helpful to suppress 'spikes' caused by poor support near the encoding domain edges.\n:ITS: For each missing value, choose a value at random with probability weighted by the probability density function of the possible outcomes. Keywords:\nrseed::Integer=1: Random seed for producing the trajectories.\nnum_trajectories::Integer=1: Number of trajectories to compute.\nrejection_threshold::Union{Float64, Symbol}=:none: Number of WMADs allowed between adjacent points. Setting this low helps suppress rapidly varying trajectories that occur by bad luck. \nmax_trials::Integer=10: Number of attempts allowed to make guesses conform to rejection_threshold before giving up.\n:kNearestNeighbour: Select the k nearest neighbours in the training set using Euclidean distance to the known data. Keyword:\nk: Number of nearest neighbours to return. See kNN_impute\nflatBaseline: Predict the missing values are just the mean of the training set.\n\nKeyword Arguments\n\nimpute_order::Symbol=:forwards: Whether to impute the missing values :forwards (left to right) or :backwards (right to left)\nNN_baseline::Bool=true: Whether to also impute the missing data with a k-Nearest Neighbour baseline.\nn_baselines::Integer=1: How many nearest neighbour baselines to compute.\nplot_fits::Bool=true: Whether to make a plot showing the target timeseries, the missing values, and the imputed region. If false, then p will be an empty vector. The plot will show the NN_baseline (if it was computed), as well as every trajectory if using the :ITS method.\nget_metrics::Bool=true: Whether to compute imputation metrics, if false, then stats, will be empty.\nfull_metrics::Bool=false: Whether to compute every metric (MAPE, SMAPE, MAE, MSE, RMSE) or just MAE and MAPE.\nprint_metric_table::Bool=false: Whether to print the stats as a table.\ninvert_transform::Bool=true:, # Whether to undo the sigmoid transform/minmax normalisation before returning the imputed points. If this is false, imputed_instance, errors, target timeseries, stats, and plot y-axis will all be scaled by the data preprocessing / normalisation and fit to the encoding domain.\nkwargs...: Extra keywords passed to MPS_impute are forwarded to the imputation method. See the Imputation Methods section.\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.get_cdfs","page":"Imputation","title":"MPSTime.get_cdfs","text":"get_cdfs(imp::ImputationProblem, \n           class::Any, \n           instance::Integer, \n           missing_sites::AbstractVector{<:Integer}, \n           [method::Symbol=:median]; \n           <keyword arguments>) -> (cdfs::Vector{Vector}, ts::Vector, pred_err::Vector, target_timeseries_full::Vector)\n\nImpute the missing_sites using an MPS-based approach, selecting the trajectory from the conditioned distribution with method, and returns the cumulative distribution function used to infer each missing value. \n\nSee MPS_impute for a list of imputation methods and keyword arguments (does not support plotting, stats, or kNN baselines). See init_imputation_problem for constructing an ImputationProblem instance. The instance number is relative to the class, so class 1, instance 2 would be distinct from class 2, instance 2.\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.state_space","page":"Imputation","title":"MPSTime.state_space","text":"state_space(T::Int, n::Int, s::Int=2; sigma::Float64=0.3, rng::AbstractRNG}=Random.GLOBAL_RNG) -> Matrix{Float64}\n\nGenerate n time series of length T each from a state space model with residual terms drawn from a normal distribution N(0, sigma) and lag order s. Time series are generated from the following model:\n\nx_t = mu_t + theta_t + eta_t\nmu_t = mu_t-1 + lambda_t-1 + xi_t\nlambda_t = lambda_t-1 + zeta_t\ntheta_t = sum_j=1^s-1 - theta_t-j + omega_t\n\nwhere x_t is the t-th value in the time series, and the residual terms eta_t, xi_t, zeta_t and omega_t are randomly drawn from a normal distribution mathcalN(0 sigma).\n\nArguments\n\nT – Time series length.\nn – Number of time-series instances.\n\nKeyword Arguments\n\ns – Lag order (optional, default: 2).\nsigma – Noise standard deviation (optional, default: 0.3).\nrng – Random number generator of type AbstractRNG (optional, default: Random.GLOBAL_RNG).\n\nReturns\n\nA Matrix{Float64} of shape (n, T) containing the simulated time-series instances. \n\n\n\n\n\n","category":"function"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Internal imputation methods:","category":"page"},{"location":"imputation/#Internal-imputation-methods","page":"Imputation","title":"Internal imputation methods","text":"","category":"section"},{"location":"imputation/#MPSTime.impute_median","page":"Imputation","title":"MPSTime.impute_median","text":"impute missing data points using the median of the conditional distribution (single site rdm ρ).\n\nArguments\n\nclass_mps::MPS: \nopts::Options: MPS parameters.\nenc_args::AbstractVector\nx_guess_range::EncodedDataRange\ntimeseries::AbstractVector{<:Number}: The input time series data that will be imputed.\ntimeseries_enc::MPS: The encoded version of the time series represented as a product state. \nimputation_sites::Vector{Int}: Indices in the time series where imputation is to be performed.\nget_wmad::Bool: Whether to compute the weighted median absolute deviation (WMAD) during imputation (default is false).\n\nReturns\n\nA tuple containing:\n\nmedian_values::Vector{Float64}: The imputed median values at the specified imputation sites.\nwmad_value::Union{Nothing, Float64}: The weighted median absolute deviation if get_wmad is true; otherwise, nothing.\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.impute_ITS","page":"Imputation","title":"MPSTime.impute_ITS","text":"Impute a SINGLE trajectory using inverse transform sampling (ITS).\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.kNN_impute","page":"Imputation","title":"MPSTime.kNN_impute","text":"kNN_impute(\n    imp::ImputationProblem, \n    [class::Any,] \n    instance::Integer, \n    missing_sites::AbstractVector{<:Integer}; \n    k::Integer=1\n) -> [neighbour1::Vector, neighbour2::Vector, ...]\n\nImpute missing_sites using the k nearest neighbours in the test set, based on Euclidean distance.\n\nSee init_imputation_problem for constructing an ImputationProblem instance. The instance number is relative to the class, so class 1, instance 2 would be distinct from class 2, instance 2.\n\n\n\n\n\n","category":"function"},{"location":"#MPSTime.jl","page":"Introduction","title":"MPSTime.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"A Julia package for time-series machine learning (ML) using Matrix-Product States (MPS) built on the ITensors.jl framework [1, 2].","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"#Overview","page":"Introduction","title":"Overview","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"MPSTime is a Julia package for learning the joint probability distribution of time series directly from data using matrix product state (MPS) methods inspired by quantum many-body physics.  It provides a unified formalism for:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Time-series classification (inferring the class of unseen time-series).\nUnivariate time-series imputation (inferring missing points within time-series instances) across fixed-length time series.\nSynthetic data generation (coming soon).","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Info\nMPSTime is currently under active development. Many features are in an experimental stage and may undergo significant changes, refinements, or removals.","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This is not yet a registered Julia package, but it will be soon (TM)! In the meantime, you can install it directly from our GitHub repository:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> ]\npkg> add https://github.com/hugopstackhouse/MPSTime.jl.git","category":"page"},{"location":"#Usage","page":"Introduction","title":"Usage","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"See the sidebars for basic usage examples.  We're continually adding more features and documentation as we go.","category":"page"},{"location":"#Citation","page":"Introduction","title":"Citation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"If you use MPSTime in your work, please read and cite the arXiv preprint:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"@misc{MPSTime2024,\n      title={Using matrix-product states for time-series machine learning}, \n      author={Joshua B. Moore and Hugo P. Stackhouse and Ben D. Fulcher and Sahand Mahmoodian},\n      year={2024},\n      eprint={2412.15826},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML},\n      url={https://arxiv.org/abs/2412.15826}, \n}","category":"page"},{"location":"docstrings/#Docstrings","page":"Docstrings","title":"Docstrings","text":"","category":"section"},{"location":"docstrings/#Useful-Structs","page":"Docstrings","title":"Useful Structs","text":"","category":"section"},{"location":"docstrings/#MPSTime.Encoding-docstrings","page":"Docstrings","title":"MPSTime.Encoding","text":"Encoding\n\nAbstract supertype of all encodings. To specify an encoding for MPS training, set the encoding keyword when calling MPSOptions.\n\nExample\n\njulia> opts = MPSOptions(; encoding=:Legendre);\n\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\n\nEncodings\n\n:Legendre: The first d L2-normalised Legendre Polynomials. Real valued, and supports passing projected_basis=true to MPSOptions.\n:Fourier: Complex valued Fourier coefficients. Supports passing projected_basis=true to MPSOptions.\n\n    Phi(x d) = left1 + 0i e^i pi x e^-i pi x e^2i pi x e^-2i pi x ldots right  sqrtd \n\n:Stoudenmire: The original complex valued \"Spin-1/2\" encoding from Stoudenmire & Schwab, 2017 arXiv. Only supports d = 2\n\n    Phi(x) = left e^3 i pi x  2 cos(fracpi2 x)  e^-3 i pi x  2 sin(fracpi2 x)right\n\n:Sahand_Legendre_Time_Dependent:  (:SLTD) A custom, real-valued encoding constructed as a data-driven adaptation of the Legendre Polynomials. At each time point, t, the training data is used to construct a probability density function that describes the distribution of the time-series amplitude x_t. This is the first basis function. \nb_1(x t) = textpdf_x_t(x_t). This is computed with KernelDensity.jl:\n\njulia> Using KernelDensity\n\njulia> xs_samps = range(-1,1, max(200,size(X_train,2)));\n\njulia> b1(xs,t) = pdf(kde(X_train[t,:]), xs_samps);\n\nThe second basis function is the first order polynomial that is L2-orthogonal to this pdf on the interval [-1,1]. \n\nb_2(xt) = a_1 x + a_0 text where  int_-1^1 b_1(xt) b_2^*(x t) textrmd x = 0  lvertlvert b_2(x t) rvertrvert_L2 = 1\n\nThe third basis function is the second order polynomial that is L2-orthogonal to the first two basis functions on [-1,1], etc.\n\n-:Custom: For use with user-defined custom bases. See function_basis\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MPSTime.EncodedTimeSeriesSet","page":"Docstrings","title":"MPSTime.EncodedTimeSeriesSet","text":"EncodedTimeSeriesSet\n\nHolds an encoded time-series dataset, as well as a copy of the original data and its class distribution.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MPSTime.TrainedMPS","page":"Docstrings","title":"MPSTime.TrainedMPS","text":"TrainedMPS\n\nContainer for a trained MPS and its associated Options and training data.\n\nFields\n\nmps::MPS: A trained Matrix Product state.\nopts::MPSOptions: User defined MPSOptions used to create the MPS.\ntrain_data::EncodedTimeSeriesSet: Stores both the raw and encoded data used to train the mps.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#Hyperparameters","page":"Docstrings","title":"Hyperparameters","text":"","category":"section"},{"location":"docstrings/#MPSTime.AbstractMPSOptions","page":"Docstrings","title":"MPSTime.AbstractMPSOptions","text":"AbstractMPSOptions\n\nAbstract supertype of \"MPSOptions\", a collection of concrete types which is used to specify options for training, and \"Options\", which is used internally and contains references to internal objects\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MPSTime.MPSOptions-docstrings","page":"Docstrings","title":"MPSTime.MPSOptions","text":"MPSOptions(; <Keyword Arguments>)\n\nSet the hyperparameters and other options for fitMPS. \n\nFields:\n\nLogging\n\nverbosity::Int=1: How much debug/progress info to print to the terminal while optimising the MPS. Higher numbers mean more output\nlog_level::Int=3: How much statistical output. 0 for nothing, >0 to print losses, accuracies, and confusion matrix at each step. Noticeable computational overhead \ntrack_cost::Bool=false: Whether to print the cost at each Bond tensor site to the terminal while training, mostly useful for debugging new cost functions or optimisers (HUGE computational overhead)\n\nMPS Training Hyperparameters\n\nnsweeps::Int=5: Number of MPS optimisation sweeps to perform (One sweep is both forwards and Backwards)\nchi_max::Int=25: Maximum bond dimension allowed within the MPS during the SVD step\neta::Float64=0.01: The learning rate. For gradient descent methods, this is the step size. For Optim and OptimKit this serves as the initial step size guess input into the linesearch\nd::Int=5: The dimension of the feature map or \"Encoding\". This is the true maximum dimension of the feature vectors. For a splitting encoding, d = numsplits * auxbasis_dim\ncutoff::Float64=1E-10: Size based cutoff for the number of singular values in the SVD (See Itensors SVD documentation)\ndtype::DataType=Float64 or ComplexF64 depending on encoding: The datatype of the elements of the MPS. Supports the arbitrary precsion types such as BigFloat and Complex{BigFloat}\nexit_early::Bool=false: Stops training if training accuracy is 1 at the end of any sweep.\n\nEncoding Options\n\nencoding::Symbol=:Legendre: The encoding to use, including :Stoudenmire, :Fourier, :Legendre, :SLTD, :Custom, etc. see Encoding docs for a complete list. Can be just a time (in)dependent orthonormal basis, or a time (in)dependent basis mapped onto a number of \"splits\" which distribute tighter basis functions where the sites of a timeseries are more likely to be measured.  \nprojected_basis::Bool=false: Whether to project a basis onto the training data at each time. Normally, when specifying a basis of dimension d, the first d lowest order terms are used. When project=true, the training data is used to construct a pdf of the possible timeseries amplitudes at each time point. The first d largest terms of this pdf expanded in a series are used to select the basis terms.\naux_basis_dim::Int=2: Unused for standard encodings. If the encoding is a SplitBasis, serves as the auxilliary dimension of a basis mapped onto the split encoding, so that the number of histogram bins = d / auxbasisdim. \nencode_classes_separately::Bool=false: Only relevant for data driven bases. If true, then data is split up by class before being encoded. Functionally, this causes the encoding method to vary depending on the class\n\nData Preprocessing and MPS initialisation\n\nsigmoid_transform::Bool: Whether to apply a sigmoid transform to the data before minmaxing. This has the form\n\nboldsymbolX = left(1 + exp-fracboldsymbolX-m_boldsymbolXr_boldsymbolX  135right)^-1\n\nwhere boldsymbolX is the un-normalized time-series data matrix, m_boldsymbolX is the median of boldsymbolX and r_boldsymbolXis its interquartile range.\n\nminmax::Bool: Whether to apply a minmax norm to [0,1] before encoding. This has the form\n\nboldsymbolX =  fracboldsymbolX - x_textminx_textmax - x_textmin\n\nwhere boldsymbolX is the scaled robust-sigmoid transformed data matrix, x_textmin and x_textmax are the minimum and maximum of boldsymbolX.\n\ndata_bounds::Tuple{Float64, Float64} = (0.,1.): The region to bound the data to if minmax=true. This is separate from the encoding domain. All encodings expect data to be scaled scaled between 0 and 1. Setting the data bounds a bit away from [0,1] can help when your basis has poor support near its boundaries.\ninit_rng::Int: Random seed used to generate the initial MPS\nchi_init::Int: Initial bond dimension of the random MPS\n\nLoss Functions and Optimisation Methods\n\nloss_grad::Symbol=:KLD: The type of cost function to use for training the MPS, typically Mean Squared Error (:MSE) or KL Divergence (:KLD), but can also be a weighted sum of the two (:Mixed) if uselegacyITensor is enabled.\nbbopt::Symbol=:TSGO: Which local Optimiser to use, builtin options are symbol gradient descent (:GD), or gradient descent with a TSGO rule (:TSGO). If use_legacy_ITensor` is enabled, can be a Conjugate Gradient descent optimisation rule using either the Optim or OptimKit package (:Optim or :OptimKit respectively). The CGD methods work well for MSE based loss functions, but seem to perform poorly for KLD base loss functions.\nrescale::Tuple{Bool,Bool}=(false,true): Has the form rescale = (before::Bool, after::Bool). Where to enforce the normalisation of the MPS during training, either calling normalise!(Bond Tensor) before or after BT is updated. Note that for an MPS that starts in canonical form, rescale = (true,true) will train identically to rescale = (false, true) but may be less performant.\nupdate_iters::Int=1: Maximum number of optimiser iterations to perform for each bond tensor optimisation. E.G. The number of steps of (Conjugate) Gradient Descent used by TSGO, Optim or OptimKit\ntrain_classes_separately::Bool=false: Whether the the trainer optimises the total MPS loss over all classes or whether it considers each class as a separate problem. Should make very little diffence\nuse_legacy_ITensor::Bool=false: Whether to use the old, slow (but possibly easier to understand) ITensor Implementation\nsvd_alg::String=\"divide_and_conquer\": SVD Algorithm to pass to ITensor\n\n`\n\nDebug\n\nreturn_encoding_meta_info::Bool=false: Debug flag: Whether to return the normalised data as well as the histogram bins for the splitbasis types\n\n\n\n\n\nMPSOptions(params::NamedTuple) -> MPSOptions\n\nConvert the named tuple params with format (:option1=value1, :option2=value2,...) to an MPSOptions object.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"The internal Options type and the functions that help convert between","category":"page"},{"location":"docstrings/#MPSTime.Options","page":"Docstrings","title":"MPSTime.Options","text":"Options(; <Keyword Arguments>)\n\nThe internal options struct. Fields have the same meaning as MPSOptions, but contains (unserialisable) objects instead of symbols, e.g. Encoding=Basis(\"Legendre\") instead of :Legendre\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MPSTime.safe_options","page":"Docstrings","title":"MPSTime.safe_options","text":"safe_options(opts::AbstractMPSOptions)\n\nTakes any AbstractMPSOptions type, and returns an instantiated Options type.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.model_encoding","page":"Docstrings","title":"MPSTime.model_encoding","text":"model_encoding(symb::Symbol, project::Bool=false)\n\nConstruct an Encoding object from symb. Not case sensitive. See Encodings documentation for the full list of options. Will use the specified project options if the encoding supports projecting. The inverse of symbolic_encoding.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.symbolic_encoding","page":"Docstrings","title":"MPSTime.symbolic_encoding","text":"symbolic_encoding(E::Encoding)\n\nConstruct a symbolic name from an Encoding object. The inverse of model_encoding\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.model_loss_func","page":"Docstrings","title":"MPSTime.model_loss_func","text":"model_loss_func(symb::Symbol)\n\nSelect a loss function (::Function) from the symb. Not case sensitive. The inverse of symboliclossfunc\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.model_bbopt","page":"Docstrings","title":"MPSTime.model_bbopt","text":"model_bbopt(symb::Symbol)\n\nConstuct a BBOpt object from symb. Not case sensitive.\n\n\n\n\n\n","category":"function"}]
}
