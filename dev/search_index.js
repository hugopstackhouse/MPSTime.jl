var documenterSearchIndex = {"docs":
[{"location":"tools/#Tools","page":"Tools","title":"Tools","text":"","category":"section"},{"location":"tools/#Entanglement-Entropy","page":"Tools","title":"Entanglement Entropy","text":"","category":"section"},{"location":"tools/#Overview","page":"Tools","title":"Overview","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"In quantum many-body physics, the entanglement entropy (EE) determines the extent to which two partitions of the collective quantum system are entangled. More simply, the EE can be thought of as quantifying the information shared between subsystem A and subsystem B within a many-body system. In practice, the EE is computed as the von Neumman entropy of the reduced density matrix for any of the two subsystems (A or B).  An EE of zero implies that there is no entanglement between the subsystems.","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"We provide functions for two types of EE: (i) single-site entanglement entropy (SEE), and (ii) bipartite entanglement entropy (BEE):","category":"page"},{"location":"tools/#(1)-Single-site-entanglement-entropy-(SEE)","page":"Tools","title":"(1) Single-site entanglement entropy (SEE)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"The single-site entanglement entropy (SEE) quantifies the degree of entanglement between a single site (time-point) in the MPS and all other sites (time points). Given a particular site in the MPS, i, the SEE is then specified by the von Neumann entropy of the reduced density matrix [3, 4]:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"S_textrmSEE = -mathrmtr rho_i log rho_i","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"where rho_i is the reduced density matrix (rdm) at site i, obtained by tracing over all sites except for the i-th site:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"rho_i = Tr_i ketpsibrapsi","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"and ketpsi is the MPS. Using the 1D spin-chain as an illustrative example, the SEE between a single site (dark blue) and the rest of the system (light blue) can be depicted as:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#(2)-Bipartite-entanglement-entropy-(BEE)","page":"Tools","title":"(2) Bipartite entanglement entropy (BEE)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"The bipartite entanglement entropy (BEE) quantifies the quantum entanglement between two complementary subsystems of a matrix product state (MPS).  For an MPS with N sites, we can create a bipartition by splitting the system at any bond l, resulting in region A (sites 1 to l) and region B (sites l+1 to N). The BEE can be expressed using the singular values of the Shmidt decomposition of either of the two subsystems:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"ketpsi = sum_i alpha_i ketu_i_A\notimes ketv_i_B","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"where alpha_i are the Schmidt coefficients (singular values) satisfying sum_i alpha_i^2 = 1, ketu_i_A and ketv_i_B are orthonormal states in subsystem A and B, respectively. The BEE is then given by the von Neumann entropy [4]:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"S_textrmBEE = -sum_i alpha_i^2 log alpha_i^2","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"The BEE can be represented schematically using the 1D spin chain analogy where the red dotted line denotes the bipartition, the light blue particles represent subsystem A and the dark blue represent subsystem B:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Bipartite-Entanglement-Entropy-(BEE)","page":"Tools","title":"Bipartite Entanglement Entropy (BEE)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"Given a trained MPS (for either classification or imputation), we can compute the bipartite entanglement entropy (BEE) using the bipartite_spectrum function:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# train the MPS as usual\nmps, _, _ = fitMPS(...);\nbees = bipartite_spectrum(mps);","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"A vector is returned where each entry contains the BEE spectrum for the class-specific MPS.  For example, in the case of a two class problem, we obtain the individual BEE spectrums for the class 0 MPS and the class 1 MPS.  For an unsupervised problem with only a single class, there is only a single BEE spectrum. ","category":"page"},{"location":"tools/#Example","page":"Tools","title":"Example","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"To illustrate how we might use the BEE in a typical analysis, consider an example involving real world time series from the ItalyPowerDemand (IPD) UCR dataset.  There are two classes corresponding to the power demand during: (i) the winter months; (ii) the summer months.  For this example, we will train an MPS to classify between summer and winter time-series data:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# load in the training data\nusing JLD2\nipd_load = jldopen(\"ipd_original.jld2\", \"r\");\n    X_train = read(ipd_load, \"X_train\")\n    y_train = read(ipd_load, \"y_train\")\n    X_test = read(ipd_load, \"X_test\")\n    y_test = read(ipd_load, \"y_test\")\nclose(ipd_load)\nopts = MPSOptions(d=10, chi_max=40, nsweeps=10; init_rng=4567)\nmps, _, _ = fitMPS(X_train, y_train, X_test, y_test, opts)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"Let's take a look at the training dataset for this problem:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: ) Using the trained MPS, we can then inspect the BEE for the class 0 (winter) and class 1 (summer) MPS individually:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"bees = bipartite_spectrum(mps);\nbee0, bee1 = bees\nb1 = bar(bee0, title=\"Winter\", label=\"\", c=palette(:tab10)[1], xlabel=\"site\", ylabel=\"entanglement entropy\");\nb2 = bar(bee1, title=\"Summer\", label=\"\", c=palette(:tab10)[2], xlabel=\"site\", ylabel=\"entanglement entropy\");\np = plot(b1, b2)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Single-Site-Entanglement-Entropy-(SEE)","page":"Tools","title":"Single-Site Entanglement Entropy (SEE)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"Given a trained MPS, we can also compute the single-site entanglement entropy (SEE) using the single_site_spectrum function:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# train MPS as usual\nmps, _, _ = fitMPS(...);\nsees = MPSTime.single_site_spectrum(mps);","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"As with the BEE, a vector is returned where each entry contains the SEE spectrum for the class-specific MPS. ","category":"page"},{"location":"tools/#Example-2","page":"Tools","title":"Example","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"Continuing our example from the BEE with the ItalyPowerDemand (IPD) dataset, we will now compute the single-site entanglement entropy (SEE) spectrum:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"sees = single_site_spectrum(mps);\nsee0, see1 = sees\nb1 = bar(see0, title=\"Winter\", label=\"\", c=palette(:tab10)[1], xlabel=\"site\", ylabel=\"SEE\");\nb2 = bar(see1, title=\"Summer\", label=\"\", c=palette(:tab10)[2], xlabel=\"site\", ylabel=\"SEE\");\np = plot(b1, b2)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Single-Site-Entanglement-Entropy-Variation","page":"Tools","title":"Single-Site Entanglement Entropy Variation","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"Another quantity we can compute is the single-site entanglement entropy (SEE) variation. In effect, the SEE variation captures the change in SEE at any given MPS site, conditional upon having measured the preceding sites. Given a trained MPS, the SEE variation can be computed:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# see_variation expects a data matrix, so we need to index as follows to feed in a single instance\nsee_variation = see_variation(mps, X_test[1:1, :])\n# if there is more than one class (e.g., classification)\nsee_variation_c0 = see_variation(mps, X_test[1:1, :], 0)\nsee_variation_c1 = see_variation(mps, X_test[1:1, :], 1)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"It can be useful to visualize the SEE variation as a barplot.  Here we will plot the SEE of the unmeasured MPS, after measuring 5 sites (i.e., 5 time pts.), 20 sites, and 50 sites: ","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"cpal = palette(:tab10)\nsee_variation = see_variation(mps, X_test[1:1, :])\nb = bar(see_variation[1, 1, :], c=cpal[1], label=\"Unmeasured\", xlabel=\"site\", ylabel=\"SEE\")\nbar!(see_variation[1, 6, :], c=cpal[2], label=\"5 Sites Measured\")\nbar!(see_variation[1, 21, :], c=cpal[3], label=\"20 Sites Measured\")\nbar!(see_variation[1, 51, :], c=cpal[4], label=\"50 Sites Measured\")","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Missing-Data-Simulation","page":"Tools","title":"Missing Data Simulation","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"In the time-series imputation literature, time-series data can be categorised into one of three types based on the underlying process responsible for the missing data: (i) missing completely at random (MCAR); (ii) missing at random (MAR); or, (iii) missing not at random (MNAR). A review of the various mechanisms in the univariate setting can be found in [5].","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"MPSTime provides implementations of all three mechanisms, adapted from the more typical multivariate setting to the case of univariate time-series data. To generate synthetic missing data, the original (uncorrupted) univariate time-series instance is passed into a function which assigns a NaN value to time points determined by the missing data mechanism of choice. ","category":"page"},{"location":"tools/#Missing-Completely-at-Random-(MCAR)","page":"Tools","title":"Missing Completely at Random (MCAR)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"To simulate missing completely at random (MCAR) data, the locations (time points) of missing points are sampled from a Bernoulli distribution where the probability of a \"successful trial\" (i.e., missing data point) is the same for all time points. Let's generate a random time-series instance and simulate 50% data missingness using an MCAR mechanism:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"using MPSTime\nusing Random\nRandom.seed!(42)\npm = 0.5 # 50% data missing\nX_clean = rand(100) # your data as a vector\nX_corrupted, X_missing_inds = mcar(X_clean, pm)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"The mcar function will return two values: a copy of the time-series with NaN values at missing positions (X_corrupted), and the indices of the missing values (X_missing_inds). Let's plot the corrupted data:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"using Plots\np1 = plot(X_clean, xlabel=\"time\", ylabel=\"x\", label=\"\", title=\"Original data\");\np2 = plot(X_corrupted, xlabel=\"time\", ylabel=\"x\", label=\"\", title=\"MCAR $(pm*100)% missing data\");\nplot(p1, p2, size=(1200, 300));","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"For reproducibility, we can optionally pass in a random seed to the mcar function:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"seed = 42; # random seed \nX_corrupted, X_missing_inds = mcar(X_clean, pm; state=seed)","category":"page"},{"location":"tools/#Missing-at-Random-(MAR)","page":"Tools","title":"Missing at Random (MAR)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"Following from the example above with randomly generated data, we can simulate a missing at random (MAR) mechanism using the mar function. Currently, MPSTime supports block missing patterns whereby a starting time point is randomly selected, and all subsequent observations within a specified block length are set to NaN:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# using the same data, X_clean, from above...\npm = 0.5\nX_corrupted, X_missing_inds = mar(X_clean, pm)","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"Plotting the corrupted data:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Missing-Not-At-Random-(MNAR)","page":"Tools","title":"Missing Not At Random (MNAR)","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"To simulate missing not at random (MNAR) data, use the mnar function. There are two possible options for the MNAR mechanims: (i) LowestMNAR (default option) and (ii) HighestMNAR. The LowestMNAR mechanism sets the lowest N values of the time-series to NaN where N is determined by the target percentage data missing.  Conversely, HighestMNAR sets the highest N value to NaN:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"# using the same data, X_clean, from above...\npm = 0.5\nX_corrupted_low, X_missing_inds_low = mnar(X_clean, pm) # default setting uses LowestMNAR\nX_corrupted_high, X_missing_inds_high = mnar(X_clean, pm, MPSTime.HighestMNAR()) # use the HighestMNAR mechanism","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"Plotting corrupted time-series from the LowestMNAR mechanism:","category":"page"},{"location":"tools/","page":"Tools","title":"Tools","text":"(Image: )","category":"page"},{"location":"tools/#Docstrings","page":"Tools","title":"Docstrings","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"MPSTime.bipartite_spectrum\nMPSTime.single_site_spectrum\nMPSTime.see_variation\nMPSTime.mcar\nMPSTime.mar\nMPSTime.mnar","category":"page"},{"location":"tools/#MPSTime.bipartite_spectrum","page":"Tools","title":"MPSTime.bipartite_spectrum","text":"bipartite_spectrum(mps::TrainedMPS; logfn::Function=log) -> Vector{Vector{Float64}}\n\nCompute the bipartite entanglement entropy (BEE) of a trained MPS across each bond. Given a single unlabeled MPS the BEE is defined as:\n\nsum_i alpha_i^2 log(alpha_i^2)\n\nwhere alpha_i are the eigenvalues obtained from the shmidt decomposition. \n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.single_site_spectrum","page":"Tools","title":"MPSTime.single_site_spectrum","text":"single_site_spectrum(mps::TrainedMPS) -> Vector{Vector{Float64}}\n\nCompute the single-site entanglement entropy (SEE) spectrum of a trained MPS.\n\nThe single-site entanglement entropy (SEE) quantifies the entanglement between each site in the MPS and all other sites. It is computed as:\n\ntextSEE = -texttr(rho log(rho))\n\nwhere rho is the single-site reduced density matrix (RDM).\n\nArguments\n\nmps::TrainedMPS: A trained Matrix Product State (MPS).\n\nReturns\n\nA vector of vectors, where the outer vector corresponds to each label in the expanded MPS, and the inner vectors contain the SEE values for the respective sites.\n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.see_variation","page":"Tools","title":"MPSTime.see_variation","text":"see_variation(mps::TrainedMPS, measure_series::Matrix, class::Int=0) -> Array{Float64, 3}\n\nCompute the variation in single-site entanglement entropy for each site N in the MPS,  having measured sites 1 to N-1.\n\nArguments\n\nmps::TrainedMPS: A trained Matrix Product State (MPS).\nmeasure_series::Matrix: A matrix of time-series instances to measure (unscaled).\nclass::Int: Class MPS to compute SEE variation if there are multiple classes (MPSs).\n\nReturns\n\nAn array of dimension 3, where the first dimension is the time series instance, the second dimension is the probe site (site at which to measure SEE) and the third dimenion is the number of previous sites measured.  \n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.mcar","page":"Tools","title":"MPSTime.mcar","text":"mcar(X::AbstractVector, fraction_missing::Float64, mechanism::MCARMechanism=BernoulliMCAR(); \n         rng::AbstractRNG=Random.GLOBAL_RNG, verbose::Bool=false) -> Tuple{Vector{Float64}, Vector{Int64}}\n\nGenerate missing data using a Missing Completely At Random (MCAR) mechanism, where the probability  of missingness is independent of both observed and unobserved values. Available mechanisms:\n\nBernoulliMCAR(): Missing values are generated by sampling from a Bernoulli distribution with   probability fraction_missing.\n\nArguments\n\nX::AbstractVector: Input time series data.\nfraction_missing::Float64: Target fraction of missing values, must be between 0 and 1 (default: 0.5).\nmechanism::MCARMechanism: Mechanism used to generate missing values (default: BernoulliMCAR()).\nrng::AbstractRNG: Random number generator for reproducibility (default: GLOBAL_RNG).\nverbose::Bool: If true, prints comparison of target vs actual percentage of missing values.\n\nReturns\n\nX_corrupted::Vector{Float64}: Copy of input vector with NaN values at missing positions.\nmissing_idxs::Vector{Int64}: Indices of missing values.\n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.mar","page":"Tools","title":"MPSTime.mar","text":"mar(X::AbstractVector, fraction_missing::Float64, mechanism::MARMechanism=BlockMissingMAR();\n       rng::AbstractRNG=Random.GLOBAL_RNG, verbose::Bool=false) -> Tuple{Vector{Float64}, Vector{Int64}}\n\nGenerate missing data using a Missing At Random (MAR) mechanism, where the probability of  missingness depends only on observed values or known information. Available mechanisms:\n\nBlockMissingMAR(): Generates a contiguous block of missing values with random start position.\n\nThe missingness of each point after the first depends on the position of previous missing values.\n\nArguments\n\nX::AbstractVector: Input time series data.\nfraction_missing::Float64: Target fraction of missing values, must be between 0 and 1 (default: 0.5).\nmechanism::MARMechanism: Mechanism used to generate missing values (default: BlockMissingMAR())\nrng::AbstractRNG: Random number generator for reproducibility (default: GLOBAL_RNG).\nverbose::Bool: If true, prints comparison of target vs actual percentage of missing values.\n\nReturns\n\nX_corrupted::Vector{Float64}: Copy of input vector with NaN values at missing positions.\nmissing_idxs::Vector{Int64}: Indices of missing values.\n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.mnar","page":"Tools","title":"MPSTime.mnar","text":"mnar(X::AbstractVector, fraction_missing::Float64, mechanism::MNARMechanism=LowestMNAR();\n        verbose::Bool=false) -> Tuple{Vector{Float64}, Vector{Int64}}\n\nGenerate missing data using a Missing Not At Random (MNAR) mechanism, where the probability  of missingness depends on the unobserved values themselves. Available mechanisms:\n\nLowestMNAR(): Introduces missing values at positions with the lowest values in the time series.   Implementation as in Twala 2019. \nHighestMNAR(): Introduces missing values at positions with the highest values in the time series.   Implementation as in Xia et al. 2017.\n\nArguments\n\nX::AbstractVector: Input time series data.\nfraction_missing::Float64: Target fraction of missing values, must be between 0 and 1 (default: 0.5).\nmechanism::MNARMechanism: Mechanism used to generate missing values (default: LowestMNAR()).\nverbose::Bool: If true, prints comparison of target vs actual percentage of missing values.\n\nReturns\n\nX_corrupted::Vector{Float64}: Copy of input vector with NaN values at missing positions.\nmissing_idxs::Vector{Int64}: Indices of missing values.\n\n\n\n\n\n","category":"function"},{"location":"tools/#Internal-Methods","page":"Tools","title":"Internal Methods","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"MPSTime.von_neumann_entropy\nMPSTime.one_site_rdm","category":"page"},{"location":"tools/#MPSTime.von_neumann_entropy","page":"Tools","title":"MPSTime.von_neumann_entropy","text":"von_neumann_entropy(mps::MPS; logfn::Function=log) -> Vector{Float64}\n\nCompute the von Neumann entanglement entropy for each site in a Matrix Product State (MPS).\n\nThe von Neumann entropy quantifies the entanglement at each bond of the MPS by computing the entropy of the singular value spectrum obtained from a singular value decomposition (SVD). The entropy is computed as:\n\nS = -sum_i p_i log(p_i)\n\nwhere p_i are the squared singular values normalized to sum to 1.\n\nArguments\n\nmps::MPS: The Matrix Product State (MPS) whose entanglement entropy is to be computed.\nlogfn::Function: (Optional) The logarithm function to use (log, log2, or log10). Defaults to the natural logarithm (log).\n\nReturns\n\nA vector of Float64 values where the i-th element represents the von Neumann entropy at site i of the MPS.\n\n\n\n\n\n","category":"function"},{"location":"tools/#MPSTime.one_site_rdm","page":"Tools","title":"MPSTime.one_site_rdm","text":"one_site_rdm(mps::MPS, site::Int) -> Matrix\n\nCompute the single-site reduced density matrix (RDM) of the  MPS at a given site.  If the RDM is not positive semidefinite, clamp the negative eigenvalues (if within the tolerance) and reconstruct the rdm.\n\n\n\n\n\n","category":"function"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"We cite the following works in the development of MPSTime.jl:","category":"page"},{"location":"references/","page":"References","title":"References","text":"M. Fishman, S. R. White and E. M. Stoudenmire. The ITensor Software Library for Tensor Network Calculations. SciPost Phys. Codebases, 4 (2022).\n\n\n\nM. Fishman, S. R. White and E. M. Stoudenmire. Codebase release 0.3 for ITensor. SciPost Phys. Codebases, 4-r0.3 (2022).\n\n\n\nS.-J. Ran, Z.-Z. Sun, S.-M. Fei, G. Su and M. Lewenstein. Tensor network compressed sensing with unsupervised machine learning. Physical Review Research 2 (2020).\n\n\n\nY. Liu, W.-J. Li, X. Zhang, M. Lewenstein, G. Su and S.-J. Ran. Entanglement-Based Feature Extraction by Tensor Network Machine Learning. Frontiers in Applied Mathematics and Statistics 7 (2021).\n\n\n\nM. S. Santos, R. C. Pereira, A. F. Costa, J. P. Soares, J. A. Santos and P. H. Abreu. Generating Synthetic Missing Data: A Review by Missing Mechanism. IEEE Access 7, 11651–11667 (2019).\n\n\n\nE. M. Stoudenmire and D. J. Schwab. Supervised Learning with Quantum-Inspired Tensor Networks (2017), arXiv:1605.05775 [stat.ML].\n\n\n\nB. D. Fulcher and N. S. Jones, hctsa: A Computational Framework for Automated Time-Series Phenotyping Using Massive Feature Extraction. Cell Systems 5, 527-531.e3 (2017).\n\n\n\n","category":"page"},{"location":"synthdatagen/#Synthetic-Data-Generation","page":"Synthetic Data Generation","title":"Synthetic Data Generation","text":"","category":"section"},{"location":"synthdatagen/","page":"Synthetic Data Generation","title":"Synthetic Data Generation","text":"info: Info\nThis section is still under development. Come back soon!","category":"page"},{"location":"classification/#Classification","page":"Classification","title":"Classification","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"This tutorial for MPSTime will take you through the basic steps needed to fit an MPS to a time-series dataset.","category":"page"},{"location":"classification/#Demo-dataset","page":"Classification","title":"Demo dataset","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"First, import or generate your data. Here, we generate a two class \"noisy trendy sine\" dataset for the sake of demonstration, but if you have a dataset in mind, you can skip to the next section. Our demonstration dataset consists of a sine function with a randomised phase, plus a linear trend, plus some normally distributed noise. Each time series in class c at time t is given by:","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"x^c_t = sinleft(frac2pi20t + psiright) + fracmtT + sigma_c n_t","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"where m is the slope of a linear trend, psi in 0 2pi) is a uniformly random phase offset, sigma_c is the noise scale, and n_t sim mathcalN(01) are  normally distributed random variables. ","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"The two classes will be distinguished by their noise levels. The class one time series x^1 have sigma_1 = 01, and the class two time series x^2 have sigma_2 = 09. Here are a few time-series instances from each class:","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"(Image: )","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"The below code sets this up:","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"using Random # fix rng seed\nrng = Xoshiro(1); # define trendy sine function\nfunction trendy_sine(T::Integer, n_inst::Integer, noise_std::Real, rng)\n    X = Matrix{Float64}(undef, n_inst, T)\n    ts = 1:T\n    for series in eachrow(X)\n        phase = 2 * pi * rand(rng)\n        @. series = sin(pi/10 *ts + phase) + 3 * ts / T + noise_std * randn(rng) \n    end\n    return X\nend;\nntimepoints = 100; # specify number of samples per instance\nntrain_instances = 300; # specify num training instances\nntest_instances = 200; # specify num test instances\nX_train = vcat(\n    trendy_sine(ntimepoints, ntrain_instances ÷ 2, 0.1, rng),\n    trendy_sine(ntimepoints, ntrain_instances ÷ 2, 0.9, rng)\n);\ny_train = vcat(\n    fill(1, ntrain_instances ÷ 2),\n    fill(2, ntrain_instances ÷ 2)\n);\nX_test = vcat(\n    trendy_sine(ntimepoints, ntest_instances ÷ 2, 0.1, rng),\n    trendy_sine(ntimepoints, ntest_instances ÷ 2, 0.9, rng)\n);\ny_test = vcat(\n    fill(1, ntest_instances ÷ 2),\n    fill(2, ntest_instances ÷ 2)\n);","category":"page"},{"location":"classification/#Training-an-MPS","page":"Classification","title":"Training an MPS","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"For the most basic use of fitMPS, select your hyperparameters, and run the fitMPS function.  Some (truncated) output from our noisy trendy sine datam with default hyperparameters is given below. ","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"julia> opts = MPSOptions() # calling this with no arguments gives default hyperparameters\njulia> mps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, opts);\nGenerating initial weight MPS with bond dimension χ_init = 4\n        using random state 1234.\nInitialising train states.\nInitialising test states.\nUsing 1 iterations per update.\nTraining KL Div. 122.43591167452153 | Training acc. 0.51.\nTest KL Div. 121.83350501986212 | Testing acc. 0.55.\n\nTest conf: [55 45; 45 55].\nUsing optimiser CustomGD with the \"TSGO\" algorithm\nStarting backward sweeep: [1/5]\nBackward sweep finished.\nStarting forward sweep: [1/5]\n    ...\n\nMPS normalised!\n\nTraining KL Div. -18.149569463050405 | Training acc. 1.0.\nTest KL Div. -1.2806885386973699 | Testing acc. 0.925.\n\nTest conf: [100 0; 15 85]. ","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"fitMPS doesn't use X_test or y_test for anything except printing performance evaluations, so it is safe to leave them blank. For unsupervised learning, input a dataset with only one class, or only pass X_train ( y_train has a default value of zeros(Int, size(X_train, 1)) ).","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"The mps::TrainedMPS can be passed directly to classify for classification, or init_imputation_problem to set up an imputation problem. The info info provides a short training summary, which can be pretty-printed with sweep_summary.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"You can use test_states to print a summary of the MPS performance on the test set.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"Julia> get_training_summary(mps, test_states; print_stats=true)\n\n         Overlap Matrix\n┌──────┬───────────┬───────────┐\n│      │   |ψ1⟩    │   |ψ2⟩    │\n├──────┼───────────┼───────────┤\n│ ⟨ψ1| │ 5.022e-01 │ 2.216e-04 │\n├──────┼───────────┼───────────┤\n│ ⟨ψ2| │ 2.216e-04 │ 4.978e-01 │\n└──────┴───────────┴───────────┘\n          Confusion Matrix\n┌──────────┬───────────┬───────────┐\n│          │ Pred. |1⟩ │ Pred. |2⟩ │\n├──────────┼───────────┼───────────┤\n│ True |1⟩ │       100 │         0 │\n├──────────┼───────────┼───────────┤\n│ True |2⟩ │        15 │        85 │\n└──────────┴───────────┴───────────┘\n┌───────────────────┬───────────┬──────────┬──────────┬─────────────┬─────────┬───────────┐\n│ test_balanced_acc │ train_acc │ test_acc │ f1_score │ specificity │  recall │ precision │\n│           Float64 │   Float64 │  Float64 │  Float64 │     Float64 │ Float64 │   Float64 │\n├───────────────────┼───────────┼──────────┼──────────┼─────────────┼─────────┼───────────┤\n│             0.925 │       1.0 │    0.925 │ 0.924576 │       0.925 │   0.925 │  0.934783 │\n└───────────────────┴───────────┴──────────┴──────────┴─────────────┴─────────┴───────────┘","category":"page"},{"location":"classification/#Hyperparameters","page":"Classification","title":"Hyperparameters","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"There are number of hyperparameters and data preprocessing options that can be specified using MPSOptions(; key=value)","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"MPSOptions","category":"page"},{"location":"classification/#MPSTime.MPSOptions","page":"Classification","title":"MPSTime.MPSOptions","text":"MPSOptions(; <Keyword Arguments>)\n\nSet the hyperparameters and other options for fitMPS. \n\nFields:\n\nLogging\n\nverbosity::Int=1: How much debug/progress info to print to the terminal while optimising the MPS. Higher numbers mean more output\nlog_level::Int=3: How much statistical output. 0 for nothing, >0 to print losses, accuracies, and confusion matrix at each step (noticeable) computational overhead) #TODO implement finer grain control\ntrack_cost::Bool=false: Whether to print the cost at each Bond tensor site to the terminal while training, mostly useful for debugging new cost functions or optimisers (HUGE computational overhead)\n\nMPS Training Hyperparameters\n\nnsweeps::Int=5: Number of MPS optimisation sweeps to perform (Both forwards and Backwards)\nchi_max::Int=25: Maximum bond dimension allowed within the MPS during the SVD step\neta::Float64=0.01: The learning rate. For gradient descent methods, this is the step size. For Optim and OptimKit this serves as the initial step size guess input into the linesearch\nd::Int=5: The dimension of the feature map or \"Encoding\". This is the true maximum dimension of the feature vectors. For a splitting encoding, d = numsplits * auxbasis_dim\ncutoff::Float64=1E-10: Size based cutoff for the number of singular values in the SVD (See Itensors SVD documentation)\ndtype::DataType=Float64 or ComplexF64 depending on encoding: The datatype of the elements of the MPS. Supports the arbitrary precsion types such as BigFloat and Complex{BigFloat}\nexit_early::Bool=false: Stops training if training accuracy is 1 at the end of any sweep.\n\nEncoding Options\n\nencoding::Symbol=:Legendre: The encoding to use, including :Stoudenmire, :Fourier, :Legendre, :SLTD, :Custom, etc. see Encoding docs for a complete list. Can be just a time (in)dependent orthonormal basis, or a time (in)dependent basis mapped onto a number of \"splits\" which distribute tighter basis functions where the sites of a timeseries are more likely to be measured.  \nprojected_basis::Bool=false: Whether toproject a basis onto the training data at each time. Normally, when specifying a basis of dimension d, the first d lowest order terms are used. When project=true, the training data is used to construct a pdf of the possible timeseries amplitudes at each time point. The first d largest terms of this pdf expanded in a series are used to select the basis terms.\naux_basis_dim::Int=2: Unused for standard encodings. If the encoding is a SplitBasis, serves as the auxilliary dimension of a basis mapped onto the split encoding, so that the number of histogram bins = d / auxbasisdim. \nencode_classes_separately::Bool=false: Only relevant for data driven bases. If true, then data is split up by class before being encoded. Functionally, this causes the encoding method to vary depending on the class\n\nData Preprocessing and MPS initialisation\n\nsigmoid_transform::Bool: Whether to apply a sigmoid transform to the data before minmaxing. This has the form\n\nboldsymbolX = left(1 + exp-fracboldsymbolX-m_boldsymbolXr_boldsymbolX  135right)^-1\n\nwhere boldsymbolX is the un-normalized time-series data matrix, m_boldsymbolX is the median of boldsymbolX and r_boldsymbolXis its interquartile range.\n\nminmax::Bool: Whether to apply a minmax norm to [0,1] before encoding. This has the form\n\nboldsymbolX =  fracboldsymbolX - x_textminx_textmax - x_textmin\n\nwhere boldsymbolX is the scaled robust-sigmoid transformed data matrix, x_textmin and x_textmax are the minimum and maximum of boldsymbolX.\n\ndata_bounds::Tuple{Float64, Float64} = (0.,1.): The region to bound the data to if minmax=true. This is separate from the encoding domain. All encodings expect data to be scaled scaled between 0 and 1. Setting the data bounds a bit away from [0,1] can help when your basis has poor support near its boundaries.\ninit_rng::Int: Random seed used to generate the initial MPS\nchi_init::Int: Initial bond dimension of the random MPS\n\nLoss Functions and Optimisation Methods\n\nloss_grad::Symbol=:KLD: The type of cost function to use for training the MPS, typically Mean Squared Error (:MSE) or KL Divergence (:MSE), but can also be a weighted sum of the two (:Mixed)\nbbopt::Symbol=:TSGO: Which local Optimiser to use, builtin options are symbol gradient descent (:GD), or gradient descent with a TSGO rule (:TSGO). Other options are Conjugate Gradient descent using either the Optim or OptimKit packages (:Optim or :OptimKit respectively). The CGD methods work well for MSE based loss functions, but seem to perform poorly for KLD base loss functions.\nrescale::Tuple{Bool,Bool}=(false,true): Has the form rescale = (before::Bool, after::Bool). Where to enforce the normalisation of the MPS during training, either calling normalise!(Bond Tensor) before or after BT is updated. Note that for an MPS that starts in canonical form, rescale = (true,true) will train identically to rescale = (false, true) but may be less performant.\nupdate_iters::Int=1: Maximum number of optimiser iterations to perform for each bond tensor optimisation. E.G. The number of steps of (Conjugate) Gradient Descent used by TSGO, Optim or OptimKit\ntrain_classes_separately::Bool=false: Whether the the trainer optimises the total MPS loss over all classes or whether it considers each class as a separate problem. Should make very little diffence\n\nDebug\n\nreturn_encoding_meta_info::Bool=false: Debug flag: Whether to return the normalised data as well as the histogram bins for the splitbasis types\n\n\n\n\n\nConvert the internal Options type into a serialisable MPSOptions.\n\n\n\n\n\n","category":"type"},{"location":"classification/","page":"Classification","title":"Classification","text":"You can also print a formatted table of options with print_opts (beware long output)","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"print_opts(opts)","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"┌────────────┬──────────────┬──────────────────────────┬────────┬───────────────────────────┬──────────┬─────────┬─────────┬──────────┬──────────────────┬───────────────────┬───────────┬─────────────────────────┬──────────┬───────┬────────────┬───────────────────────────┬─────────┬───────────────────┬───────────────┬────────┬───────────┬───────────┬─────────────────┬─────────┐\n│ track_cost │ update_iters │ train_classes_separately │ minmax │ return_encoding_meta_info │    dtype │ nsweeps │  cutoff │ chi_init │         encoding │           rescale │ loss_grad │             data_bounds │ init_rng │     d │ exit_early │ encode_classes_separately │     eta │ sigmoid_transform │ aux_basis_dim │  bbopt │ log_level │ verbosity │ projected_basis │ chi_max │\n│       Bool │        Int64 │                     Bool │   Bool │                      Bool │ DataType │   Int64 │ Float64 │    Int64 │           Symbol │ Tuple{Bool, Bool} │    Symbol │ Tuple{Float64, Float64} │    Int64 │ Int64 │       Bool │                      Bool │ Float64 │              Bool │         Int64 │ Symbol │     Int64 │     Int64 │            Bool │   Int64 │\n├────────────┼──────────────┼──────────────────────────┼────────┼───────────────────────────┼──────────┼─────────┼─────────┼──────────┼──────────────────┼───────────────────┼───────────┼─────────────────────────┼──────────┼───────┼────────────┼───────────────────────────┼─────────┼───────────────────┼───────────────┼────────┼───────────┼───────────┼─────────────────┼─────────┤\n│      false │            1 │                    false │   true │                     false │  Float64 │       5 │ 1.0e-10 │        4 │ Legendre_No_Norm │     (false, true) │       KLD │              (0.0, 1.0) │     1234 │     5 │      false │                     false │    0.01 │              true │             2 │   TSGO │         3 │         1 │           false │      25 │\n└────────────┴──────────────┴──────────────────────────┴────────┴───────────────────────────┴──────────┴─────────┴─────────┴──────────┴──────────────────┴───────────────────┴───────────┴─────────────────────────┴──────────┴───────┴────────────┴───────────────────────────┴─────────┴───────────────────┴───────────────┴────────┴───────────┴───────────┴─────────────────┴─────────┘\n\njulia> ","category":"page"},{"location":"classification/#Classification-2","page":"Classification","title":"Classification","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"To predict the class of unseen data, use the classify function.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"classify(::TrainedMPS, ::AbstractMatrix)","category":"page"},{"location":"classification/#MPSTime.classify-Tuple{TrainedMPS, AbstractMatrix}","page":"Classification","title":"MPSTime.classify","text":"classify(mps::TrainedMPS, X_test::AbstractMatrix)) -> (predictions::Vector)\n\nUse the mps to predict the class of the rows of X_test by computing the maximum overlap.\n\nExample\n\njulia> W, info, test_states = fitMPS( X_train, y_train);\njulia> preds  = classify(W, X_test); # make some predictions\njulia> mean(preds .== y_test)\n0.9504373177842566\n\n\n\n\n\n","category":"method"},{"location":"classification/","page":"Classification","title":"Classification","text":"For example, for the noisy trendy sine from earlier:","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"julia> predictions = classify(mps, X_test);\njulia> using Statistics\njulia> mean(predictions .== y_test)\n0.925","category":"page"},{"location":"classification/#Training-with-a-custom-basis","page":"Classification","title":"Training with a custom basis","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"To train with a custom basis, first, declare a custom basis with function_basis, and pass it in as the last argument to fitMPS. For this to work, the encoding hyperparameter must be set to :Custom in MPSOptions","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"encoding = function_basis(...)\nfitMPS(X_train, y_train, X_test, y_test, MPSOptions(; encoding=:Custom), encoding)","category":"page"},{"location":"classification/#Docstrings","page":"Classification","title":"Docstrings","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"fitMPS(::Matrix, ::Vector, ::Matrix, ::Vector, ::MPSOptions, ::Nothing)\nsweep_summary(info)\nget_training_summary(mps::TrainedMPS, test_states::EncodedTimeSeriesSet)\nprint_opts","category":"page"},{"location":"classification/#MPSTime.fitMPS-Tuple{Matrix, Vector, Matrix, Vector, MPSOptions, Nothing}","page":"Classification","title":"MPSTime.fitMPS","text":"fitMPS(X_train::AbstractMatrix, \n       y_train::AbstractVector=zeros(Int, size(X_train, 1)), \n       X_test::AbstractMatrix=zeros(0,0), \n       y_test::AbstractVector=zeros(Int, 0), \n       opts::AbstractMPSOptions=MPSOptions(),\n       custom_encoding::Union{Encoding, Nothing}=nothing) -> (MPS::TrainedMPS, training_info::Dict, encoded_test_states::EncodedTimeSeriesSet)\n\nTrain an MPS on the data X_train using the hyperparameters opts, see MPSOptions. The number of classes are determined by the entries of y_train.\n\nReturns a trained MPS, a dictionary containing training info, and the encoded test states. X_test and y_test are used only to print performance evaluations, and may be empty. The custom_encoding argument allows the use of user defined custom encodings, see function_basis. This requires that encoding=:Custom is specified in MPSOptions\n\nNOTE: the return value encoded_test_states will be sorted by class, so predictions shouldn't be compared directly to y_test. \n\nSee also: Encoding\n\nExample\n\nSee ??fitMPS to for a more verbose example\n\njulia> opts = MPSOptions(; d=5, chi_max=30, encoding=:Legendre, eta=0.05);\njulia> print_opts(opts) # Prints options as a table\n       ...\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\nGenerating initial weight MPS with bond dimension χ_init = 4\n        using random state 1234.\nInitialising train states.\nUsing 1 iterations per update.\nTraining KL Div. 28.213032851945012 | Training acc. 0.31343283582089554.\nUsing optimiser CustomGD with the \"TSGO\" algorithm\nStarting backward sweeep: [1/5]\n        ...\n\nStarting forward sweep: [5/5]\nFinished sweep 5. Time for sweep: 0.76s\nTraining KL Div. -12.577920427063361 | Training acc. 1.0.\n\nMPS normalised!\n\nTraining KL Div. -12.57792042706337 | Training acc. 1.0.\nTest KL Div. -9.815236609211746 | Testing acc. 0.9504373177842566.\n\nTest conf: [497 16; 35 481].\n\njulia> \n\n\nExtended help\n\njulia> Using JLD2 # load some data\njulia> dloc = \"test/Data/italypower/datasets/ItalyPowerDemandOrig.jld2\"\njulia> f = jldopen(dloc, \"r\") \n           X_train = read(f, \"X_train\")\n           y_train = read(f, \"y_train\")\n           X_test = read(f, \"X_test\")\n           y_test = read(f, \"y_test\")\n       close(f);\njulia> opts = MPSOptions(; d=5, chi_max=30, encoding=:Legendre, eta=0.05);\njulia> print_opts(opts) # Prints options as a table\n       ...\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\nGenerating initial weight MPS with bond dimension χ_init = 4\n        using random state 1234.\nInitialising train states.\nUsing 1 iterations per update.\nTraining KL Div. 28.213032851945012 | Training acc. 0.31343283582089554.\nUsing optimiser CustomGD with the \"TSGO\" algorithm\nStarting backward sweeep: [1/5]\n        ...\n\nStarting forward sweep: [5/5]\nFinished sweep 5. Time for sweep: 0.76s\nTraining KL Div. -12.577920427063361 | Training acc. 1.0.\n\nMPS normalised!\n\nTraining KL Div. -12.57792042706337 | Training acc. 1.0.\nTest KL Div. -9.815236609211746 | Testing acc. 0.9504373177842566.\n\nTest conf: [497 16; 35 481].\n\njulia> get_training_summary(W, test_states; print_stats=true);\n         Overlap Matrix\n┌──────┬───────────┬───────────┐\n│      │   |ψ0⟩    │   |ψ1⟩    │\n├──────┼───────────┼───────────┤\n│ ⟨ψ0| │ 5.074e-01 │ 1.463e-02 │\n├──────┼───────────┼───────────┤\n│ ⟨ψ1| │ 1.463e-02 │ 4.926e-01 │\n└──────┴───────────┴───────────┘\n          Confusion Matrix\n┌──────────┬───────────┬───────────┐\n│          │ Pred. |0⟩ │ Pred. |1⟩ │\n├──────────┼───────────┼───────────┤\n│ True |0⟩ │       497 │        16 │\n├──────────┼───────────┼───────────┤\n│ True |1⟩ │        35 │       481 │\n└──────────┴───────────┴───────────┘\n┌───────────────────┬───────────┬──────────┬──────────┬─────────────┬──────────┬───────────┐\n│ test_balanced_acc │ train_acc │ test_acc │ f1_score │ specificity │   recall │ precision │\n│           Float64 │   Float64 │  Float64 │  Float64 │     Float64 │  Float64 │   Float64 │\n├───────────────────┼───────────┼──────────┼──────────┼─────────────┼──────────┼───────────┤\n│          0.950491 │       1.0 │ 0.950437 │ 0.950425 │    0.950491 │ 0.950491 │  0.951009 │\n└───────────────────┴───────────┴──────────┴──────────┴─────────────┴──────────┴───────────┘\n\njulia> sweep_summary(info)\n┌────────────────┬──────────┬───────────────┬───────────────┬───────────────┬───────────────┬───────────────┬────────────┬──────────┐\n│                │ Initial  │ After Sweep 1 │ After Sweep 2 │ After Sweep 3 │ After Sweep 4 │ After Sweep 5 │ After Norm │   Mean   │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│ Train Accuracy │ 0.313433 │      1.0      │      1.0      │      1.0      │      1.0      │      1.0      │    1.0     │   1.0    │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│  Test Accuracy │ 0.409135 │   0.947522    │   0.951409    │   0.948494    │   0.948494    │   0.950437    │  0.950437  │ 0.949271 │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│  Train KL Div. │  28.213  │   -11.7855    │    -12.391    │   -12.4831    │   -12.5466    │   -12.5779    │  -12.5779  │ -12.3568 │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│   Test KL Div. │ 27.7435  │   -9.12893    │   -9.73479    │   -9.79248    │    -9.8158    │   -9.81524    │  -9.81524  │ -9.65745 │\n├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤\n│     Time taken │   0.0    │   0.658366    │    0.75551    │   0.719035    │   0.718444    │    1.16256    │    NaN     │ 0.802783 │\n└────────────────┴──────────┴───────────────┴───────────────┴───────────────┴───────────────┴───────────────┴────────────┴──────────┘\n\n\n\n\n\n\n","category":"method"},{"location":"classification/#MPSTime.sweep_summary-Tuple{Any}","page":"Classification","title":"MPSTime.sweep_summary","text":"sweep_summary(info; io::IO=stdin)\n\nPrint a pretty summary of what happened in every sweep\n\n\n\n\n\n","category":"method"},{"location":"classification/#MPSTime.get_training_summary-Tuple{TrainedMPS, EncodedTimeSeriesSet}","page":"Classification","title":"MPSTime.get_training_summary","text":"get_training_summary(mps::TrainedMPS, \n                     test_states::EncodedTimeSeriesSet;  \n                     print_stats::Bool=false, \n                     io::IO=stdin) -> stats::Dict\n\nPrint a summary of the training process of mps, with performane evaluated on test_states.\n\n\n\n\n\n","category":"method"},{"location":"classification/#MPSTime.print_opts","page":"Classification","title":"MPSTime.print_opts","text":"print_opts(opts::AbstractMPSOptions; io::IO=stdin)\n\nPrint the MPSOptions struct in a table.\n\n\n\n\n\n","category":"function"},{"location":"encodings/#Encodings","page":"Encodings","title":"Encodings","text":"","category":"section"},{"location":"encodings/#Overview","page":"Encodings","title":"Overview","text":"","category":"section"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"To use MPS methods on time-series data, the continuous time-series amplitudes must be mapped to MPS compatible vectors using an encoding. There are a number of encodings built into this library, and they can be specified by the encoding keyword in MPSOptions.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Encoding","category":"page"},{"location":"encodings/#MPSTime.Encoding","page":"Encodings","title":"MPSTime.Encoding","text":"Encoding\n\nAbstract supertype of all encodings. To specify an encoding for MPS training, set the encoding keyword when calling MPSOptions.\n\nExample\n\njulia> opts = MPSOptions(; encoding=:Legendre);\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\n\nEncodings\n\n:Legendre: The first d L2-normalised Legendre Polynomials. Real valued, and supports passing projected_basis=true to MPSOptions.\n:Fourier: Complex valued Fourier coefficients. Supports passing projected_basis=true to MPSOptions.\n\n    Phi(x d) = left1 + 0i e^i pi x e^-i pi x e^2i pi x e^-2i pi x ldots right  sqrtd \n\n:Stoudenmire: The original complex valued \"Spin-1/2\" encoding from Stoudenmire & Schwab, 2017 arXiv. Only supports d = 2\n\n    Phi(x) = left e^3 i pi x  2 cos(fracpi2 x)  e^-3 i pi x  2 sin(fracpi2 x)right\n\n:Sahand_Legendre_Time_Dependent:  (:SLTD) A custom, real-valued encoding constructed as a data-driven adaptation of the Legendre Polynomials. At each time point, t, the training data is used to construct a probability density function that describes the distribution of the time-series amplitude x_t. This is the first basis function. \nb_1(x t) = textpdf_x_t(x_t). This is computed with KernelDensity.jl:\n\njulia> Using KernelDensity\njulia> xs_samps = range(-1,1, max(200,size(X_train,2)))\njulia> b1(xs,t) = pdf(kde(X_train[t,:]), xs_samps)\n\nThe second basis function is the first order polynomial that is L2-orthogonal to this pdf on the interval [-1,1]. \n\nb_2(xt) = a_1 x + a_0 text where  int_-1^1 b_1(xt) b_2^*(x t) textrmd x = 0  lvertlvert b_2(x t) rvertrvert_L2 = 1\n\nThe third basis function is the second order polynomial that is L2-orthogonal to the first two basis functions on [-1,1], etc.\n\n-:Custom: For use with user-defined custom bases. See function_basis\n\n\n\n\n\n","category":"type"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Encodings can be visualized with the plot_encoding function.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(:legendre, 4)","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"For data driven bases, the data histograms can be plotted alongside for reference:","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(:sahand_legendre_time_dependent, 4, X_train; tis=[1,20]); # X_train is taken from the noisy trendy sine demo in the Imputation section","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/#Using-a-SplitBasis-encoding","page":"Encodings","title":"Using a SplitBasis encoding","text":"","category":"section"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"One way to increase the encoding dimension is to repeat a basis many times across the encoding domain in 'bins'. In theory, this can be advantageous when data is concentrated in narrow regions in the encoding domain, as very fine bins can be used to reduce encoding error in well-populated regions, while computational effort can be saved with wide bins in sparsely-population regions. To this end, we provide the \"Split\" bases.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"The uniform-split encoding, which simply bins data up as a proof of concept:","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(uniform_split(:legendre), 8, X_train; tis=[1,20], aux_basis_dim=4);","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"And the histogram-split encoding, which narrows the bins in frequently occurring regions.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(histogram_split(:legendre), 8, X_train; tis=[1,20], aux_basis_dim=4);","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Every data-independent encoding can be histogram split and uniform split, including other split bases:","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"basis, p = plot_encoding(histogram_split(uniform_split(:legendre)), 16, X_train; tis=[1,20], aux_basis_dim=8, size=(1600,900));","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"(Image: )","category":"page"},{"location":"encodings/#Custom-encodings","page":"Encodings","title":"Custom encodings","text":"","category":"section"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"Custom encodings can be declared using function_basis.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"function_basis","category":"page"},{"location":"encodings/#MPSTime.function_basis","page":"Encodings","title":"MPSTime.function_basis","text":"function_basis(basis::Function, is_complex::Bool, range::Tuple{<:Real,<:Real}, <args>; name::String=\"Custom\")\n\nConstructs a time-(in)dependent encoding from the function basis, which is either real or complex, and has support on the interval range.\n\nFor a time independent basis, the input function must have the signature :\n\nbasis(x::Float64, d::Integer, init_args...)\n\nand return a d-dimensional numerical Vector.  A vector x_1 x_2 x_3  x_N will be encoded as b(x_1) b(x_2) b(x_3) b(x_N)\n\nTo use a time dependent basis, set is_time_dependent to true. The input function must have the signature \n\nbasis(x::Float64, d::Integer, ti::Int, init_args...)\n\nand return a d-dimensional numerical Vector. A vector x_1 x_2 x_3  x_N will be encoded as  b_1(x_1) b_2(x_2) b_3(x_3) b_N(x_N)\n\nOptional Arguments\n\nis_time_dependent::Bool=false: Whether the basis is time dependent \nis_data_driven::Bool=false: Whether functional form of the basis depends on the training data\ninit::Function=no_init: The initialiser function for the basis. This is used to compute arguments for the function that are not known in advance,\n\nfor example, the polynomial coefficients for the Sahand-Legendre basis. This function should have the form\n\ninit_args = opts.encoding.init(X_normalised::AbstractMatrix, y::AbstractVector; opts::MPSTime.Options=opts)\n\nX_normalised will be preprocessed (with sigmoid transform and MinMax transform pre-applied), with Time series as columns\n\nExample\n\nThe Legendre Polynomial Basis:\n\njulia> Using LegendrePolynomials\njulia> function legendre_encode(x::Float64, d::Int)\n    # default legendre encoding: choose the first n-1 legendre polynomials\n\n    leg_basis = [Pl(x, i; norm = Val(:normalized)) for i in 0:(d-1)] \n    \n    return leg_basis\njulia> custom_basis = function_basis(legendre_encode, false, (-1., 1.))\n\n\n\n\n\n","category":"function"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"To use a custom encoding, you must manually pass it into fitMPS.","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"encoding = function_basis(...)\nfitMPS(X_train, y_train, X_test, y_test, MPSOptions(; encoding=:Custom), encoding)","category":"page"},{"location":"encodings/","page":"Encodings","title":"Encodings","text":"plot_encoding(::Symbol, ::Integer)","category":"page"},{"location":"encodings/#MPSTime.plot_encoding-Tuple{Symbol, Integer}","page":"Encodings","title":"MPSTime.plot_encoding","text":"plot_encoding(E::Union(Symbol, MPSTime.Encoding), \n              d::Integer, \n              X_train::Matrix{Float64}=zeros(0,0), \n              y_train::Vector{Any}=[];\n              <keyword arguments>) -> encoding::Vector, plot::Plots.Plot\n\nPlot the first d terms of the encoding E across its entire domain.\n\nX_train and y_train are only needed if E is data driven or time dependent, or plot_hist is true.\n\nKeyword Arguments\n\nplot_hist::Bool=E.isdatadriven: Whether to plot the histogram of the traing data at several time points. Useful for understanding the behviour of data-driven bases.\ntis::Vector{<:Integer} = Int[]: Time(s) to plot the Encoding at.\nds::Vector{<:Integer} = collect(1:d): Enables plotting of a subset of a d-dimensional Encoding, e.g. ds=[1,3,5] plots the first, third and fifth basis functions.\nnum_xvals::Integer=500: \nsize::Tuple=(1200, 800): \npadding::Real=6.: \n\nUsed for data-driven Encodings\n\nsigmoid_transform::Bool=false: Whether to apply a robust sigmoid transform to the training data, see MPSOptions\nminmax::Bool=true: Whether to apply a minmax normalsation to the training data after the sigmoid, see MPSOptions\ndata_bounds::Tuple{<:Real, <:Real}=(0.,1.): Whether to apply a robust sigmoid transform to the X data, see MPSOptions\nproject_basis::Bool=false: Whether to project the basis onto the data. Supported only by :legendre and :Fourier basis when E has type Symbol\naux_basis_dim::Integer=2: Dimension of each auxilliary basis. Only relevent when E is a MPSTime.SplitBasis. \n\nMisc\n\nkwargs: Passed to Plots.Plot()\n\n\n\n\n\n","category":"method"},{"location":"imputation/#Imputation_top","page":"Imputation","title":"Imputation","text":"","category":"section"},{"location":"imputation/#Overview","page":"Imputation","title":"Overview","text":"","category":"section"},{"location":"imputation/#Imputation-Scenarios","page":"Imputation","title":"Imputation Scenarios","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"MPSTime supports univariate time-series imputation with three key patterns of missing data:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Individual missing points (e.g., values missing at t = 10 20 80)\nSingle contiguous blocks (e.g., values missing from t = 25-70)\nMultiple contiguous blocks (e.g., values missing from t = 5-10, t = 25-50 and t = 80-90)","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"MPSTime can also handle any combination of these patterns. For instance, you might need to impute a single contiguous block from t = 10-30, plus individual missing points at t = 50 and t=80.","category":"page"},{"location":"imputation/#Setup","page":"Imputation","title":"Setup","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"The first step is to train an MPS.  Here, we'll train an MPS in an unsupervised manner (no class labels) using a noisy trendy sinusoid.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"# Fix rng seed\nusing Random\nrng = Xoshiro(1)\n\n# dataset size\nntimepoints = 100\nntrain_instances = 300\nntest_instances = 200\n\n# generate the train and test datasets\nX_train, _ = trendy_sine(ntimepoints, ntrain_instances; sigma=0.1, rng=rng);\nX_test, _ = trendy_sine(ntimepoints, ntest_instances; sigma=0.1, rng=rng);\n\n# hyper parameters and training\nopts = MPSOptions(d=10, chi_max=40, sigmoid_transform=false);\nmps, info, test_states= fitMPS(X_train, opts);","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Next, we initialize an imputation problem. This does a lot of necessary pre-computation:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"julia> imp = init_imputation_problem(mps, X_test);\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n                         Summary:\n\n - Dataset has 300 training samples and 200 testing samples.\nSlicing MPS into individual states...\n - 1 class(es) were detected.\n - Time independent encoding - Legendre - detected.\n - d = 10, chi_max = 40\nRe-encoding the training data to get the encoding arguments...\n\n Created 1 ImputationProblem struct(s) containing class-wise mps and test samples.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"A summary of the imputation problem setup is printed to verify the model parameters and dataset information. For multi-class data, you can pass y_test to init_imputation_problem in order to exploit the labels / class information while doing imputation.","category":"page"},{"location":"imputation/#Imputing-missing-values","page":"Imputation","title":"Imputing missing values","text":"","category":"section"},{"location":"imputation/#Single-block-Imputation","page":"Imputation","title":"Single-block Imputation","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Now, decide how you want to impute the missing data. The necessary options are:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"class::Integer: The class of the time-series instance we are going to impute, leave as zero for \"unlabelled\" data (i.e., all data belong to the same class).\nimpute_sites: The MPS sites (time points) that are missing (inclusive).\ninstance_idx: The time-series instance from the chosen class in the test set.\nmethod: The imputation method to use. Can be trajectories (ITS), median, mode, mean, etc...","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"In this example, we will consider a single block of contiguous missing values, simulated from a missing-at-random mechanism (MAR). We will use the median to impute the missing values, as well as computing a 1-Nearest Neighbor Imputation (1-NNI) baseline for comparison:   ","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"class = 0\npm = 0.8 # 80% missing data\ninstance_idx = 59 # time series instance in test set\n_, impute_sites = mar(X_test[instance_idx, :], pm; state=42) # simulate MAR mechanism\nmethod = :median\n\nimputed_ts, pred_err, target_ts, stats, plots = MPS_impute(\n    imp,\n    class, \n    instance_idx, \n    impute_sites, \n    method; \n    NN_baseline=true, # whether to also do a baseline imputation using 1-NNI\n    plot_fits=true, # whether to plot the fits\n)","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Several outputs are returned from MPS_impute:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"imputed_ts: The imputed time-series instance, containing the original data points and the predicted values.\npred_err: The prediction error for each imputed value, given a known ground-truth.\ntarget_ts: The original time-series instance containing missing values.\nstats: A collection of statistical metrics (MAE and MAPE) evaluating imputation performance with respect to a ground truth. Includes baseline performance when NN_baseline=true.\nplots: Stores plot object(s) in an array for visualization when plot_fits=true.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"We can inspect the imputation performance in a summary table:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"julia> using PrettyTables\njulia> pretty_table(stats[1]; header=[\"Metric\", \"Value\"], header_crayon= crayon\"yellow bold\", tf = tf_unicode_rounded);\n╭─────────┬───────────╮\n│  Metric │     Value │\n├─────────┼───────────┤\n│     MAE │ 0.0855211 │\n│    MAPE │  0.125649 │\n│  NN_MAE │   0.11304 │\n│ NN_MAPE │  0.168085 │\n╰─────────┴───────────╯","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Here, MAE and MAPE correspond to the mean absolute error (MAE) and mean absolute percentage error (MAPE) for the MPS, while the NN_ prefix corresponds to the same errors for the 1-NNI baseline. ","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"To plot the imputed time series, we can call the plot function as follows: ","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"julia> using Plots\njulia> plot(plots...)","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"The solid orange line depicts the \"ground-truth\" (observed) time-series values, the dotted blue line is the MPS-imputed data points and the dotted red line is the 1-NNI baseline. The blue shading indicates the uncertainty due to encoding error.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"There are a lot of other options, and many more imputation methods to choose from! See MPS_impute for more details.","category":"page"},{"location":"imputation/#Multi-block-Imputation","page":"Imputation","title":"Multi-block Imputation","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Building on the previous example of single-block imputation, MPSTime can also be used to impute missing values in multiple blocks of contiguous points.  For example, consider missing points between t = 10-25, t = 40-60 and t = 75-90:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"class = 0\nimpute_sites = vcat(collect(10:25), collect(40:60), collect(65:90))\ninstance_idx = 32\nmethod = :median\n\nimputed_ts, pred_err, target_ts, stats, plots = MPS_impute(\n    imp,\n    class, \n    instance_idx, \n    impute_sites, \n    method; \n    NN_baseline=true, # whether to also do a baseline imputation using 1-NNI\n    plot_fits=true, # whether to plot the fits\n)","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/#Individual-Point-Imputation","page":"Imputation","title":"Individual Point Imputation","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"To impute individual points rather than ranges of consecutive points (blocks), we can simply pass their respective time points into the imputation function as a vector:","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"impute_sites = [10] # only impute t = 10\nimpute_sites = [10, 25, 50] # impute multiple individual points","category":"page"},{"location":"imputation/#Plotting-Trajectories","page":"Imputation","title":"Plotting Trajectories","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"To plot individual trajectories from the conditional distribution, use method=:ITS.  Here, we'll plot 10 randomly selected trajectories for the missing points by setting the num_trajectories keyword: ","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"class = 0\nimpute_sites = collect(10:90)\ninstance_idx = 59\nmethod = :ITS\n\nimputed_ts, pred_err, target_ts, stats, plots = MPS_impute(\n    imp,\n    class, \n    instance_idx, \n    impute_sites, \n    method; \n    NN_baseline=false, # whether to also do a baseline imputation using 1-NN\n    plot_fits=true, # whether to plot the fits\n    num_trajectories=10, # number of trajectories to plot\n    rejection_threshold=2.5 # limits how unlikely we allow the random trajectories to be.\n    # there are more options! see [`MPS_impute`](@ref)\n)\n\nplot(plots...)","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/#Plotting-cumulative-distribution-functions","page":"Imputation","title":"Plotting cumulative distribution functions","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"It can be interesting to inspect the probability distribution being sampled from at each missing time point.  To enable this, we provide the get_cdfs function, which works very similarly to MPS_impute, only it returns the CDF at each missing time point in the encoding domain.","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"cdfs, ts, pred_err, target = get_cdfs(\n    imp, \n    class, \n    instance_idx, \n    impute_sites\n    );\n\nxvals = imp.x_guess_range.xvals[1:10:end]\n\nplot(xvals, cdfs[1][1:10:end]; legend=:none)\np = last([plot!(xvals, cdfs[i][1:10:end]) for i in eachindex(cdfs)])\nylabel!(\"cdf(x)\")\nxlabel!(\"x_t\")\ntitle!(\"CDF at each time point.\")","category":"page"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"(Image: )","category":"page"},{"location":"imputation/#Docstrings","page":"Imputation","title":"Docstrings","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"init_imputation_problem(::TrainedMPS, ::Matrix)\nMPS_impute\nget_cdfs\ntrendy_sine\nMPSTime.state_space","category":"page"},{"location":"imputation/#MPSTime.init_imputation_problem-Tuple{TrainedMPS, Matrix}","page":"Imputation","title":"MPSTime.init_imputation_problem","text":"init_imputation_problem(W::TrainedMPS, X_test::AbstractMatrix, y_test::AbstractArray=zeros(Int, size(X_test,1)), [custom_encoding::MPSTime.Encoding]; <keyword arguments>) -> imp::ImputationProblem\ninit_imputation_problem(W::TrainedMPS, X_test::AbstractMatrix, [custom_encoding::MPSTime.Encoding]); <keyword arguments>) -> imp::ImputationProblem\n\nInitialise an imputation problem using a trained MPS and relevent test data.\n\nThis involves a lot of pre-computation, which can be quite time intensive for data-driven bases. For unclassed/unsupervised data y_test may be omitted.  If the MPS was trained with a custom encoding, then this encoding must be passed to init_imputation_problem.\n\nKeyword Arguments\n\nguess_range::Union{Nothing, Tuple{<:Real,<:Real}}=nothing: The range of values that guesses are allowed to take. This range is applied to normalised, encoding-adjusted time-series data. To allow any guess, leave as nothing, or set to encoding.range (e.g. [(-1., 1.) for the legendre encoding]).\ndx::Float64 = 1e-4: The spacing between possible guesses in normalised, encoding-adjusted units. When imputing missing data with an MPS method, the imputed values will be selected from    range(guess_range...; step=dx)\nverbosity::Integer=1: The verbosity of the initialisation process. Useful for debugging, or to completely suppress output.\ntest_encoding::Bool=true: Whether to double check the encoding and scaling options are correct. This is strongly recommended but has a slight performance cost, so may be disabled.\n\n\n\n\n\n","category":"method"},{"location":"imputation/#MPSTime.MPS_impute","page":"Imputation","title":"MPSTime.MPS_impute","text":"MPS_impute(imp::ImputationProblem, \n           class::Any, \n           instance::Integer, \n           missing_sites::AbstractVector{<:Integer}, \n           method::Symbol=:median; \n           <keyword arguments>) -> (imputed_instance::Vector, errors::Vector, target::Vector, stats::Dict, p::Vector{Plots.Plot})\n\nImpute the missing_sites using an MPS-based approach, selecting the trajectory from the conditioned distribution with method\n\nSee init_imputation_problem for constructing an ImputationProbleminstance out of a trained MPS. Theinstance` number is relative to the class, so class 1, instance 2 would be distinct from class 2, instance 2. \n\nImputation Methods\n\n:median: For each missing value, compute the probability density function of the possible outcomes from the MPS, and choose the median. This method is the most robust to outliers. Keywords:\nget_wmad::Bool=true: Whether to return an 'error' vector that computes the Weighted Median Absolute Deviation (WMAD) of each imputed value.\n:mean: For each missing value, compute the probability density function of the possible outcomes from the MPS, and choose the expected value. Keywords:\nget_std::Bool=true: Whether to return an 'error' vector that computes standard deviation of each imputed value.\n:mode: For each missing value, choose the most likely outcome predicted by the MPS. Keywords:\nmax_jump::Union{Number,Nothing}=nothing: The largest jump allowed between two adjacent imputations. Leave as nothing to allow any jump. Helpful to suppress 'spikes' caused by poor support near the encoding domain edges.\n:ITS: For each missing value, choose a value at random with probability weighted by the probability density function of the possible outcomes. Keywords:\nrseed::Integer=1: Random seed for producing the trajectories.\n`num_trajectories::Integer=1: Number of trajectories to compute.\nrejection_threshold::Union{Float64, Symbol}=:none: Number of WMADs allowed between adjacent points. Setting this low helps suppress rapidly varying trajectories that occur by bad luck. \nmax_trials::Integer=10: Number of attempts allowed to make guesses conform to rejection_threshold before giving up.\n:kNearestNeighbour: Select the k nearest neighbours in the training set using Euclidean distance to the known data. Keyword:\nk: Number of nearest neighbours to return. See kNN_impute\n\nKeyword Arguments\n\nimpute_order::Symbol=:forwards: Whether to impute the missing values :forwards (left to right) or :backwards (right to left)\nNN_baseline::Bool=true: Whether to also impute the missing data with a k-Nearest Neighbour baseline.\nn_baselines::Integer=1: How many nearest neighbour baselines to compute.\nplot_fits::Bool=true: Whether to make a plot showing the target timeseries, the missing values, and the imputed region. If false, then p will be an empty vector. The plot will show the NN_baseline (if it was computed), as well as every trajectory if using the :ITS method.\nget_metrics::Bool=true: Whether to compute imputation metrics, if false, then stats, will be empty.\nfull_metrics::Bool=false: Whether to compute every metric (MAPE, SMAPE, MAE, MSE, RMSE) or just MAE and MAPE.\nprint_metric_table::Bool=false: Whether to print the stats as a table.\ninvert_transform::Bool=true:, # Whether to undo the sigmoid transform/minmax normalisation before returning the imputed points. If this is false, imputed_instance, errors, target timeseries, stats, and plot y-axis will all be scaled by the data preprocessing / normalisation and fit to the encoding domain.\nkwargs...: Extra keywords passed to the imputation method. See the Imputation Methods section.\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.get_cdfs","page":"Imputation","title":"MPSTime.get_cdfs","text":"get_cdfs(imp::ImputationProblem, \n           class::Any, \n           instance::Integer, \n           missing_sites::AbstractVector{<:Integer}, \n           method::Symbol=:median; \n           <keyword arguments>) -> (cdfs::Vector{Vector}, ts::Vector, pred_err::Vector, target_timeseries_full::Vector)\n\nImpute the missing_sites using an MPS-based approach, selecting the trajectory from the conditioned distribution with method, and returns the cumulative distribution function used to infer each missing value. \n\nSee MPS_impute for a list of imputation methods and keyword arguments (does not support plotting, stats, or kNN baselines). See init_imputation_problem for constructing an ImputationProblem instance. The instance number is relative to the class, so class 1, instance 2 would be distinct from class 2, instance 2.\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.trendy_sine","page":"Imputation","title":"MPSTime.trendy_sine","text":"trendy_sine(T::Int, n::Int; period=nothing, slope=nothing, phase=nothing, sigma=0.0, \n    rng=Random.GLOBAL_RNG, return_metadata=true) -> Tuple{Matrix{Float64}, Dict{Symbol, Any}}\n\nGenerate n time series of length T, each composed of a sine wave with an optional linear trend and Gaussian noise defined by the equation:\n\nx_t = sinleft(frac2pitaut + psiright) + fracmtT + sigma n_t\n\nwith period tau, time point t, linear trend slope m, phase offset psi, noise scale sigma and n_t sim N(01)\n\nArguments\n\nT::Int: Length of each time series\nn::Int: Number of time series instances to generate\n\nKeyword Arguments\n\nperiod: Period of the sinusoid, τ\nnothing: Random values between 1.0 and 50.0 (default)\nFloat64: Fixed period for all time series\nTuple: Bounds for uniform random values, e.g., (1.0, 20.0) → τ ~ U(1.0, 20.0)\nVector: Sample from discrete uniform distribution, e.g., τ ∈ 10, 20, 30\nslope: Linear trend gradient, m\nnothing: Random values bewteen -5.0 and 5.0 (default)\nFloat64: Fixed slope for all time series\nTuple: Bounds for uniform random values, e.g., (-3.0, 3.0) → m ~ U(-3.0, 3.0)\nVector: Sample from discrete uniform distribution, e.g., m ∈ -3.0, 0.0, 3.0\nphase: Phase offset, ψ\nnothing: Random values between 0 and 2π (default)\nFloat64: Fixed phase for all time series\nTuple: Bounds for uniform random values, e.g., (0.0, π) → ψ ~ U(0.0, π)\nVector: Sample from discrete uniform distribution\nsigma::Real: Standard deviation of Gaussian noise, σ (default: 0.0)\nrng::AbstractRNG: Random number generator for reproducibility (default: Random.GLOBAL_RNG)\nreturn_metadata::Bool: Return generation parameters (default: true)\n\nReturns\n\nMatrix{Float64} of shape (n, T)\nDictionary of generation parameters (:period, :slope, :phase, :sigma, :T, :n)\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.state_space","page":"Imputation","title":"MPSTime.state_space","text":"state_space(T::Int, n::Int, s::Int=2; sigma::Float64=0.3, rng::AbstractRNG}=Random.GLOBAL_RNG) -> Matrix{Float64}\n\nGenerate n time series of length T each from a state space model with residual terms drawn from a normal distribution N(0, sigma) and lag order s. Time series are generated from the following model:\n\nx_t = mu_t + theta_t + eta_t\nmu_t = mu_t-1 + lambda_t-1 + xi_t\nlambda_t = lambda_t-1 + zeta_t\ntheta_t = sum_j=1^s-1 - theta_t-j + omega_t\n\nwhere x_t is the t-th value in the time series, and the residual terms eta_t, xi_t, zeta_t and omega_t are randomly drawn from a normal distribution N(0 sigma).\n\nArguments\n\nT – Time series length.\nn – Number of time-series instances.\n\nKeyword Arguments\n\ns – Lag order (optional, default: 2).\nsigma – Noise standard deviation (optional, default: 0.3).\nrng – Random number generator of type AbstractRNG (optional, default: Random.GLOBAL_RNG).\n\nReturns\n\nA Matrix{Float64} of shape (n, T) containing the simulated time-series instances. \n\n\n\n\n\n","category":"function"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"Internal imputation methods:","category":"page"},{"location":"imputation/#Internal-imputation-methods","page":"Imputation","title":"Internal imputation methods","text":"","category":"section"},{"location":"imputation/","page":"Imputation","title":"Imputation","text":"MPSTime.impute_median\nMPSTime.impute_ITS\nMPSTime.kNN_impute","category":"page"},{"location":"imputation/#MPSTime.impute_median","page":"Imputation","title":"MPSTime.impute_median","text":"impute missing data points using the median of the conditional distribution (single site rdm ρ).\n\nArguments\n\nclass_mps::MPS: \nopts::Options: MPS parameters.\nenc_args::AbstractVector\nx_guess_range::EncodedDataRange\ntimeseries::AbstractVector{<:Number}: The input time series data that will be imputed.\ntimeseries_enc::MPS: The encoded version of the time series represented as a product state. \nimputation_sites::Vector{Int}: Indices in the time series where imputation is to be performed.\nget_wmad::Bool: Whether to compute the weighted median absolute deviation (WMAD) during imputation (default is false).\n\nReturns\n\nA tuple containing:\n\nmedian_values::Vector{Float64}: The imputed median values at the specified imputation sites.\nwmad_value::Union{Nothing, Float64}: The weighted median absolute deviation if get_wmad is true; otherwise, nothing.\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.impute_ITS","page":"Imputation","title":"MPSTime.impute_ITS","text":"Impute a SINGLE trajectory using inverse transform sampling (ITS).\n\n\n\n\n\n","category":"function"},{"location":"imputation/#MPSTime.kNN_impute","page":"Imputation","title":"MPSTime.kNN_impute","text":"kNN_impute(imp::ImputationProblem, \n           class::Any, instance::Integer, \n           missing_sites::AbstractVector{<:Integer}; \n           k::Integer=1) -> [neighbour1::Vector, neighbour2::Vector, ...]\n\nImpute missing_sites using the k nearest neighbours in the test set, based on Euclidean distance.\n\nSee init_imputation_problem for constructing an ImputationProblem instance. The instance number is relative to the class, so class 1, instance 2 would be distinct from class 2, instance 2.\n\n\n\n\n\n","category":"function"},{"location":"#MPSTime.jl","page":"Introduction","title":"MPSTime.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"A Julia package for time-series machine learning (ML) using Matrix-Product States (MPS) built on the ITensors.jl framework [1, 2].","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"#Overview","page":"Introduction","title":"Overview","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"MPSTime is a Julia package for learning the joint probability distribution of time series directly from data using matrix product state (MPS) methods inspired by quantum many-body physics.  It provides a unified formalism for:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Time-series classification (inferring the class of unseen time-series).\nUnivariate time-series imputation (inferring missing points within time-series instances) across fixed-length time series.\nSynthetic data generation (coming soon).","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Info\nMPSTime is currently under active development. Many features are in an experimental stage and may undergo significant changes, refinements, or removals.","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This is not yet a registered Julia package, but it will be soon (TM)! In the meantime, you can install it directly from our GitHub repository:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> ]\npkg> add https://github.com/joshuabmoore/MPSTime.jl.git ","category":"page"},{"location":"#Usage","page":"Introduction","title":"Usage","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"See the sidebars for basic usage examples.  We're continually adding more features and documentation as we go.","category":"page"},{"location":"#Citation","page":"Introduction","title":"Citation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"If you use MPSTime in your work, please read and cite the arXiv preprint:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"@misc{MPSTime2024,\n      title={Using matrix-product states for time-series machine learning}, \n      author={Joshua B. Moore and Hugo P. Stackhouse and Ben D. Fulcher and Sahand Mahmoodian},\n      year={2024},\n      eprint={2412.15826},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML},\n      url={https://arxiv.org/abs/2412.15826}, \n}","category":"page"},{"location":"docstrings/#Docstrings","page":"Docstrings","title":"Docstrings","text":"","category":"section"},{"location":"docstrings/#Useful-Structs","page":"Docstrings","title":"Useful Structs","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"MPSTime.Encoding","category":"page"},{"location":"docstrings/#MPSTime.Encoding-docstrings","page":"Docstrings","title":"MPSTime.Encoding","text":"Encoding\n\nAbstract supertype of all encodings. To specify an encoding for MPS training, set the encoding keyword when calling MPSOptions.\n\nExample\n\njulia> opts = MPSOptions(; encoding=:Legendre);\njulia> W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);\n\nEncodings\n\n:Legendre: The first d L2-normalised Legendre Polynomials. Real valued, and supports passing projected_basis=true to MPSOptions.\n:Fourier: Complex valued Fourier coefficients. Supports passing projected_basis=true to MPSOptions.\n\n    Phi(x d) = left1 + 0i e^i pi x e^-i pi x e^2i pi x e^-2i pi x ldots right  sqrtd \n\n:Stoudenmire: The original complex valued \"Spin-1/2\" encoding from Stoudenmire & Schwab, 2017 arXiv. Only supports d = 2\n\n    Phi(x) = left e^3 i pi x  2 cos(fracpi2 x)  e^-3 i pi x  2 sin(fracpi2 x)right\n\n:Sahand_Legendre_Time_Dependent:  (:SLTD) A custom, real-valued encoding constructed as a data-driven adaptation of the Legendre Polynomials. At each time point, t, the training data is used to construct a probability density function that describes the distribution of the time-series amplitude x_t. This is the first basis function. \nb_1(x t) = textpdf_x_t(x_t). This is computed with KernelDensity.jl:\n\njulia> Using KernelDensity\njulia> xs_samps = range(-1,1, max(200,size(X_train,2)))\njulia> b1(xs,t) = pdf(kde(X_train[t,:]), xs_samps)\n\nThe second basis function is the first order polynomial that is L2-orthogonal to this pdf on the interval [-1,1]. \n\nb_2(xt) = a_1 x + a_0 text where  int_-1^1 b_1(xt) b_2^*(x t) textrmd x = 0  lvertlvert b_2(x t) rvertrvert_L2 = 1\n\nThe third basis function is the second order polynomial that is L2-orthogonal to the first two basis functions on [-1,1], etc.\n\n-:Custom: For use with user-defined custom bases. See function_basis\n\n\n\n\n\n","category":"type"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"MPSTime.EncodedTimeSeriesSet\nTrainedMPS","category":"page"},{"location":"docstrings/#MPSTime.EncodedTimeSeriesSet","page":"Docstrings","title":"MPSTime.EncodedTimeSeriesSet","text":"EncodedTimeSeriesSet\n\nHolds an encoded time-series dataset, as well as a copy of the original data and its class distribution.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MPSTime.TrainedMPS","page":"Docstrings","title":"MPSTime.TrainedMPS","text":"TrainedMPS\n\nContainer for a trained MPS and its associated Options and training data.\n\nFields\n\nmps::MPS: A trained Matrix Product state.\nopts::MPSOptions: User defined MPSOptions used to create the MPS.\ntrain_data::EncodedTimeSeriesSet: Stores both the raw and encoded data used to train the mps.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#Hyperparameters","page":"Docstrings","title":"Hyperparameters","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"MPSTime.AbstractMPSOptions","category":"page"},{"location":"docstrings/#MPSTime.AbstractMPSOptions","page":"Docstrings","title":"MPSTime.AbstractMPSOptions","text":"AbstractMPSOptions\n\nAbstract supertype of \"MPSOptions\", a collection of concrete types which is used to specify options for training, and \"Options\", which is used internally and contains references to internal objects\n\n\n\n\n\n","category":"type"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"MPSTime.MPSOptions","category":"page"},{"location":"docstrings/#MPSTime.MPSOptions-docstrings","page":"Docstrings","title":"MPSTime.MPSOptions","text":"MPSOptions(; <Keyword Arguments>)\n\nSet the hyperparameters and other options for fitMPS. \n\nFields:\n\nLogging\n\nverbosity::Int=1: How much debug/progress info to print to the terminal while optimising the MPS. Higher numbers mean more output\nlog_level::Int=3: How much statistical output. 0 for nothing, >0 to print losses, accuracies, and confusion matrix at each step (noticeable) computational overhead) #TODO implement finer grain control\ntrack_cost::Bool=false: Whether to print the cost at each Bond tensor site to the terminal while training, mostly useful for debugging new cost functions or optimisers (HUGE computational overhead)\n\nMPS Training Hyperparameters\n\nnsweeps::Int=5: Number of MPS optimisation sweeps to perform (Both forwards and Backwards)\nchi_max::Int=25: Maximum bond dimension allowed within the MPS during the SVD step\neta::Float64=0.01: The learning rate. For gradient descent methods, this is the step size. For Optim and OptimKit this serves as the initial step size guess input into the linesearch\nd::Int=5: The dimension of the feature map or \"Encoding\". This is the true maximum dimension of the feature vectors. For a splitting encoding, d = numsplits * auxbasis_dim\ncutoff::Float64=1E-10: Size based cutoff for the number of singular values in the SVD (See Itensors SVD documentation)\ndtype::DataType=Float64 or ComplexF64 depending on encoding: The datatype of the elements of the MPS. Supports the arbitrary precsion types such as BigFloat and Complex{BigFloat}\nexit_early::Bool=false: Stops training if training accuracy is 1 at the end of any sweep.\n\nEncoding Options\n\nencoding::Symbol=:Legendre: The encoding to use, including :Stoudenmire, :Fourier, :Legendre, :SLTD, :Custom, etc. see Encoding docs for a complete list. Can be just a time (in)dependent orthonormal basis, or a time (in)dependent basis mapped onto a number of \"splits\" which distribute tighter basis functions where the sites of a timeseries are more likely to be measured.  \nprojected_basis::Bool=false: Whether toproject a basis onto the training data at each time. Normally, when specifying a basis of dimension d, the first d lowest order terms are used. When project=true, the training data is used to construct a pdf of the possible timeseries amplitudes at each time point. The first d largest terms of this pdf expanded in a series are used to select the basis terms.\naux_basis_dim::Int=2: Unused for standard encodings. If the encoding is a SplitBasis, serves as the auxilliary dimension of a basis mapped onto the split encoding, so that the number of histogram bins = d / auxbasisdim. \nencode_classes_separately::Bool=false: Only relevant for data driven bases. If true, then data is split up by class before being encoded. Functionally, this causes the encoding method to vary depending on the class\n\nData Preprocessing and MPS initialisation\n\nsigmoid_transform::Bool: Whether to apply a sigmoid transform to the data before minmaxing. This has the form\n\nboldsymbolX = left(1 + exp-fracboldsymbolX-m_boldsymbolXr_boldsymbolX  135right)^-1\n\nwhere boldsymbolX is the un-normalized time-series data matrix, m_boldsymbolX is the median of boldsymbolX and r_boldsymbolXis its interquartile range.\n\nminmax::Bool: Whether to apply a minmax norm to [0,1] before encoding. This has the form\n\nboldsymbolX =  fracboldsymbolX - x_textminx_textmax - x_textmin\n\nwhere boldsymbolX is the scaled robust-sigmoid transformed data matrix, x_textmin and x_textmax are the minimum and maximum of boldsymbolX.\n\ndata_bounds::Tuple{Float64, Float64} = (0.,1.): The region to bound the data to if minmax=true. This is separate from the encoding domain. All encodings expect data to be scaled scaled between 0 and 1. Setting the data bounds a bit away from [0,1] can help when your basis has poor support near its boundaries.\ninit_rng::Int: Random seed used to generate the initial MPS\nchi_init::Int: Initial bond dimension of the random MPS\n\nLoss Functions and Optimisation Methods\n\nloss_grad::Symbol=:KLD: The type of cost function to use for training the MPS, typically Mean Squared Error (:MSE) or KL Divergence (:MSE), but can also be a weighted sum of the two (:Mixed)\nbbopt::Symbol=:TSGO: Which local Optimiser to use, builtin options are symbol gradient descent (:GD), or gradient descent with a TSGO rule (:TSGO). Other options are Conjugate Gradient descent using either the Optim or OptimKit packages (:Optim or :OptimKit respectively). The CGD methods work well for MSE based loss functions, but seem to perform poorly for KLD base loss functions.\nrescale::Tuple{Bool,Bool}=(false,true): Has the form rescale = (before::Bool, after::Bool). Where to enforce the normalisation of the MPS during training, either calling normalise!(Bond Tensor) before or after BT is updated. Note that for an MPS that starts in canonical form, rescale = (true,true) will train identically to rescale = (false, true) but may be less performant.\nupdate_iters::Int=1: Maximum number of optimiser iterations to perform for each bond tensor optimisation. E.G. The number of steps of (Conjugate) Gradient Descent used by TSGO, Optim or OptimKit\ntrain_classes_separately::Bool=false: Whether the the trainer optimises the total MPS loss over all classes or whether it considers each class as a separate problem. Should make very little diffence\n\nDebug\n\nreturn_encoding_meta_info::Bool=false: Debug flag: Whether to return the normalised data as well as the histogram bins for the splitbasis types\n\n\n\n\n\nConvert the internal Options type into a serialisable MPSOptions.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"The internal Options type and the functions that help convert between","category":"page"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"MPSTime.Options\nMPSTime.safe_options\nMPSTime.model_encoding\nMPSTime.symbolic_encoding\nMPSTime.model_loss_func\nMPSTime.model_bbopt","category":"page"},{"location":"docstrings/#MPSTime.Options","page":"Docstrings","title":"MPSTime.Options","text":"Options(; <Keyword Arguments>)\n\nThe internal options struct. Fields have the same meaning as MPSOptions, but contains objects instead of symbols, e.g. Encoding=Basis(\"Legendre\") instead of :Legendre\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#MPSTime.safe_options","page":"Docstrings","title":"MPSTime.safe_options","text":"safe_options(opts::AbstractMPSOptions)\n\nTakes any AbstractMPSOptions type, and returns an instantiated Options type.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.model_encoding","page":"Docstrings","title":"MPSTime.model_encoding","text":"model_encoding(symb::Symbol, project::Bool=false)\n\nConstruct an Encoding object from symb. Not case sensitive. See Encodings documentation for the full list of options. Will use the specified project options if the encoding supports projecting. The inverse of symbolic_encoding.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.symbolic_encoding","page":"Docstrings","title":"MPSTime.symbolic_encoding","text":"symbolic_encoding(E::Encoding)\n\nConstruct a symbolic name from an Encoding object. The inverse of model_encoding\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.model_loss_func","page":"Docstrings","title":"MPSTime.model_loss_func","text":"model_loss_func(symb::Symbol)\n\nSelect a loss function (::Function) from the symb. Not case sensitive. The inverse of symboliclossfunc\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#MPSTime.model_bbopt","page":"Docstrings","title":"MPSTime.model_bbopt","text":"model_bbopt(symb::Symbol)\n\nConstuct a BBOpt object from symb. Not case sensitive.\n\n\n\n\n\n","category":"function"}]
}
