<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Classification · MPSTime</title><meta name="title" content="Classification · MPSTime"/><meta property="og:title" content="Classification · MPSTime"/><meta property="twitter:title" content="Classification · MPSTime"/><meta name="description" content="Documentation for MPSTime."/><meta property="og:description" content="Documentation for MPSTime."/><meta property="twitter:description" content="Documentation for MPSTime."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/citations.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.svg" alt="MPSTime logo"/><img class="docs-dark-only" src="../assets/logo-dark.svg" alt="MPSTime logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li class="is-active"><a class="tocitem" href>Classification</a><ul class="internal"><li><a class="tocitem" href="#nts_demo"><span>Demo dataset</span></a></li><li><a class="tocitem" href="#classification_training"><span>Training an MPS</span></a></li><li><a class="tocitem" href="#c_hparams"><span>Hyperparameters</span></a></li><li><a class="tocitem" href="#Classification"><span>Classification</span></a></li><li><a class="tocitem" href="#Training-with-a-custom-basis"><span>Training with a custom basis</span></a></li><li><a class="tocitem" href="#Docstrings"><span>Docstrings</span></a></li></ul></li><li><a class="tocitem" href="../imputation/">Imputation</a></li><li><a class="tocitem" href="../synthdatagen/">Synthetic Data Generation</a></li><li><a class="tocitem" href="../encodings/">Encodings</a></li><li><a class="tocitem" href="../hyperparameters/">Hyperparameter Tuning</a></li><li><a class="tocitem" href="../tools/">Tools</a></li><li><a class="tocitem" href="../docstrings/">Docstrings</a></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Classification</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Classification</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/hugopstackhouse/MPSTime.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/hugopstackhouse/MPSTime.jl/blob/main/docs/src/classification.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Classification_top"><a class="docs-heading-anchor" href="#Classification_top">Classification</a><a id="Classification_top-1"></a><a class="docs-heading-anchor-permalink" href="#Classification_top" title="Permalink"></a></h1><p>This tutorial for MPSTime will take you through the basic steps needed to fit an MPS to a time-series dataset.</p><h2 id="nts_demo"><a class="docs-heading-anchor" href="#nts_demo">Demo dataset</a><a id="nts_demo-1"></a><a class="docs-heading-anchor-permalink" href="#nts_demo" title="Permalink"></a></h2><p>First, import or generate your data. Here, we generate a two class &quot;noisy trendy sine&quot; dataset for the sake of demonstration, but if you have a dataset in mind, you can skip to the next section. Our demonstration dataset consists of a sine function with a randomised phase, plus a linear trend, plus some normally distributed noise. Each <span>$T$</span>-length time series in class <span>$c$</span> at time <span>$t$</span> is given by:</p><p class="math-container">\[x^c_t = \sin{\left(\frac{2\pi}{\tau}t + \psi\right)} + \frac{mt}{T} + \sigma_c n_t\,,\]</p><p>where <span>$\tau$</span> is the period, <span>$m$</span> is the slope of a linear trend, <span>$\psi \in [0, 2\pi)$</span> is a uniformly random phase offset, <span>$\sigma_c$</span> is the noise scale, and <span>$n_t \sim \mathcal{N}(0,1)$</span> are  normally distributed random variables. </p><p>For the demonstration dataset, the two classes will be generated with different distributions of periods. The class one time series <span>$x^1$</span> have <span>$\tau \in[12, 15]$</span>, and the class two time series <span>$x^2$</span> will have<span>$\tau \in[16, 19]$</span>. We&#39;ll use <span>$\sigma_c = 0.2$</span>, and the slope <span>$m$</span> will be randomly selected from <span>$\{-3,0,3\}$</span>.</p><p>We&#39;ll set up this dataset using the <a href="#MPSTime.trendy_sine"><code>trendy_sine</code></a> function from MPSTime.</p><pre><code class="language-julia hljs">using MPSTime, Random
rng = Xoshiro(1); # fix rng seed
ntimepoints = 100; # specify number of samples per instance
ntrain_instances = 300; # specify num training instances
ntest_instances = 200; # specify num test instances
X_train = vcat(
    trendy_sine(ntimepoints, ntrain_instances ÷ 2; sigma=0.1, slope=[-3,0,3], period=(12,15), rng=rng)[1],
    trendy_sine(ntimepoints, ntrain_instances ÷ 2; sigma=0.1, slope=[-3,0,3], period=(16,19), rng=rng)[1]
);
y_train = vcat(
    fill(1, ntrain_instances ÷ 2),
    fill(2, ntrain_instances ÷ 2)
);
X_test = vcat(
    trendy_sine(ntimepoints, ntest_instances ÷ 2; sigma=0.2, slope=[-3,0,3], period=(12,15), rng=rng)[1],
    trendy_sine(ntimepoints, ntest_instances ÷ 2; sigma=0.2, slope=[-3,0,3], period=(16,19), rng=rng)[1]
);
y_test = vcat(
    fill(1, ntest_instances ÷ 2),
    fill(2, ntest_instances ÷ 2)
);</code></pre><pre><code class="language-julia hljs">using Plots
p1 = plot(X_train[1:30,:]&#39;; colour=&quot;blue&quot;, alpha=0.5, legend=:none, title=&quot;Class 1&quot;);
p2 = plot(X_train[end-30:end,:]&#39;; colour=&quot;blue&quot;, alpha=0.5, legend=:none, title=&quot;Class 2&quot;);
plot(p1,p2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">GKS: cannot open display - headless operation mode active</code></pre><p><img src="../figs_generated/classification_classes.svg" alt/></p><h2 id="classification_training"><a class="docs-heading-anchor" href="#classification_training">Training an MPS</a><a id="classification_training-1"></a><a class="docs-heading-anchor-permalink" href="#classification_training" title="Permalink"></a></h2><div class="admonition is-warning" id="Floating-Point-Error-1ff6dba3e056f686"><header class="admonition-header">Floating Point Error<a class="admonition-anchor" href="#Floating-Point-Error-1ff6dba3e056f686" title="Permalink"></a></header><div class="admonition-body"><p>Depending on the dataset, the results of <code>fitMPS</code> can be noticeably affected by what machine it is running on. If you&#39;re trying to replicate these tutorials, expect a classification uncertainty of 1-2% (the noisy trendy sine can be something of an extreme case). You can resolve this by either using higher precision computing (pass <code>dtype=BigFloat</code> or <code>Complex{BigFloat}</code> to <a href="#MPSTime.MPSOptions"><code>MPSOptions</code></a>), or use the <a href="../hyperparameters/#MPSTime.evaluate"><code>evaluate</code></a> function to resample  your data and average the result. This is generally not significant for scientific computing applications as for real word datasets, the floating point error of up to a few percent is much less than the resampling error caused by choosing different train/test splits.</p></div></div><p>To train an MPS on your dataset, first, set up the hyperparameters (see <a href="#c_hparams"><code>Hyperparameters</code></a>):</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; opts = MPSOptions(); # calling this with no arguments gives default hyperparameters</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; print_opts(opts; long=false); # pretty print the options table</code><code class="nohighlight hljs ansi" style="display:block;">┌─────────┬──────────────────┬─────────┬─────────┬───────────────────┬───────────┬───────┐
│ nsweeps │         encoding │     eta │ chi_max │ sigmoid_transform │ loss_grad │     d │
│   Int64 │           Symbol │ Float64 │   Int64 │              Bool │    Symbol │ Int64 │
├─────────┼──────────────────┼─────────┼─────────┼───────────────────┼───────────┼───────┤
│      10 │ Legendre_No_Norm │    0.01 │      25 │              true │       KLD │     5 │
└─────────┴──────────────────┴─────────┴─────────┴───────────────────┴───────────┴───────┘</code></pre><p>and then pass the data and hyperparameters to the <a href="#MPSTime.fitMPS-Tuple{Matrix, Vector, Matrix, Vector, MPSOptions, Nothing}"><code>fitMPS</code></a> function:</p><pre><code class="language-julia-repl hljs">julia&gt; mps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, opts);</code></pre><details class="admonition is-details" id="output-collapsed-e9085acb1ccf9817"><summary class="admonition-header">output collapsed<a class="admonition-anchor" href="#output-collapsed-e9085acb1ccf9817" title="Permalink"></a></summary><div class="admonition-body"><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; mps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, opts);</code><code class="nohighlight hljs ansi" style="display:block;">Generating initial weight MPS with bond dimension χ_init = 4
        using random state 1234.
The test set couldn&#39;t be perfectly rescaled by the training set normalization, 2 additional rescaling operations had to be performed!
Initialising train states.
Initialising test states.
Using 1 iterations per update.
Training KL Div. 115.83859037734713 | Training acc. 0.5433333333333333.
Test KL Div. 114.9569611924355 | Testing acc. 0.52.

Test conf: [38 62; 34 66].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [1/10]
Backward sweep finished.
Starting forward sweep: [1/10]
Finished sweep 1. Time for sweep: 22.0s
Training KL Div. -32.2972055851588 | Training acc. 0.8566666666666667.
Test KL Div. -25.665024406900578 | Testing acc. 0.71.

Test conf: [69 31; 27 73].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [2/10]
Backward sweep finished.
Starting forward sweep: [2/10]
Finished sweep 2. Time for sweep: 4.69s
Training KL Div. -36.94842047195944 | Training acc. 0.8466666666666667.
Test KL Div. -30.837176149367387 | Testing acc. 0.74.

Test conf: [75 25; 27 73].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [3/10]
Backward sweep finished.
Starting forward sweep: [3/10]
Finished sweep 3. Time for sweep: 4.07s
Training KL Div. -38.412530478067474 | Training acc. 0.8966666666666666.
Test KL Div. -32.44056421039497 | Testing acc. 0.8.

Test conf: [77 23; 17 83].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [4/10]
Backward sweep finished.
Starting forward sweep: [4/10]
Finished sweep 4. Time for sweep: 4.75s
Training KL Div. -40.1375971296207 | Training acc. 0.97.
Test KL Div. -34.15883937879675 | Testing acc. 0.85.

Test conf: [81 19; 11 89].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [5/10]
Backward sweep finished.
Starting forward sweep: [5/10]
Finished sweep 5. Time for sweep: 4.03s
Training KL Div. -41.992515926282 | Training acc. 0.9866666666666667.
Test KL Div. -36.02053883050395 | Testing acc. 0.93.

Test conf: [92 8; 6 94].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [6/10]
Backward sweep finished.
Starting forward sweep: [6/10]
Finished sweep 6. Time for sweep: 4.92s
Training KL Div. -43.15730357183779 | Training acc. 0.9966666666666667.
Test KL Div. -37.122287770243496 | Testing acc. 0.935.

Test conf: [93 7; 6 94].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [7/10]
Backward sweep finished.
Starting forward sweep: [7/10]
Finished sweep 7. Time for sweep: 4.23s
Training KL Div. -44.03586281380655 | Training acc. 0.9966666666666667.
Test KL Div. -38.01536217563261 | Testing acc. 0.945.

Test conf: [94 6; 5 95].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [8/10]
Backward sweep finished.
Starting forward sweep: [8/10]
Finished sweep 8. Time for sweep: 4.8s
Training KL Div. -44.7376304096775 | Training acc. 0.9966666666666667.
Test KL Div. -38.76277089269799 | Testing acc. 0.95.

Test conf: [93 7; 3 97].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [9/10]
Backward sweep finished.
Starting forward sweep: [9/10]
Finished sweep 9. Time for sweep: 4.15s
Training KL Div. -45.3176870677532 | Training acc. 0.9966666666666667.
Test KL Div. -39.34052417200651 | Testing acc. 0.95.

Test conf: [93 7; 3 97].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [10/10]
Backward sweep finished.
Starting forward sweep: [10/10]
Finished sweep 10. Time for sweep: 4.94s
Training KL Div. -45.80868418941644 | Training acc. 0.9966666666666667.
Test KL Div. -39.79946176878575 | Testing acc. 0.95.

Test conf: [93 7; 3 97].

MPS normalised!

Training KL Div. -45.80868418941644 | Training acc. 0.9966666666666667.
Test KL Div. -39.79946176878575 | Testing acc. 0.95.

Test conf: [93 7; 3 97].</code></pre></div></details><p><a href="#MPSTime.fitMPS-Tuple{Matrix, Vector, Matrix, Vector, MPSOptions, Nothing}"><code>fitMPS</code></a> doesn&#39;t use <code>X_test</code> or <code>y_test</code> for anything except printing performance evaluations, so it is safe to leave them blank. For unsupervised learning, input a dataset with only one class, or only pass <code>X_train</code> ( <code>y_train</code> has a default value of <code>zeros(Int, size(X_train, 1))</code> ).</p><p>The <code>mps::TrainedMPS</code> can be passed directly to <a href="#MPSTime.classify-Tuple{TrainedMPS, AbstractMatrix}"><code>classify</code></a> for classification, or <a href="../imputation/#MPSTime.init_imputation_problem-Tuple{TrainedMPS, Matrix}"><code>init_imputation_problem</code></a> to set up an imputation problem. <code>info</code> provides a short training summary, which can be pretty-printed with the <a href="#MPSTime.sweep_summary-Tuple{Union{Nothing, IO}, Any}"><code>sweep_summary</code></a> function.</p><p>You can use also <code>test_states</code> to print a summary of the MPS performance on the test set.</p><pre><code class="language-julia hljs">get_training_summary(mps, test_states; print_stats=true);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Dict{Symbol, Any} with 8 entries:
  :confmat           =&gt; [93 7; 3 97]
  :test_balanced_acc =&gt; 0.95
  :recall            =&gt; 0.95
  :test_acc          =&gt; 0.95
  :precision         =&gt; 0.950721
  :specificity       =&gt; 0.95
  :f1_score          =&gt; 0.94998
  :train_acc         =&gt; 0.996667</code></pre><h2 id="c_hparams"><a class="docs-heading-anchor" href="#c_hparams">Hyperparameters</a><a id="c_hparams-1"></a><a class="docs-heading-anchor-permalink" href="#c_hparams" title="Permalink"></a></h2><p>There are number of hyperparameters and data preprocessing opttrendy_sine ions that can be specified using <code>MPSOptions(; key=value)</code></p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MPSTime.MPSOptions" href="#MPSTime.MPSOptions"><code>MPSTime.MPSOptions</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MPSOptions(; &lt;Keyword Arguments&gt;)</code></pre><p>Set the hyperparameters and other options for fitMPS. </p><p><strong>Fields:</strong></p><p><strong>Logging</strong></p><ul><li><code>verbosity::Int=1</code>: How much debug/progress info to print to the terminal while optimising the MPS. Higher numbers mean more output</li><li><code>log_level::Int=3</code>: How much statistical output. 0 for nothing, &gt;0 to print losses, accuracies, and confusion matrix at each step. Noticeable computational overhead </li><li><code>track_cost::Bool=false</code>: Whether to print the cost at each Bond tensor site to the terminal while training, mostly useful for debugging new cost functions or optimisers (<strong>HUGE</strong> computational overhead)</li></ul><p><strong>MPS Training Hyperparameters</strong></p><ul><li><code>nsweeps::Int=5</code>: Number of MPS optimisation sweeps to perform (One sweep is both forwards and Backwards)</li><li><code>chi_max::Int=25</code>: Maximum bond dimension allowed within the MPS during the SVD step</li><li><code>eta::Float64=0.01</code>: The learning rate. For gradient descent methods, this is the step size. For Optim and OptimKit this serves as the initial step size guess input into the linesearch</li><li><code>d::Int=5</code>: The dimension of the feature map or &quot;Encoding&quot;. This is the true maximum dimension of the feature vectors. For a splitting encoding, d = num<em>splits * aux</em>basis_dim</li><li><code>cutoff::Float64=1E-10</code>: Size based cutoff for the number of singular values in the SVD (See Itensors SVD documentation)</li><li><code>dtype::DataType=Float64 or ComplexF64 depending on encoding</code>: The datatype of the elements of the MPS. Supports the arbitrary precsion types such as BigFloat and Complex{BigFloat}</li><li><code>exit_early::Bool=false</code>: Stops training if training accuracy is 1 at the end of any sweep.</li></ul><p><strong>Encoding Options</strong></p><ul><li><code>encoding::Symbol=:Legendre</code>: The encoding to use, including :Stoudenmire, :Fourier, :Legendre, :SLTD, :Custom, etc. see Encoding docs for a complete list. Can be just a time (in)dependent orthonormal basis, or a time (in)dependent basis mapped onto a number of &quot;splits&quot; which distribute tighter basis functions where the sites of a timeseries are more likely to be measured.  </li><li><code>projected_basis::Bool=false</code>: Whether to project a basis onto the training data at each time. Normally, when specifying a basis of dimension <em>d</em>, the first <em>d</em> lowest order terms are used. When project=true, the training data is used to construct a pdf of the possible timeseries amplitudes at each time point. The first <em>d</em> largest terms of this pdf expanded in a series are used to select the basis terms.</li><li><code>aux_basis_dim::Int=2</code>: Unused for standard encodings. If the encoding is a SplitBasis, serves as the auxilliary dimension of a basis mapped onto the split encoding, so that the number of histogram bins = <em>d</em> / <em>aux<em>basis</em>dim</em>. </li><li><code>encode_classes_separately::Bool=false</code>: Only relevant for data driven bases. If true, then data is split up by class before being encoded. Functionally, this causes the encoding method to vary depending on the class</li></ul><p><strong>Data Preprocessing and MPS initialisation</strong></p><ul><li><code>sigmoid_transform::Bool</code>: Whether to apply a sigmoid transform to the data before minmaxing. This has the form</li></ul><p class="math-container">\[\boldsymbol{X&#39;} = \left(1 + \exp{-\frac{\boldsymbol{X}-m_{\boldsymbol{X}}}{r_{\boldsymbol{X}} / 1.35}}\right)^{-1}\]</p><p>where <span>$\boldsymbol{X}$</span> is the un-normalized time-series data matrix, <span>$m_{\boldsymbol{X}}$</span> is the median of <span>$\boldsymbol{X}$</span> and <span>$r_{\boldsymbol{X}}$</span>is its interquartile range.</p><ul><li><code>minmax::Bool</code>: Whether to apply a minmax norm to <code>[0,1]</code> before encoding. This has the form</li></ul><p class="math-container">\[\boldsymbol{X&#39;} =  \frac{\boldsymbol{X} - x&#39;_{\text{min}}}{x&#39;_{\text{max}} - x&#39;_{\text{min}}},\]</p><p>where <span>$\boldsymbol{X&#39;&#39;}$</span> is the scaled robust-sigmoid transformed data matrix, <span>$x&#39;_\text{min}$</span> and <span>$x&#39;_\text{max}$</span> are the minimum and maximum of <span>$\boldsymbol{X&#39;}$</span>.</p><ul><li><p><code>data_bounds::Tuple{Float64, Float64} = (0.,1.)</code>: The region to bound the data to if minmax=true. This is separate from the encoding domain. All encodings expect data to be scaled scaled between 0 and 1. Setting the data bounds a bit away from [0,1] can help when your basis has poor support near its boundaries.</p></li><li><p><code>init_rng::Int</code>: Random seed used to generate the initial MPS</p></li><li><p><code>chi_init::Int</code>: Initial bond dimension of the random MPS</p></li></ul><p><strong>Loss Functions and Optimisation Methods</strong></p><ul><li><p><code>loss_grad::Symbol=:KLD</code>: The type of cost function to use for training the MPS, typically Mean Squared Error (:MSE) or KL Divergence (:KLD), but can also be a weighted sum of the two (:Mixed) if use<em>legacy</em>ITensor is enabled.</p></li><li><p><code>bbopt::Symbol=:TSGO</code>: Which local Optimiser to use, builtin options are symbol gradient descent (:GD), or gradient descent with a TSGO rule (:TSGO). If <code>use_legacy_ITensor</code>` is enabled, can be a Conjugate Gradient descent optimisation rule using either the Optim or OptimKit package (:Optim or :OptimKit respectively). The CGD methods work well for MSE based loss functions, but seem to perform poorly for KLD base loss functions.</p></li><li><p><code>rescale::Tuple{Bool,Bool}=(false,true)</code>: Has the form <code>rescale = (before::Bool, after::Bool)</code>. Where to enforce the normalisation of the MPS during training, either calling normalise!(<em>Bond Tensor</em>) before or after BT is updated. Note that for an MPS that starts in canonical form, rescale = (true,true) will train identically to rescale = (false, true) but may be less performant.</p></li><li><p><code>update_iters::Int=1</code>: Maximum number of optimiser iterations to perform for each bond tensor optimisation. E.G. The number of steps of (Conjugate) Gradient Descent used by TSGO, Optim or OptimKit</p></li><li><p><code>train_classes_separately::Bool=false</code>: Whether the the trainer optimises the total MPS loss over all classes or whether it considers each class as a separate problem. Should make very little diffence</p></li><li><p><code>use_legacy_ITensor::Bool=false</code>: Whether to use the old, slow (but possibly easier to understand) ITensor Implementation</p></li><li><p><code>svd_alg::String=&quot;divide_and_conquer&quot;</code>: SVD Algorithm to pass to ITensor</p></li></ul><p>`</p><p><strong>Debug</strong></p><ul><li><code>return_encoding_meta_info::Bool=false</code>: Debug flag: Whether to return the normalised data as well as the histogram bins for the splitbasis types</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/hugopstackhouse/MPSTime.jl/blob/5e4e8a2c461323132483d8fb560db4bfb218169a/src/Structs/options.jl#L42-L105">source</a></section><section><div><pre><code class="language-julia hljs">MPSOptions(params::NamedTuple) -&gt; MPSOptions</code></pre><p>Convert the named tuple <code>params</code> with format (:option1=value1, :option2=value2,...) to an MPSOptions object.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/hugopstackhouse/MPSTime.jl/blob/5e4e8a2c461323132483d8fb560db4bfb218169a/src/Structs/options.jl#L364-L369">source</a></section></article><h2 id="Classification"><a class="docs-heading-anchor" href="#Classification">Classification</a><a id="Classification-1"></a><a class="docs-heading-anchor-permalink" href="#Classification" title="Permalink"></a></h2><p>To predict the class of unseen data, use the <a href="#MPSTime.classify-Tuple{TrainedMPS, AbstractMatrix}"><code>classify</code></a> function.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MPSTime.classify-Tuple{TrainedMPS, AbstractMatrix}" href="#MPSTime.classify-Tuple{TrainedMPS, AbstractMatrix}"><code>MPSTime.classify</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-Julia hljs">classify(mps::TrainedMPS, X_test::AbstractMatrix)) -&gt; (predictions::Vector)</code></pre><p>Use the <code>mps</code> to predict the class of the rows of <code>X_test</code> by computing the maximum overlap.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; W, info, test_states = fitMPS( X_train, y_train);

julia&gt; preds  = classify(W, X_test); # make some predictions

julia&gt; mean(preds .== y_test)
0.9504373177842566</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/hugopstackhouse/MPSTime.jl/blob/5e4e8a2c461323132483d8fb560db4bfb218169a/src/summary.jl#L138-L154">source</a></section></article><p>For example, for the noisy trendy sine from earlier:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; predictions = classify(mps, X_test);</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using StatsBase</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mean(predictions .== y_test)</code><code class="nohighlight hljs ansi" style="display:block;">0.95</code></pre><h2 id="Training-with-a-custom-basis"><a class="docs-heading-anchor" href="#Training-with-a-custom-basis">Training with a custom basis</a><a id="Training-with-a-custom-basis-1"></a><a class="docs-heading-anchor-permalink" href="#Training-with-a-custom-basis" title="Permalink"></a></h2><p>To train with a custom basis, first, declare a custom basis with <a href="../encodings/#MPSTime.function_basis"><code>function_basis</code></a>, and pass it in as the last argument to <a href="#MPSTime.fitMPS-Tuple{Matrix, Vector, Matrix, Vector, MPSOptions, Nothing}"><code>fitMPS</code></a>. For this to work, the encoding hyperparameter must be set to <code>:Custom</code> in <code>MPSOptions</code></p><pre><code class="language-julia hljs">using LegendrePolynomials
function legendre_encode(x::Float64, d::Int)
    # default legendre encoding: choose the first n-1 legendre polynomials

    leg_basis = [Pl(x, i; norm = Val(:normalized)) for i in 0:(d-1)]

    return leg_basis
end
custom_basis = function_basis(legendre_encode, false, (-1., 1.))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Basis(Custom)</code></pre><pre><code class="language-julia-repl hljs">julia&gt; mps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, MPSOptions(; encoding=:Custom), custom_basis);</code></pre><details class="admonition is-details" id="output-collapsed-3c22603bd80b8a1f"><summary class="admonition-header">output collapsed<a class="admonition-anchor" href="#output-collapsed-3c22603bd80b8a1f" title="Permalink"></a></summary><div class="admonition-body"><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; mps, info, test_states = fitMPS(X_train, y_train, X_test, y_test, MPSOptions(; encoding=:Custom), custom_basis);</code><code class="nohighlight hljs ansi" style="display:block;">Generating initial weight MPS with bond dimension χ_init = 4
        using random state 1234.
The test set couldn&#39;t be perfectly rescaled by the training set normalization, 2 additional rescaling operations had to be performed!
Initialising train states.
Initialising test states.
Using 1 iterations per update.
Training KL Div. 115.83859037734713 | Training acc. 0.5433333333333333.
Test KL Div. 114.9569611924355 | Testing acc. 0.52.

Test conf: [38 62; 34 66].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [1/10]
Backward sweep finished.
Starting forward sweep: [1/10]
Finished sweep 1. Time for sweep: 3.45s
Training KL Div. -32.2972055851588 | Training acc. 0.8566666666666667.
Test KL Div. -25.665024406900578 | Testing acc. 0.71.

Test conf: [69 31; 27 73].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [2/10]
Backward sweep finished.
Starting forward sweep: [2/10]
Finished sweep 2. Time for sweep: 3.99s
Training KL Div. -36.94842047195944 | Training acc. 0.8466666666666667.
Test KL Div. -30.837176149367387 | Testing acc. 0.74.

Test conf: [75 25; 27 73].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [3/10]
Backward sweep finished.
Starting forward sweep: [3/10]
Finished sweep 3. Time for sweep: 4.16s
Training KL Div. -38.412530478067474 | Training acc. 0.8966666666666666.
Test KL Div. -32.44056421039497 | Testing acc. 0.8.

Test conf: [77 23; 17 83].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [4/10]
Backward sweep finished.
Starting forward sweep: [4/10]
Finished sweep 4. Time for sweep: 4.01s
Training KL Div. -40.1375971296207 | Training acc. 0.97.
Test KL Div. -34.15883937879675 | Testing acc. 0.85.

Test conf: [81 19; 11 89].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [5/10]
Backward sweep finished.
Starting forward sweep: [5/10]
Finished sweep 5. Time for sweep: 4.07s
Training KL Div. -41.992515926282 | Training acc. 0.9866666666666667.
Test KL Div. -36.02053883050395 | Testing acc. 0.93.

Test conf: [92 8; 6 94].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [6/10]
Backward sweep finished.
Starting forward sweep: [6/10]
Finished sweep 6. Time for sweep: 4.81s
Training KL Div. -43.15730357183779 | Training acc. 0.9966666666666667.
Test KL Div. -37.122287770243496 | Testing acc. 0.935.

Test conf: [93 7; 6 94].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [7/10]
Backward sweep finished.
Starting forward sweep: [7/10]
Finished sweep 7. Time for sweep: 4.15s
Training KL Div. -44.03586281380655 | Training acc. 0.9966666666666667.
Test KL Div. -38.01536217563261 | Testing acc. 0.945.

Test conf: [94 6; 5 95].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [8/10]
Backward sweep finished.
Starting forward sweep: [8/10]
Finished sweep 8. Time for sweep: 4.74s
Training KL Div. -44.7376304096775 | Training acc. 0.9966666666666667.
Test KL Div. -38.76277089269799 | Testing acc. 0.95.

Test conf: [93 7; 3 97].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [9/10]
Backward sweep finished.
Starting forward sweep: [9/10]
Finished sweep 9. Time for sweep: 4.08s
Training KL Div. -45.3176870677532 | Training acc. 0.9966666666666667.
Test KL Div. -39.34052417200651 | Testing acc. 0.95.

Test conf: [93 7; 3 97].
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [10/10]
Backward sweep finished.
Starting forward sweep: [10/10]
Finished sweep 10. Time for sweep: 4.78s
Training KL Div. -45.80868418941644 | Training acc. 0.9966666666666667.
Test KL Div. -39.79946176878575 | Testing acc. 0.95.

Test conf: [93 7; 3 97].

MPS normalised!

Training KL Div. -45.80868418941644 | Training acc. 0.9966666666666667.
Test KL Div. -39.79946176878575 | Testing acc. 0.95.

Test conf: [93 7; 3 97].</code></pre></div></details><h2 id="Docstrings"><a class="docs-heading-anchor" href="#Docstrings">Docstrings</a><a id="Docstrings-1"></a><a class="docs-heading-anchor-permalink" href="#Docstrings" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MPSTime.trendy_sine" href="#MPSTime.trendy_sine"><code>MPSTime.trendy_sine</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-Julia hljs">trendy_sine(T::Int, n::Int; period=nothing, slope=nothing, phase=nothing, sigma=0.0, 
    rng=Random.GLOBAL_RNG, return_metadata=true) -&gt; Tuple{Matrix{Float64}, Dict{Symbol, Any}}</code></pre><p>Generate <code>n</code> time series of length <code>T</code>, each composed of a sine wave with an optional linear trend and Gaussian noise defined by the equation:</p><p class="math-container">\[x_t = \sin\left(\frac{2\pi}{\tau}t + \psi\right) + \frac{m t}{T} + \sigma n_t\]</p><p>with period <span>$\tau$</span>, time point <span>$t$</span>, linear trend slope <span>$m$</span>, phase offset <span>$\psi$</span>, noise scale <span>$\sigma$</span> and <span>$n_t \sim \mathcal{N}(0,1)$</span></p><p><strong>Arguments</strong></p><ul><li><code>T::Int</code>: Length of each time series</li><li><code>n::Int</code>: Number of time series instances to generate</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>period</code>: Period of the sinusoid, τ<ul><li><code>nothing</code>: Random values between 1.0 and 50.0 (default)</li><li><code>Float64</code>: Fixed period for all time series</li><li><code>Tuple</code>: Bounds for uniform random values, e.g., (1.0, 20.0) → τ ~ U(1.0, 20.0)</li><li><code>Vector</code>: Sample from discrete uniform distribution, e.g., τ ∈ 10, 20, 30</li></ul></li><li><code>slope</code>: Linear trend gradient, m<ul><li><code>nothing</code>: Random values bewteen -5.0 and 5.0 (default)</li><li><code>Float64</code>: Fixed slope for all time series</li><li><code>Tuple</code>: Bounds for uniform random values, e.g., (-3.0, 3.0) → m ~ U(-3.0, 3.0)</li><li><code>Vector</code>: Sample from discrete uniform distribution, e.g., m ∈ -3.0, 0.0, 3.0</li></ul></li><li><code>phase</code>: Phase offset, ψ<ul><li><code>nothing</code>: Random values between 0 and 2π (default)</li><li><code>Float64</code>: Fixed phase for all time series</li><li><code>Tuple</code>: Bounds for uniform random values, e.g., (0.0, π) → ψ ~ U(0.0, π)</li><li><code>Vector</code>: Sample from discrete uniform distribution</li></ul></li><li><code>sigma::Real</code>: Standard deviation of Gaussian noise, σ (default: 0.0)</li><li><code>rng::AbstractRNG</code>: Random number generator for reproducibility (default: Random.GLOBAL_RNG)</li><li><code>return_metadata::Bool</code>: Return generation parameters (default: true)</li></ul><p><strong>Returns</strong></p><ul><li>Matrix{Float64} of shape (n, T)</li><li>Dictionary of generation parameters (:period, :slope, :phase, :sigma, :T, :n)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/hugopstackhouse/MPSTime.jl/blob/5e4e8a2c461323132483d8fb560db4bfb218169a/src/Simulation/toy_data.jl#L14-L52">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MPSTime.fitMPS-Tuple{Matrix, Vector, Matrix, Vector, MPSOptions, Nothing}" href="#MPSTime.fitMPS-Tuple{Matrix, Vector, Matrix, Vector, MPSOptions, Nothing}"><code>MPSTime.fitMPS</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-Julia hljs">fitMPS(X_train::AbstractMatrix, 
       y_train::AbstractVector=zeros(Int, size(X_train, 1)), 
       X_test::AbstractMatrix=zeros(0,0), 
       y_test::AbstractVector=zeros(Int, 0), 
       opts::AbstractMPSOptions=MPSOptions(),
       custom_encoding::Union{Encoding, Nothing}=nothing) -&gt; (MPS::TrainedMPS, training_info::Dict, encoded_test_states::EncodedTimeSeriesSet)</code></pre><p>Train an MPS on the data <code>X_train</code> using the hyperparameters <code>opts</code>, see <a href="#MPSTime.MPSOptions"><code>MPSOptions</code></a>. The number of classes are determined by the entries of <code>y_train</code>.</p><p>Returns a trained MPS, a dictionary containing training info, and the encoded test states. <code>X_test</code> and <code>y_test</code> are used only to print performance evaluations, and may be empty. The <code>custom_encoding</code> argument allows the use of user defined custom encodings, see <a href="../encodings/#MPSTime.function_basis"><code>function_basis</code></a>. This requires that <code>encoding=:Custom</code> is specified in <a href="#MPSTime.MPSOptions"><code>MPSOptions</code></a></p><p>NOTE: the return value <code>encoded_test_states</code> will be sorted by class, so predictions shouldn&#39;t be compared directly to <code>y_test</code>. </p><p>See also: <a href="../encodings/#MPSTime.Encoding"><code>Encoding</code></a></p><p><strong>Example</strong></p><p>See ??fitMPS to for a more verbose example</p><pre><code class="language-julia-repl hljs">julia&gt; opts = MPSOptions(; d=5, chi_max=30, encoding=:Legendre, eta=0.05);

julia&gt; print_opts(opts) # Prints options as a table
[...]

julia&gt; W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);
Generating initial weight MPS with bond dimension χ_init = 4
        using random state 1234.
Initialising train states.
Using 1 iterations per update.
Training KL Div. 28.213032851945012 | Training acc. 0.31343283582089554.
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [1/5]
[...]

Starting forward sweep: [5/5]
Finished sweep 5. Time for sweep: 0.76s
Training KL Div. -12.577920427063361 | Training acc. 1.0.

MPS normalised!

Training KL Div. -12.57792042706337 | Training acc. 1.0.
Test KL Div. -9.815236609211746 | Testing acc. 0.9504373177842566.

Test conf: [497 16; 35 481].
</code></pre><p><strong>Extended help</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Using JLD2 # load some data

julia&gt; dloc = &quot;your_data_path/data.jld2&quot;;

julia&gt; f = jldopen(dloc, &quot;r&quot;) 
           X_train = read(f, &quot;X_train&quot;)
           y_train = read(f, &quot;y_train&quot;)
           X_test = read(f, &quot;X_test&quot;)
           y_test = read(f, &quot;y_test&quot;)
       close(f);

julia&gt; opts = MPSOptions(; d=5, chi_max=30, encoding=:Legendre, eta=0.05);

julia&gt; print_opts(opts) # Prints options as a table
[...]

julia&gt; W, info, test_states = fitMPS( X_train, y_train, X_test, y_test, opts);
Generating initial weight MPS with bond dimension χ_init = 4
        using random state 1234.
Initialising train states.
Using 1 iterations per update.
Training KL Div. 28.213032851945012 | Training acc. 0.31343283582089554.
Using optimiser CustomGD with the &quot;TSGO&quot; algorithm
Starting backward sweeep: [1/5]
[...]

Starting forward sweep: [5/5]
Finished sweep 5. Time for sweep: 0.76s
Training KL Div. -12.577920427063361 | Training acc. 1.0.

MPS normalised!

Training KL Div. -12.57792042706337 | Training acc. 1.0.
Test KL Div. -9.815236609211746 | Testing acc. 0.9504373177842566.

Test conf: [497 16; 35 481].

julia&gt; get_training_summary(W, test_states; print_stats=true);
         Overlap Matrix
┌──────┬───────────┬───────────┐
│      │   |ψ0⟩    │   |ψ1⟩    │
├──────┼───────────┼───────────┤
│ ⟨ψ0| │ 5.074e-01 │ 1.463e-02 │
├──────┼───────────┼───────────┤
│ ⟨ψ1| │ 1.463e-02 │ 4.926e-01 │
└──────┴───────────┴───────────┘
          Confusion Matrix
┌──────────┬───────────┬───────────┐
│          │ Pred. |0⟩ │ Pred. |1⟩ │
├──────────┼───────────┼───────────┤
│ True |0⟩ │       497 │        16 │
├──────────┼───────────┼───────────┤
│ True |1⟩ │        35 │       481 │
└──────────┴───────────┴───────────┘
┌───────────────────┬───────────┬──────────┬──────────┬─────────────┬──────────┬───────────┐
│ test_balanced_acc │ train_acc │ test_acc │ f1_score │ specificity │   recall │ precision │
│           Float64 │   Float64 │  Float64 │  Float64 │     Float64 │  Float64 │   Float64 │
├───────────────────┼───────────┼──────────┼──────────┼─────────────┼──────────┼───────────┤
│          0.950491 │       1.0 │ 0.950437 │ 0.950425 │    0.950491 │ 0.950491 │  0.951009 │
└───────────────────┴───────────┴──────────┴──────────┴─────────────┴──────────┴───────────┘

julia&gt; sweep_summary(info)
┌────────────────┬──────────┬───────────────┬───────────────┬───────────────┬───────────────┬───────────────┬────────────┬──────────┐
│                │ Initial  │ After Sweep 1 │ After Sweep 2 │ After Sweep 3 │ After Sweep 4 │ After Sweep 5 │ After Norm │   Mean   │
├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤
│ Train Accuracy │ 0.313433 │      1.0      │      1.0      │      1.0      │      1.0      │      1.0      │    1.0     │   1.0    │
├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤
│  Test Accuracy │ 0.409135 │   0.947522    │   0.951409    │   0.948494    │   0.948494    │   0.950437    │  0.950437  │ 0.949271 │
├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤
│  Train KL Div. │  28.213  │   -11.7855    │    -12.391    │   -12.4831    │   -12.5466    │   -12.5779    │  -12.5779  │ -12.3568 │
├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤
│   Test KL Div. │ 27.7435  │   -9.12893    │   -9.73479    │   -9.79248    │    -9.8158    │   -9.81524    │  -9.81524  │ -9.65745 │
├────────────────┼──────────┼───────────────┼───────────────┼───────────────┼───────────────┼───────────────┼────────────┼──────────┤
│     Time taken │   0.0    │   0.658366    │    0.75551    │   0.719035    │   0.718444    │    1.16256    │    NaN     │ 0.802783 │
└────────────────┴──────────┴───────────────┴───────────────┴───────────────┴───────────────┴───────────────┴────────────┴──────────┘
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/hugopstackhouse/MPSTime.jl/blob/5e4e8a2c461323132483d8fb560db4bfb218169a/src/Training/RealRealHighDimension.jl#L253-L382">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MPSTime.sweep_summary-Tuple{Union{Nothing, IO}, Any}" href="#MPSTime.sweep_summary-Tuple{Union{Nothing, IO}, Any}"><code>MPSTime.sweep_summary</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-Julia hljs">sweep_summary([io::IO], info)</code></pre><p>Print a pretty summary of what happened in every sweep</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/hugopstackhouse/MPSTime.jl/blob/5e4e8a2c461323132483d8fb560db4bfb218169a/src/summary.jl#L373-L379">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MPSTime.get_training_summary-Tuple{IO, TrainedMPS, EncodedTimeSeriesSet}" href="#MPSTime.get_training_summary-Tuple{IO, TrainedMPS, EncodedTimeSeriesSet}"><code>MPSTime.get_training_summary</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-Julia hljs">get_training_summary(
    [io::IO],
    mps::TrainedMPS, 
    test_states::EncodedTimeSeriesSet;  
    print_stats::Bool=false
    ) -&gt; stats::Dict</code></pre><p>Print a summary of the training process of <code>mps</code>, with performane evaluated on <code>test_states</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/hugopstackhouse/MPSTime.jl/blob/5e4e8a2c461323132483d8fb560db4bfb218169a/src/summary.jl#L357-L368">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MPSTime.print_opts" href="#MPSTime.print_opts"><code>MPSTime.print_opts</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">print_opts([io::IO], opts::AbstractMPSOptions; long::Bool=false)</code></pre><p>Print the MPSOptions struct in a table. Summarises (<code>long=false</code>) by default.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/hugopstackhouse/MPSTime.jl/blob/5e4e8a2c461323132483d8fb560db4bfb218169a/src/summary.jl#L432-L437">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Introduction</a><a class="docs-footer-nextpage" href="../imputation/">Imputation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Sunday 18 May 2025 21:19">Sunday 18 May 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
